[
    {
        "func_name": "_read_xml",
        "idx": "1",
        "repo_name": "undercutspiky___SFL",
        "func_path": "preprocess/CAMELYON17/generate_all_patch_coords.py",
        "orig_func": "def _read_xml(xml_path, mask_level):\n    \"\"\"\n    Read an XML file with annotations and return coordinates of tumor and normal areas\n    Coordinates are scaled if mask_level > 0\n    \"\"\"\n    xml = parse(xml_path).getroot()\n    tumor_coord_list = []\n    normal_coord_list = []\n    for annotation in xml.iter('Annotation'):\n        annotation_type = annotation.get('PartOfGroup')\n        assert annotation_type in ['metastases', 'normal', 'None']\n        if annotation_type == 'metastases':\n            coord_list = tumor_coord_list\n        elif annotation_type == 'normal':\n            coord_list = normal_coord_list\n        elif annotation_type == 'None':\n            continue\n        for (region_idx, region) in enumerate(annotation.iter('Coordinates')):\n            assert region_idx == 0\n            coords = []\n            for coord in region:\n                coords.append([round(float(coord.get('X')) / 2 ** mask_level), round(float(coord.get('Y')) / 2 ** mask_level)])\n            coord_list.append(coords)\n    return (tumor_coord_list, normal_coord_list)",
        "orig_context": "```python\n## preprocess/CAMELYON17/generate_all_patch_coords.py\nfrom xml.etree.ElementTree import parse\n\ndef _read_xml(xml_path, mask_level):\n    \"\"\"\n    Read an XML file with annotations and return coordinates of tumor and normal areas\n    Coordinates are scaled if mask_level > 0\n    \"\"\"\n    xml = parse(xml_path).getroot()\n\n    tumor_coord_list = []\n    normal_coord_list = []\n    for annotation in xml.iter('Annotation'):\n        annotation_type = annotation.get('PartOfGroup')\n        assert annotation_type in ['metastases', 'normal', 'None']\n        if annotation_type == 'metastases':\n            coord_list = tumor_coord_list\n        elif annotation_type == 'normal':\n            coord_list = normal_coord_list\n        elif annotation_type == 'None':\n            continue\n\n        for region_idx, region in enumerate(annotation.iter('Coordinates')):\n            assert region_idx == 0\n            coords = []\n            for coord in region:\n                coords.append([round(float(coord.get('X')) / (2 ** mask_level)),\n                               round(float(coord.get('Y')) / (2 ** mask_level))])\n            coord_list.append(coords)\n\n    return tumor_coord_list, normal_coord_list\n\n```\n\n\n",
        "eval_script": "## preprocess/CAMELYON17/generate_all_patch_coords.py\nfrom xml.etree.ElementTree import parse\nimport os\n\ndef _read_xml(xml_path, mask_level):\n    \"\"\"\n    Read an XML file with annotations and return coordinates of tumor and normal areas.\n    Coordinates are scaled if mask_level > 0\n    \"\"\"\n    xml = parse(xml_path).getroot()\n\n    tumor_coord_list = []\n    normal_coord_list = []\n    for annotation in xml.iter('Annotation'):\n        annotation_type = annotation.get('PartOfGroup')\n        assert annotation_type in ['metastases', 'normal', 'None']\n        if annotation_type == 'metastases':\n            coord_list = tumor_coord_list\n        elif annotation_type == 'normal':\n            coord_list = normal_coord_list\n        elif annotation_type == 'None':\n            continue\n\n        for region_idx, region in enumerate(annotation.iter('Coordinates')):\n            assert region_idx == 0\n            coords = []\n            for coord in region:\n                coords.append([round(float(coord.get('X')) / (2 ** mask_level)),\n                               round(float(coord.get('Y')) / (2 ** mask_level))])\n            coord_list.append(coords)\n\n    return tumor_coord_list, normal_coord_list\n\n\n\ndef test__read_xml():\n    xml_test_path = '/home/user/tmp/test_annotations.xml'\n    sample_xml_content = '''\n    <Annotations>\n        <Annotation PartOfGroup=\"metastases\">\n            <Coordinates>\n                <Coordinate X=\"1.0\" Y=\"2.0\"/>\n                <Coordinate X=\"3.0\" Y=\"4.0\"/>\n            </Coordinates>\n        </Annotation>\n        <Annotation PartOfGroup=\"normal\">\n            <Coordinates>\n                <Coordinate X=\"5.0\" Y=\"6.0\"/>\n                <Coordinate X=\"7.0\" Y=\"8.0\"/>\n            </Coordinates>\n        </Annotation>\n    </Annotations>\n    '''\n    \n    # Write the sample XML content to a file\n    os.makedirs(os.path.dirname(xml_test_path), exist_ok=True)\n    with open(xml_test_path, 'w') as f:\n        f.write(sample_xml_content)\n    \n    mask_level = 0  # No scaling for simplicity\n\n    coords1 = _read_xml(xml_test_path, mask_level)\n    coords2 = _read_xml_new_implementation(xml_test_path, mask_level)\n\n    assert coords1 == coords2, \"The two implementations do not produce the same results.\"\n    assert coords1[0] == [[[1, 2], [3, 4]]], \"Tumor coordinates are incorrect.\"\n    assert coords1[1] == [[[5, 6], [7, 8]]], \"Normal coordinates are incorrect.\"\n\n    os.remove(xml_test_path)  # Clean-up\n\nif __name__ == \"__main__\":\n    test__read_xml()"
    },
    {
        "func_name": "save_checkpoint",
        "idx": "4",
        "repo_name": "undercutspiky___SFL",
        "func_path": "train.py",
        "orig_func": "def save_checkpoint(file_name, model, root_dir):\n    checkpoint = {'model': model.state_dict()}\n    save_path = root_dir / f'{file_name}.pth'\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(checkpoint, save_path)\n    return save_path",
        "orig_context": "```python\n## train.py\nimport torch\n\ndef save_checkpoint(file_name, model, root_dir):\n    checkpoint = {'model': model.state_dict()}\n    save_path = root_dir / f'{file_name}.pth'\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(checkpoint, save_path)\n    return save_path\n\n```\n\n\n",
        "eval_script": "## train.py\nimport torch\nimport os\nfrom pathlib import Path\n\ndef save_checkpoint(file_name, model, root_dir):\n    checkpoint = {'model': model.state_dict()}\n    save_path = root_dir / f'{file_name}.pth'\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(checkpoint, save_path)\n    return save_path\n\n\ndef test_save_checkpoint():\n    class SimpleModel:\n        def state_dict(self):\n            return {'weight': torch.tensor([1.0, 2.0, 3.0])}\n\n    model = SimpleModel()\n    root_dir = Path('/home/user/tmp')\n    file_name = 'test_model'\n\n    path1 = save_checkpoint(file_name, model, root_dir)\n    path2 = save_checkpoint_new_implementation(file_name, model, root_dir)\n\n    # Assert that the returned paths are the same\n    assert path1 == path2, \"The paths returned by both functions should be the same.\"\n\n    # Assert that the saved file exists\n    assert os.path.exists(path1), \"The checkpoint file was not created.\"\n\n    # Load the saved checkpoints and compare\n    checkpoint1 = torch.load(path1)\n    checkpoint2 = torch.load(path2)\n    assert torch.equal(checkpoint1['model']['weight'], checkpoint2['model']['weight']), \"The model states should be identical.\"\n\nif __name__ == '__main__':\n    test_save_checkpoint()"
    },
    {
        "func_name": "generate_file",
        "idx": "5",
        "repo_name": "undercutspiky___SFL",
        "func_path": "preprocess/OCELOT/generate_all_patch_coords.py",
        "orig_func": "def generate_file(mask_path, slide_path):\n    (slide_map, tumor_mask, normal_mask) = _make_masks(slide_path, mask_path)\n    df = _record_patches(slide_map, tumor_mask, normal_mask)\n    return df",
        "orig_context": "```python\n## preprocess/OCELOT/generate_all_patch_coords.py\nimport os\n\nimport cv2\n\nimport pandas as pd\n\nCENTER_SIZE = 90\n\nNORMAL_THRESHOLD = 1\n\nTUMOUR_THRESHOLD = 0\n\ndef _make_masks(slide_path, mask_path):\n    \"\"\"\n    Return a slide with annotated tumor, normal, and tissue masks using an Otsu threshold\n    \"\"\"\n    print(f'_make_masks on {os.path.basename(slide_path)}')\n\n    slide_map = cv2.imread(slide_path)\n    slide_map = cv2.cvtColor(slide_map, cv2.COLOR_BGR2RGB)\n\n    slide_labels = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    h, w, _ = slide_map.shape\n    slide_labels = cv2.resize(slide_labels, (w, h), interpolation=cv2.INTER_CUBIC)\n    tumor_mask = slide_labels == 2\n    normal_mask = slide_labels == 1\n\n    return slide_map, tumor_mask, normal_mask\n\ndef _record_patches(slide_map, tumor_mask, normal_mask):\n    \"\"\"\n    Extract all tumor and non-tumor patches from a slide, using the given masks.\n    \"\"\"\n\n    # Extract normal, tumor patches using normal, tumor mask\n    height, width = slide_map.shape[0] // CENTER_SIZE, slide_map.shape[1] // CENTER_SIZE\n\n    print(f'_record_patches(w={width},h={height})')\n    margin = 5  # 3\n    mask_max = 1\n\n    width_mask_step = CENTER_SIZE\n    height_mask_step = CENTER_SIZE\n\n    patch_list = []\n    pos_count, neg_count = 0, 0\n\n    # These mark the coordinates of the central region of the patch\n    for i in range(margin, width - margin):\n        for j in range(margin, height - margin):\n\n            mask_i_start = round(width_mask_step * i)\n            mask_i_end = round(width_mask_step * (i + 1))\n            mask_j_start = round(height_mask_step * j)\n            mask_j_end = round(height_mask_step * (j + 1))\n\n            # Compute masks only over central region\n            tumor_mask_avg = tumor_mask[\n                             mask_j_start: mask_j_end,\n                             mask_i_start: mask_i_end].mean()\n            normal_mask_avg = normal_mask[\n                              mask_j_start: mask_j_end,\n                              mask_i_start: mask_i_end].mean()\n            # Calculate tissue avg to avoid having non-tissue slide background as normal tissue\n            tissue_avg = slide_map[mask_j_start: mask_j_end, mask_i_start: mask_i_end].mean()\n\n            tumor_area_ratio = tumor_mask_avg / mask_max\n            normal_area_ratio = normal_mask_avg / mask_max\n\n            # Extract patch coordinates\n            # Coords correspond to the center only, not the entire patch\n            if tumor_area_ratio > TUMOUR_THRESHOLD:\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 1))\n                pos_count += 1\n\n            elif normal_area_ratio >= NORMAL_THRESHOLD and tissue_avg < 243:  # threshold tissue_avg to avoid white bg\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 0))\n                neg_count += 1\n\n    print(f'patch_list length = {len(patch_list)}, pos_count={pos_count}, neg_count={neg_count}')\n\n    df = pd.DataFrame(patch_list, columns=['x_coord', 'y_coord', 'tumor'])\n    return df\n\ndef generate_file(mask_path, slide_path):\n    slide_map, tumor_mask, normal_mask = _make_masks(slide_path, mask_path)\n    df = _record_patches(slide_map, tumor_mask, normal_mask)\n\n    return df\n\n```\n\n\n",
        "eval_script": "import os\nimport pandas as pd\nimport numpy as np\n\nCENTER_SIZE = 90\nNORMAL_THRESHOLD = 1\nTUMOUR_THRESHOLD = 0\n\ndef _make_masks(slide_path, mask_path):\n    \"\"\"\n    Return a slide with annotated tumor, normal, and tissue masks using an Otsu threshold\n    \"\"\"\n    print(f'_make_masks on {os.path.basename(slide_path)}')\n\n    # Simulate reading an image by creating a dummy array\n    slide_map = np.random.randint(0, 255, (360, 360, 3), dtype=np.uint8)  # Mock slide image, size 360x360x3\n\n    # Simulate reading a mask by creating a dummy array\n    slide_labels = np.random.randint(0, 3, (360, 360), dtype=np.uint8)  # Mock slide label, size 360x360\n    h, w, _ = slide_map.shape\n    # Directly resize using numpy\n    tumor_mask = np.resize(slide_labels, (h, w)) == 2\n    normal_mask = np.resize(slide_labels, (h, w)) == 1\n\n    return slide_map, tumor_mask, normal_mask\n\ndef _record_patches(slide_map, tumor_mask, normal_mask):\n    \"\"\"\n    Extract all tumor and non-tumor patches from a slide, using the given masks.\n    \"\"\"\n    height, width = slide_map.shape[0] // CENTER_SIZE, slide_map.shape[1] // CENTER_SIZE\n\n    print(f'_record_patches(w={width},h={height})')\n    margin = 5  # 3\n    mask_max = 1\n\n    width_mask_step = CENTER_SIZE\n    height_mask_step = CENTER_SIZE\n\n    patch_list = []\n    pos_count, neg_count = 0, 0\n\n    # These mark the coordinates of the central region of the patch\n    for i in range(margin, width - margin):\n        for j in range(margin, height - margin):\n\n            mask_i_start = round(width_mask_step * i)\n            mask_i_end = round(width_mask_step * (i + 1))\n            mask_j_start = round(height_mask_step * j)\n            mask_j_end = round(height_mask_step * (j + 1))\n\n            # Compute masks only over central region\n            tumor_mask_avg = tumor_mask[mask_j_start: mask_j_end, mask_i_start: mask_i_end].mean()\n            normal_mask_avg = normal_mask[mask_j_start: mask_j_end, mask_i_start: mask_i_end].mean()\n            tissue_avg = slide_map[mask_j_start: mask_j_end, mask_i_start: mask_i_end].mean()\n\n            tumor_area_ratio = tumor_mask_avg / mask_max\n            normal_area_ratio = normal_mask_avg / mask_max\n\n            if tumor_area_ratio > TUMOUR_THRESHOLD:\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 1))\n                pos_count += 1\n            elif normal_area_ratio >= NORMAL_THRESHOLD and tissue_avg < 243:\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 0))\n                neg_count += 1\n\n    print(f'patch_list length = {len(patch_list)}, pos_count={pos_count}, neg_count={neg_count}')\n\n    df = pd.DataFrame(patch_list, columns=['x_coord', 'y_coord', 'tumor'])\n    return df\n\ndef generate_file(mask_path, slide_path):\n    slide_map, tumor_mask, normal_mask = _make_masks(slide_path, mask_path)\n    df = _record_patches(slide_map, tumor_mask, normal_mask)\n    return df\n\n\n# Dummy implementation for demonstration purposes\n\n\ndef test_generate_file():\n    # Simulate paths\n    mask_path = '/home/user/tmp/mask'\n    slide_path = '/home/user/tmp/slide'\n    \n    # Mock test cases\n    expected_df_1 = generate_file(mask_path, slide_path)\n    result_df_1 = generate_file_new_implementation(mask_path, slide_path)\n    assert expected_df_1.equals(result_df_1), \"Test case 1 failed\"\n\n    expected_df_2 = generate_file(mask_path, slide_path)\n    result_df_2 = generate_file_new_implementation(mask_path, slide_path)\n    assert expected_df_2.equals(result_df_2), \"Test case 2 failed\"\n\n    expected_df_3 = generate_file(mask_path, slide_path)\n    result_df_3 = generate_file_new_implementation(mask_path, slide_path)\n    assert expected_df_3.equals(result_df_3), \"Test case 3 failed\"\n\n    print(\"All test cases passed.\")\n\nif __name__ == \"__main__\":\n    test_generate_file()"
    },
    {
        "func_name": "generate_files",
        "idx": "6",
        "repo_name": "undercutspiky___SFL",
        "func_path": "preprocess/OCELOT/generate_all_patch_coords.py",
        "orig_func": "def generate_files(ocelot_root, output_root):\n    ocelot_root = Path(ocelot_root)\n    with open(ocelot_root / 'metadata.json', 'r') as f:\n        metadata = json.load(f)\n    aggregate_df = pd.DataFrame(columns=['slide_name', 'file_name', 'organ', 'x_coord', 'y_coord', 'tumor'])\n    for (file_name, data) in metadata['sample_pairs'].items():\n        file_path = ocelot_root / 'images' / f'{file_name}.png'\n        if not file_path.exists():\n            print(f'Skipping {file_name} as it does not exist hopefully cuz mpp (in the next sentence) is > 0.26.', f\"MPP_X = {data['mpp_x']}\")\n            continue\n        mask_path = ocelot_root / 'annotations' / f'{file_name}.png'\n        df = generate_file(str(mask_path), str(file_path))\n        df['slide_name'] = data['slide_name']\n        df['file_name'] = file_name\n        df['organ'] = data['organ']\n        aggregate_df = pd.concat([aggregate_df, df])\n    aggregate_df = aggregate_df.reset_index(drop=True)\n    aggregate_df.to_csv(os.path.join(output_root, 'all_patch_coords.csv'))\n    return aggregate_df",
        "orig_context": "```python\n## preprocess/OCELOT/generate_all_patch_coords.py\nimport json\n\nimport os\n\nfrom pathlib import Path\n\nimport cv2\n\nimport pandas as pd\n\nCENTER_SIZE = 90\n\nNORMAL_THRESHOLD = 1\n\nTUMOUR_THRESHOLD = 0\n\ndef _make_masks(slide_path, mask_path):\n    \"\"\"\n    Return a slide with annotated tumor, normal, and tissue masks using an Otsu threshold\n    \"\"\"\n    print(f'_make_masks on {os.path.basename(slide_path)}')\n\n    slide_map = cv2.imread(slide_path)\n    slide_map = cv2.cvtColor(slide_map, cv2.COLOR_BGR2RGB)\n\n    slide_labels = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    h, w, _ = slide_map.shape\n    slide_labels = cv2.resize(slide_labels, (w, h), interpolation=cv2.INTER_CUBIC)\n    tumor_mask = slide_labels == 2\n    normal_mask = slide_labels == 1\n\n    return slide_map, tumor_mask, normal_mask\n\ndef _record_patches(slide_map, tumor_mask, normal_mask):\n    \"\"\"\n    Extract all tumor and non-tumor patches from a slide, using the given masks.\n    \"\"\"\n\n    # Extract normal, tumor patches using normal, tumor mask\n    height, width = slide_map.shape[0] // CENTER_SIZE, slide_map.shape[1] // CENTER_SIZE\n\n    print(f'_record_patches(w={width},h={height})')\n    margin = 5  # 3\n    mask_max = 1\n\n    width_mask_step = CENTER_SIZE\n    height_mask_step = CENTER_SIZE\n\n    patch_list = []\n    pos_count, neg_count = 0, 0\n\n    # These mark the coordinates of the central region of the patch\n    for i in range(margin, width - margin):\n        for j in range(margin, height - margin):\n\n            mask_i_start = round(width_mask_step * i)\n            mask_i_end = round(width_mask_step * (i + 1))\n            mask_j_start = round(height_mask_step * j)\n            mask_j_end = round(height_mask_step * (j + 1))\n\n            # Compute masks only over central region\n            tumor_mask_avg = tumor_mask[\n                             mask_j_start: mask_j_end,\n                             mask_i_start: mask_i_end].mean()\n            normal_mask_avg = normal_mask[\n                              mask_j_start: mask_j_end,\n                              mask_i_start: mask_i_end].mean()\n            # Calculate tissue avg to avoid having non-tissue slide background as normal tissue\n            tissue_avg = slide_map[mask_j_start: mask_j_end, mask_i_start: mask_i_end].mean()\n\n            tumor_area_ratio = tumor_mask_avg / mask_max\n            normal_area_ratio = normal_mask_avg / mask_max\n\n            # Extract patch coordinates\n            # Coords correspond to the center only, not the entire patch\n            if tumor_area_ratio > TUMOUR_THRESHOLD:\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 1))\n                pos_count += 1\n\n            elif normal_area_ratio >= NORMAL_THRESHOLD and tissue_avg < 243:  # threshold tissue_avg to avoid white bg\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 0))\n                neg_count += 1\n\n    print(f'patch_list length = {len(patch_list)}, pos_count={pos_count}, neg_count={neg_count}')\n\n    df = pd.DataFrame(patch_list, columns=['x_coord', 'y_coord', 'tumor'])\n    return df\n\ndef generate_file(mask_path, slide_path):\n    slide_map, tumor_mask, normal_mask = _make_masks(slide_path, mask_path)\n    df = _record_patches(slide_map, tumor_mask, normal_mask)\n\n    return df\n\ndef generate_files(ocelot_root, output_root):\n    ocelot_root = Path(ocelot_root)\n    with open(ocelot_root / 'metadata.json', 'r') as f:\n        metadata = json.load(f)\n\n    aggregate_df = pd.DataFrame(\n        columns=[\n            'slide_name',\n            'file_name',\n            'organ',\n            'x_coord',\n            'y_coord',\n            'tumor'\n        ])\n\n    for file_name, data in metadata['sample_pairs'].items():\n        file_path = ocelot_root / 'images' / f'{file_name}.png'\n        if not file_path.exists():\n            print(f'Skipping {file_name} as it does not exist hopefully cuz mpp (in the next sentence) is > 0.26.',\n                  f'MPP_X = {data[\"mpp_x\"]}')\n            continue\n        mask_path = ocelot_root / 'annotations' / f'{file_name}.png'\n        df = generate_file(str(mask_path), str(file_path))\n        df['slide_name'] = data['slide_name']\n        df['file_name'] = file_name\n        df['organ'] = data['organ']\n        aggregate_df = pd.concat([aggregate_df, df])\n\n    aggregate_df = aggregate_df.reset_index(drop=True)\n    aggregate_df.to_csv(os.path.join(output_root, 'all_patch_coords.csv'))\n    return aggregate_df\n\n```\n\n\n",
        "eval_script": "import json\nimport os\nfrom pathlib import Path\nfrom PIL import Image, ImageOps\nimport pandas as pd\nimport numpy as np\n\nCENTER_SIZE = 90\nNORMAL_THRESHOLD = 1\nTUMOUR_THRESHOLD = 0\n\ndef _make_masks(slide_path, mask_path):\n    print(f'_make_masks on {os.path.basename(slide_path)}')\n    slide_map = Image.open(slide_path).convert('RGB')\n    slide_labels = Image.open(mask_path).convert('L')\n    slide_map = np.array(slide_map)\n    slide_labels = slide_labels.resize(slide_map.shape[:2][::-1], Image.BICUBIC)\n    slide_labels = np.array(slide_labels)\n    tumor_mask = slide_labels == 2\n    normal_mask = slide_labels == 1\n    return slide_map, tumor_mask, normal_mask\n\ndef _record_patches(slide_map, tumor_mask, normal_mask):\n    height, width = slide_map.shape[0] // CENTER_SIZE, slide_map.shape[1] // CENTER_SIZE\n    print(f'_record_patches(w={width},h={height})')\n    margin = 5  \n    mask_max = 1\n    width_mask_step = CENTER_SIZE\n    height_mask_step = CENTER_SIZE\n    patch_list = []\n    pos_count, neg_count = 0, 0\n\n    for i in range(margin, width - margin):\n        for j in range(margin, height - margin):\n            mask_i_start = round(width_mask_step * i)\n            mask_i_end = round(width_mask_step * (i + 1))\n            mask_j_start = round(height_mask_step * j)\n            mask_j_end = round(height_mask_step * (j + 1))\n            tumor_mask_avg = tumor_mask[\n                             mask_j_start: mask_j_end,\n                             mask_i_start: mask_i_end].mean()\n            normal_mask_avg = normal_mask[\n                              mask_j_start: mask_j_end,\n                              mask_i_start: mask_i_end].mean()\n            tissue_avg = slide_map[mask_j_start: mask_j_end, mask_i_start: mask_i_end].mean()\n            tumor_area_ratio = tumor_mask_avg / mask_max\n            normal_area_ratio = normal_mask_avg / mask_max\n\n            if tumor_area_ratio > TUMOUR_THRESHOLD:\n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 1))\n                pos_count += 1\n            elif normal_area_ratio >= NORMAL_THRESHOLD and tissue_avg < 243: \n                patch_list.append((CENTER_SIZE * i, CENTER_SIZE * j, 0))\n                neg_count += 1\n\n    print(f'patch_list length = {len(patch_list)}, pos_count={pos_count}, neg_count={neg_count}')\n    df = pd.DataFrame(patch_list, columns=['x_coord', 'y_coord', 'tumor'])\n    return df\n\ndef generate_file(mask_path, slide_path):\n    slide_map, tumor_mask, normal_mask = _make_masks(slide_path, mask_path)\n    df = _record_patches(slide_map, tumor_mask, normal_mask)\n    return df\n\ndef generate_files(ocelot_root, output_root):\n    ocelot_root = Path(ocelot_root)\n    with open(ocelot_root / 'metadata.json', 'r') as f:\n        metadata = json.load(f)\n\n    aggregate_df = pd.DataFrame(\n        columns=[\n            'slide_name',\n            'file_name',\n            'organ',\n            'x_coord',\n            'y_coord',\n            'tumor'\n        ])\n\n    for file_name, data in metadata['sample_pairs'].items():\n        file_path = ocelot_root / 'images' / f'{file_name}.png'\n        if not file_path.exists():\n            print(f'Skipping {file_name} as it does not exist hopefully cuz mpp (in the next sentence) is > 0.26.',\n                  f'MPP_X = {data[\"mpp_x\"]}')\n            continue\n        mask_path = ocelot_root / 'annotations' / f'{file_name}.png'\n        df = generate_file(str(mask_path), str(file_path))\n        df['slide_name'] = data['slide_name']\n        df['file_name'] = file_name\n        df['organ'] = data['organ']\n        aggregate_df = pd.concat([aggregate_df, df])\n\n    aggregate_df = aggregate_df.reset_index(drop=True)\n    aggregate_df.to_csv(os.path.join(output_root, 'all_patch_coords.csv'))\n    return aggregate_df\n\n\ndef create_mock_data():\n    base_path = '/home/user/tmp/ocelot'\n    os.makedirs(f'{base_path}/images', exist_ok=True)\n    os.makedirs(f'{base_path}/annotations', exist_ok=True)\n\n    metadata = {\n        \"sample_pairs\": {\n            \"sample1\": {\n                \"slide_name\": \"Slide1\",\n                \"organ\": \"Organ1\",\n                \"mpp_x\": 0.25,\n                \"mpp_y\": 0.25\n            }\n        }\n    }\n\n    # Create mock metadata.json\n    with open(f'{base_path}/metadata.json', 'w') as f:\n        json.dump(metadata, f)\n    \n    # Create mock image files\n    mock_image = np.ones((180, 180, 3), dtype=np.uint8) * 255\n    Image.fromarray(mock_image).save(f'{base_path}/images/sample1.png')\n\n    mock_annotation = np.zeros((180, 180), dtype=np.uint8)\n    Image.fromarray(mock_annotation).save(f'{base_path}/annotations/sample1.png')\n\ndef test_generate_files():\n    ocelot_root = '/home/user/tmp/ocelot'\n    output_root = '/home/user/tmp'\n    \n    create_mock_data()\n    original_output = generate_files(ocelot_root, output_root)\n    new_output = generate_files_new_implementation(ocelot_root, output_root)\n    \n    # Ensure the two DataFrames are essentially the same\n    assert original_output.equals(new_output), \"DataFrames do not match!\"\n    \n    # Check some subsets of data for further robustness\n    assert (original_output['slide_name'] == new_output['slide_name']).all(), \"Slide names differ between implementations\"\n    assert (original_output['tumor'] == new_output['tumor']).all(), \"Tumor classifications differ between implementations\"\n    assert len(original_output) == len(new_output), \"The length of the dataframes do not match\"\n\nif __name__ == \"__main__\":\n    test_generate_files()\n    print(\"All tests passed!\")"
    },
    {
        "func_name": "import_config",
        "idx": "8",
        "repo_name": "undercutspiky___SFL",
        "func_path": "train.py",
        "orig_func": "def import_config(config_path):\n    spec = importlib.util.spec_from_file_location('config', config_path)\n    config_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(config_module)\n    return config_module",
        "orig_context": "```python\n## train.py\nimport importlib\n\ndef import_config(config_path):\n    spec = importlib.util.spec_from_file_location('config', config_path)\n    config_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(config_module)\n    return config_module\n\n```\n\n\n",
        "eval_script": "## train.py\nimport importlib\n\ndef import_config(config_path):\n    spec = importlib.util.spec_from_file_location('config', config_path)\n    config_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(config_module)\n    return config_module\n\n\n\ndef test_import_config():\n    # Create a sample config file\n    config_file_path = '/home/user/tmp/sample_config.py'\n    with open(config_file_path, 'w') as f:\n        f.write('attribute1 = 42\\nattribute2 = \"test\"\\nattribute3 = [1, 2, 3]\\n')\n\n    # Use both implementations to import the same config\n    config1 = import_config(config_file_path)\n    config2 = import_config_new_implementation(config_file_path)\n    \n    # Assertions to verify both implementations yield the same results\n    assert config1.attribute1 == config2.attribute1, \"Mismatch in attribute1\"\n    assert config1.attribute2 == config2.attribute2, \"Mismatch in attribute2\"\n    assert config1.attribute3 == config2.attribute3, \"Mismatch in attribute3\"\n\nif __name__ == \"__main__\":\n    try:\n        test_import_config()\n        print(\"All tests passed.\")\n    except AssertionError as e:\n        print(f\"Test failed: {e}\")\n        exit(1)"
    },
    {
        "func_name": "get_indices_to_keep",
        "idx": "9",
        "repo_name": "undercutspiky___SFL",
        "func_path": "preprocess/CAMELYON17/generate_final_metadata.py",
        "orig_func": "def get_indices_to_keep(df):\n    indices_to_keep = []\n    np.random.seed(0)\n    tumor_mask = df['tumor'] == 1\n    for slide in set(df['slide']):\n        slide_mask = df['slide'] == slide\n        num_tumor = np.sum(slide_mask & tumor_mask)\n        num_non_tumor = np.sum(slide_mask & ~tumor_mask)\n        slide_indices_with_tumor = list(df.index[slide_mask & tumor_mask])\n        indices_to_keep += list(np.random.choice(slide_indices_with_tumor, size=min(num_tumor, num_non_tumor), replace=False))\n    return indices_to_keep",
        "orig_context": "```python\n## preprocess/CAMELYON17/generate_final_metadata.py\nimport numpy as np\n\ndef get_indices_to_keep(df):\n    indices_to_keep = []\n    np.random.seed(0)\n    tumor_mask = df['tumor'] == 1\n\n    for slide in set(df['slide']):\n        slide_mask = (df['slide'] == slide)\n        num_tumor = np.sum(slide_mask & tumor_mask)\n        num_non_tumor = np.sum(slide_mask & ~tumor_mask)\n        slide_indices_with_tumor = list(df.index[slide_mask & tumor_mask])\n        indices_to_keep += list(np.random.choice(\n            slide_indices_with_tumor,\n            size=min(num_tumor, num_non_tumor),\n            replace=False))\n\n    return indices_to_keep\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\n\ndef get_indices_to_keep(df):\n    indices_to_keep = []\n    np.random.seed(0)\n    tumor_mask = df['tumor'] == 1\n\n    for slide in set(df['slide']):\n        slide_mask = (df['slide'] == slide)\n        num_tumor = np.sum(slide_mask & tumor_mask)\n        num_non_tumor = np.sum(slide_mask & ~tumor_mask)\n        slide_indices_with_tumor = list(df.index[slide_mask & tumor_mask])\n        indices_to_keep += list(np.random.choice(\n            slide_indices_with_tumor,\n            size=min(num_tumor, num_non_tumor),\n            replace=False))\n\n    return indices_to_keep\n\n\n\ndef test_get_indices_to_keep():\n    # Test case 1: Equal number of tumor and non-tumor\n    df1 = pd.DataFrame({\n        'slide': [1, 1, 1, 2, 2, 2],\n        'tumor': [1, 0, 1, 0, 1, 0]\n    })\n    assert get_indices_to_keep(df1) == get_indices_to_keep_new_implementation(df1)\n    \n    # Test case 2: More tumor than non-tumor\n    df2 = pd.DataFrame({\n        'slide': [1, 1, 1, 2, 2, 2],\n        'tumor': [1, 1, 1, 0, 1, 0]\n    })\n    assert get_indices_to_keep(df2) == get_indices_to_keep_new_implementation(df2)\n\n    # Test case 3: No tumor\n    df3 = pd.DataFrame({\n        'slide': [1, 2, 2],\n        'tumor': [0, 0, 0]\n    })\n    assert get_indices_to_keep(df3) == get_indices_to_keep_new_implementation(df3)\n\nif __name__ == \"__main__\":\n    test_get_indices_to_keep()"
    },
    {
        "func_name": "OcelotDataset.get_input",
        "idx": "10",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/ocelot.py",
        "orig_func": "def get_input(self, idx):\n    \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n    x = self._read_image(self.patches_dir, idx)\n    x = self._transform_image(x, self.transform)\n    return x",
        "orig_context": "```python\n## datasets/ocelot.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\nimport os\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import to_tensor\nfrom pathlib import Path\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\ndef test_get_input():\n    root_dir = '/home/user/tmp'  # test directory\n    organ = 'some_organ'  # example organ\n\n    # Create a mock metadata.csv file for test purposes\n    metadata = {\n        'file_name': ['test1', 'test2', 'test3'],\n        'x_coord': [0, 1, 2],\n        'y_coord': [0, 1, 2],\n        'slide_name': ['slide1', 'slide2', 'slide3'],\n        'organ': ['some_organ', 'some_organ', 'some_organ'],\n        'tumor': [0, 1, 1]\n    }\n    metadata_df = pd.DataFrame(metadata)\n    os.makedirs(root_dir, exist_ok=True)\n    metadata_df.to_csv(os.path.join(root_dir, 'metadata.csv'))\n\n    # Prepare mock image data\n    os.makedirs(f'{root_dir}/patches/test1', exist_ok=True)\n    os.makedirs(f'{root_dir}/patches/test2', exist_ok=True)\n    os.makedirs(f'{root_dir}/patches/test3', exist_ok=True)\n\n    for i in range(3):\n        image = Image.new('RGB', (64, 64), color = (i*40, i*40, i*40))\n        image.save(f'{root_dir}/patches/test{i+1}/patch_test{i+1}_x_{i}_y_{i}.png')\n\n    ocelot_dataset = OcelotDataset(root_dir=root_dir, organ=organ, transform=None)\n\n    # Test assertions\n    assert torch.equal(ocelot_dataset.get_input(0), ocelot_dataset.get_input_new_implementation(0)), \"Mismatch at index 0\"\n    assert torch.equal(ocelot_dataset.get_input(1), ocelot_dataset.get_input_new_implementation(1)), \"Mismatch at index 1\"\n    assert torch.equal(ocelot_dataset.get_input(2), ocelot_dataset.get_input_new_implementation(2)), \"Mismatch at index 2\"\n\nif __name__ == \"__main__\":\n    test_get_input()"
    },
    {
        "func_name": "OcelotDataset._create_input_arrays",
        "idx": "12",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/ocelot.py",
        "orig_func": "def _create_input_arrays(self):\n    cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n    label_array = []\n    for (file_name, x, y, tumor) in self.df.loc[:, cols].itertuples(index=False, name=None):\n        patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n        path = f'{file_name}/{patch_name}'\n        self._input_array.append(path)\n        label_array.append(tumor)\n    self.y_array = torch.LongTensor(label_array)",
        "orig_context": "```python\n## datasets/ocelot.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "## datasets/ocelot.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        self._input_array.clear()  # Clear the list before filling it\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\ndef test__create_input_arrays():\n    # Setup test directory and mock metadata\n    root_dir = '/home/user/tmp/dataset'\n    Path(root_dir).mkdir(parents=True, exist_ok=True)\n    metadata_csv = Path(root_dir) / 'metadata.csv'\n    \n    # Create mock metadata file\n    metadata_csv.write_text(\n        \"index,file_name,slide_name,organ,tumor,x_coord,y_coord\\n\"\n        \"0,file1,slideA,lung,1,100,200\\n\"\n        \"1,file2,slideB,lung,0,150,250\\n\"\n    )\n    \n    organ = 'lung'\n    dataset = OcelotDataset(root_dir, organ)\n\n    # Store the results of the original implementation\n    original_input_array = dataset._input_array[:]\n    original_y_array = dataset.y_array.clone()\n\n    # Perform the operation using the new implementation\n    dataset._create_input_arrays_new_implementation()\n\n    # Assert the results are the same\n    assert dataset._input_array == original_input_array, \"Input arrays do not match.\"\n    assert torch.equal(dataset.y_array, original_y_array), \"Target arrays do not match.\"\n    assert len(dataset._input_array) == len(original_input_array), \"Input array lengths do not match.\"\n\nif __name__ == \"__main__\":\n    test__create_input_arrays()"
    },
    {
        "func_name": "OcelotDataset._read_and_initialise_metadata",
        "idx": "14",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/ocelot.py",
        "orig_func": "def _read_and_initialise_metadata(self):\n    df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n    self.df = df[df['organ'] == self.organ]\n    self.df.reset_index(drop=True, inplace=True)",
        "orig_context": "```python\n## datasets/ocelot.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "## datasets/ocelot.py\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import to_tensor\nimport os\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\ndef test__read_and_initialise_metadata():\n    # Create dummy metadata.csv file\n    test_dir = '/home/user/tmp'\n    os.makedirs(test_dir, exist_ok=True)\n    metadata_path = os.path.join(test_dir, 'metadata.csv')\n    if not os.path.exists(metadata_path):\n        data = {\n            'file_name': ['file1', 'file2'],\n            'slide_name': ['slideA', 'slideB'],\n            'organ': ['sample_organ', 'sample_organ'],\n            'tumor': [1, 0],\n            'x_coord': [1, 2],\n            'y_coord': [3, 4],\n        }\n        df = pd.DataFrame(data)\n        df.to_csv(metadata_path, index=False)\n\n    original_dataset = OcelotDataset(test_dir, 'sample_organ')\n    original_dataset._read_and_initialise_metadata()\n    original_df = original_dataset.df.copy()\n\n    new_dataset = OcelotDataset(test_dir, 'sample_organ')\n    new_dataset._read_and_initialise_metadata_new_implementation()\n    new_df = new_dataset.df.copy()\n\n    assert original_df.columns.equals(new_df.columns), \"Column mismatch between original and new implementation\"\n    assert len(original_df) == len(new_df), \"Row count mismatch between original and new implementation\"\n\n    # Simple data checks, comparing the first few rows for equality\n    assert original_df.head().equals(new_df.head()), \"Data mismatch in the first few rows between implementations\"\n\nif __name__ == \"__main__\":\n    test__read_and_initialise_metadata()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "OcelotDataset._transform_image",
        "idx": "15",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/ocelot.py",
        "orig_func": "@staticmethod\ndef _transform_image(image, transform):\n    if transform is not None:\n        return transform(image)\n    return to_tensor(image)",
        "orig_context": "```python\n## datasets/ocelot.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "## datasets/ocelot.py\nfrom pathlib import Path\nimport io\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import to_tensor\n\nclass OcelotDataset(Dataset):\n    def __init__(self, root_dir, organ, transform=None):\n        super(OcelotDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.organ = organ\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0,\n                         dtype={'file_name': str, 'slide_name': str, 'organ': str, 'tumor': int})\n        self.df = df[df['organ'] == self.organ]\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            patch_name = f'patch_{file_name}_x_{x}_y_{y}.png'\n            path = f'{file_name}/{patch_name}'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n\n\n\ndef test__transform_image():\n    # Create a simple red, green, and blue image for testing\n    red_image = Image.new(\"RGB\", (10, 10), \"red\")\n    green_image = Image.new(\"RGB\", (10, 10), \"green\")\n    blue_image = Image.new(\"RGB\", (10, 10), \"blue\")\n\n    # Example AS-IS transform: Convert image to tensor\n    transform_as_is = lambda x: to_tensor(x)\n    \n    # Test without any transform (should just convert to tensor)\n    assert torch.equal(\n        OcelotDataset._transform_image(red_image, None),\n        OcelotDataset._transform_image_new_implementation(red_image, None)\n    )\n    assert torch.equal(\n        OcelotDataset._transform_image(green_image, None),\n        OcelotDataset._transform_image_new_implementation(green_image, None)\n    )\n    # Test with a simple transform that converts the image to tensor\n    assert torch.equal(\n        OcelotDataset._transform_image(blue_image, transform_as_is),\n        OcelotDataset._transform_image_new_implementation(blue_image, transform_as_is)\n    )\n\nif __name__ == \"__main__\":\n    test__transform_image()"
    },
    {
        "func_name": "Camelyon17DatasetWithMasks._transform_image",
        "idx": "16",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/camelyon17.py",
        "orig_func": "@staticmethod\ndef _transform_image(image, transform):\n    if transform is not None:\n        return transform(image)\n    return to_tensor(image)",
        "orig_context": "```python\n## datasets/camelyon17.py\nimport random\n\nimport tarfile\n\nfrom io import BytesIO\n\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor, hflip, vflip, affine\n\nclass Camelyon17DatasetWithMasks(Dataset):\n    def __init__(self, root_dir, hospital: int, split, transform=None, get_mask=False,\n                 geometric_aug=True, affine_aug=False):\n        super(Camelyon17DatasetWithMasks, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.hospital = hospital\n        if split == 'train':\n            self.split = [0]\n        elif split == 'val':\n            self.split = [1]\n        elif split == 'all':\n            self.split = [0, 1]\n        else:\n            raise ValueError(f'split must be \"train\" or \"val\" or \"all\" but received split={split} instead.')\n        self.transform = transform\n        self.get_mask = get_mask\n\n        self.images_dir = self.root_dir / 'images'\n        self.masks_dir = self.root_dir / 'masks'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n\n        self.using_tar = True\n        self.tar_files = {}\n\n        self._create_input_arrays()\n\n        self.multiply_mask_with_x_prob = 0.5\n        self.geometric_aug = geometric_aug\n        self.affine_aug = affine_aug\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n        metadata = self.metadata_array[idx]\n        return x, y, metadata\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'patient': 'str'})\n        self.df = df[(df['center'] == self.hospital) & (df['split'].isin(self.split))]\n        self.df.reset_index(drop=True, inplace=True)\n\n        self.metadata_array = torch.stack(\n            (torch.LongTensor(self.df['center'].values.astype('long')),\n             torch.LongTensor(self.df['slide'].values)), dim=1\n        )\n\n    def _create_input_arrays(self):\n        cols = ['patient', 'node', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for patient, node, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            tar_filename = f'patient_{patient}_node_{node}.tar'\n            patch_name = f'patch_patient_{patient}_node_{node}_x_{x}_y_{y}.png'\n            # This tuple is same for both patches and their masks\n            self._input_array.append((tar_filename, patch_name))\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.images_dir, idx)\n        x = self._transform_image(x, self.transform)\n        if self.get_mask:\n            mask_x = self._read_image(self.masks_dir, idx)\n            mask_x = self._transform_image(mask_x, None)\n            # Data Augmentation: Either use binary mask times input instead\n            if torch.rand(1) < self.multiply_mask_with_x_prob:\n                x = mask_x * x\n\n        if self.geometric_aug and torch.rand(1) < 0.5:\n            x = hflip(x)\n            if self.get_mask:\n                mask_x = hflip(mask_x)\n\n        if self.geometric_aug and torch.rand(1) < 0.5:\n            x = vflip(x)\n            if self.get_mask:\n                mask_x = vflip(mask_x)\n\n        if self.affine_aug:\n            translate = [random.randint(-45, 45), random.randint(-45, 45)]\n            angle = random.randint(-90, 90)\n            x = affine(x, translate=translate, angle=angle, scale=1, shear=[0], fill=[1.])\n            if self.get_mask:\n                mask_x = affine(mask_x, translate=translate, angle=angle, scale=1, shear=[0], fill=[0.])\n\n        if self.get_mask:\n            return x, mask_x\n        return x, x\n\n    def _read_image(self, patches_dir, idx):\n        \"\"\"Check my blog post explaining why this: https://medium.com/p/35097f72ebbd\"\"\"\n        patient_tar_filename, patch_name = self._input_array[idx]\n        if str(patches_dir / patient_tar_filename) in self.tar_files:\n            patient_tar = self.tar_files[str(patches_dir / patient_tar_filename)]\n        else:\n            patient_tar = tarfile.open(patches_dir / patient_tar_filename)\n            self.tar_files[str(patches_dir / patient_tar_filename)] = patient_tar\n        x = self._read_image_from_tar(patient_tar, patch_name)\n\n        return x\n\n    @staticmethod\n    def _read_image_from_tar(patient_tar, patch_name):\n        try:\n            img = Image.open(BytesIO(patient_tar.extractfile(f'./{patch_name}').read())).convert('RGB')\n        except KeyError as e:\n            print(f'Received KeyError for file {patch_name} and btw tar file has len {len(patient_tar.getmembers())}')\n            raise e\n\n        return img\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "## datasets/camelyon17.py\nimport random\nimport tarfile\nfrom io import BytesIO\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import to_tensor, hflip, vflip, affine\n\nclass Camelyon17DatasetWithMasks(Dataset):\n    def __init__(self, root_dir, hospital: int, split, transform=None, get_mask=False,\n                 geometric_aug=True, affine_aug=False):\n        super(Camelyon17DatasetWithMasks, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.hospital = hospital\n        if split == 'train':\n            self.split = [0]\n        elif split == 'val':\n            self.split = [1]\n        elif split == 'all':\n            self.split = [0, 1]\n        else:\n            raise ValueError(f'split must be \"train\" or \"val\" or \"all\" but received split={split} instead.')\n        self.transform = transform\n        self.get_mask = get_mask\n\n        self.images_dir = self.root_dir / 'images'\n        self.masks_dir = self.root_dir / 'masks'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n\n        self.using_tar = True\n        self.tar_files = {}\n\n        self._create_input_arrays()\n\n        self.multiply_mask_with_x_prob = 0.5\n        self.geometric_aug = geometric_aug\n        self.affine_aug = affine_aug\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n        metadata = self.metadata_array[idx]\n        return x, y, metadata\n\n    def _read_and_initialise_metadata(self):\n        df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'patient': 'str'})\n        self.df = df[(df['center'] == self.hospital) & (df['split'].isin(self.split))]\n        self.df.reset_index(drop=True, inplace=True)\n\n        self.metadata_array = torch.stack(\n            (torch.LongTensor(self.df['center'].values.astype('long')),\n             torch.LongTensor(self.df['slide'].values)), dim=1\n        )\n\n    def _create_input_arrays(self):\n        cols = ['patient', 'node', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for patient, node, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            tar_filename = f'patient_{patient}_node_{node}.tar'\n            patch_name = f'patch_patient_{patient}_node_{node}_x_{x}_y_{y}.png'\n            # This tuple is same for both patches and their masks\n            self._input_array.append((tar_filename, patch_name))\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.images_dir, idx)\n        x = self._transform_image(x, self.transform)\n        if self.get_mask:\n            mask_x = self._read_image(self.masks_dir, idx)\n            mask_x = self._transform_image(mask_x, None)\n            # Data Augmentation: Either use binary mask times input instead\n            if torch.rand(1) < self.multiply_mask_with_x_prob:\n                x = mask_x * x\n\n        if self.geometric_aug and torch.rand(1) < 0.5:\n            x = hflip(x)\n            if self.get_mask:\n                mask_x = hflip(mask_x)\n\n        if self.geometric_aug and torch.rand(1) < 0.5:\n            x = vflip(x)\n            if self.get_mask:\n                mask_x = vflip(mask_x)\n\n        if self.affine_aug:\n            translate = [random.randint(-45, 45), random.randint(-45, 45)]\n            angle = random.randint(-90, 90)\n            x = affine(x, translate=translate, angle=angle, scale=1, shear=[0], fill=[1.])\n            if self.get_mask:\n                mask_x = affine(mask_x, translate=translate, angle=angle, scale=1, shear=[0], fill=[0.])\n\n        if self.get_mask:\n            return x, mask_x\n        return x, x\n\n    def _read_image(self, patches_dir, idx):\n        \"\"\"Check my blog post explaining why this: https://medium.com/p/35097f72ebbd\"\"\"\n        patient_tar_filename, patch_name = self._input_array[idx]\n        if str(patches_dir / patient_tar_filename) in self.tar_files:\n            patient_tar = self.tar_files[str(patches_dir / patient_tar_filename)]\n        else:\n            patient_tar = tarfile.open(patches_dir / patient_tar_filename)\n            self.tar_files[str(patches_dir / patient_tar_filename)] = patient_tar\n        x = self._read_image_from_tar(patient_tar, patch_name)\n\n        return x\n\n    @staticmethod\n    def _read_image_from_tar(patient_tar, patch_name):\n        try:\n            img = Image.open(BytesIO(patient_tar.extractfile(f'./{patch_name}').read())).convert('RGB')\n        except KeyError as e:\n            print(f'Received KeyError for file {patch_name} and btw tar file has len {len(patient_tar.getmembers())}')\n            raise e\n\n        return img\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n\n\n\ndef test__transform_image():\n    # Test case 1: No transformation applied\n    image = Image.new('RGB', (100, 100), color = 'red')\n    assert torch.equal(\n        Camelyon17DatasetWithMasks._transform_image(image, None),\n        Camelyon17DatasetWithMasks._transform_image_new_implementation(image, None)\n    ), \"Test case 1 failed\"\n\n    # Test case 2: A simple resize transformation\n    transform = lambda x: to_tensor(x.resize((50, 50)))\n    assert torch.equal(\n        Camelyon17DatasetWithMasks._transform_image(image, transform),\n        Camelyon17DatasetWithMasks._transform_image_new_implementation(image, transform)\n    ), \"Test case 2 failed\"\n\n    # Test case 3: A transformation that converts image to grayscale\n    transform = lambda x: to_tensor(x.convert('L'))\n    assert torch.equal(\n        Camelyon17DatasetWithMasks._transform_image(image, transform),\n        Camelyon17DatasetWithMasks._transform_image_new_implementation(image, transform)\n    ), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test__transform_image()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "BCSSDataset._read_image",
        "idx": "17",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/bcss.py",
        "orig_func": "def _read_image(self, patches_dir, idx):\n    patch_path = patches_dir / self._input_array[idx]\n    x = Image.open(patch_path).convert('RGB')\n    return x",
        "orig_context": "```python\n## datasets/bcss.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass BCSSDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(BCSSDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        self.df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'file_name': 'str'})\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            path = f'{file_name}/patch_{file_name}_x_{x}_y_{y}.png'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "## datasets/bcss.py\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import to_tensor\nfrom unittest.mock import patch, MagicMock\n\nclass BCSSDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(BCSSDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        self.df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'file_name': 'str'})\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            path = f'{file_name}/patch_{file_name}_x_{x}_y_{y}.png'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\ndef test__read_image():\n    # Setup mock for pandas.read_csv\n    with patch('pandas.read_csv', return_value=pd.DataFrame({\n        'file_name': ['file1'],\n        'x_coord': [10],\n        'y_coord': [20],\n        'tumor': [1]\n    })), patch('PIL.Image.open', return_value=MagicMock(spec=Image.Image)) as mock_open:\n        # Mock convert method to return a test value\n        mock_instance = mock_open.return_value\n        mock_instance.convert = MagicMock(return_value='test_image_data')\n\n        # Initiate dataset\n        dataset = BCSSDataset('/home/user/tmp')\n        \n        # Test first condition\n        idx = 0\n        original_image = dataset._read_image(dataset.patches_dir, idx)\n        new_image = dataset._read_image_new_implementation(dataset.patches_dir, idx)\n\n        # Assert type of both images is same\n        assert isinstance(original_image, str)\n        assert isinstance(new_image, str)\n\n        # Assert the content is same\n        assert original_image == new_image\n\n        # Verify that opening and converting were called once for each function\n        mock_instance.convert.assert_called_with('RGB')\n\ndef main():\n    test__read_image()\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
        "func_name": "BCSSDataset._transform_image",
        "idx": "19",
        "repo_name": "undercutspiky___SFL",
        "func_path": "datasets/bcss.py",
        "orig_func": "@staticmethod\ndef _transform_image(image, transform):\n    if transform is not None:\n        return transform(image)\n    return to_tensor(image)",
        "orig_context": "```python\n## datasets/bcss.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass BCSSDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(BCSSDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        self.df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'file_name': 'str'})\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            path = f'{file_name}/patch_{file_name}_x_{x}_y_{y}.png'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n```\n\n\n",
        "eval_script": "## datasets/bcss.py\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport torch\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms.functional import to_tensor\n\nclass BCSSDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(BCSSDataset, self).__init__()\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.patches_dir = Path(root_dir) / 'patches'\n\n        self._input_array = []\n        self.y_array = []\n\n        self._read_and_initialise_metadata()\n        self._create_input_arrays()\n\n    def __len__(self):\n        return len(self._input_array)\n\n    def __getitem__(self, idx):\n        x = self.get_input(idx)\n        y = self.y_array[idx]\n\n        return x, y\n\n    def _read_and_initialise_metadata(self):\n        self.df = pd.read_csv(self.root_dir / 'metadata.csv', index_col=0, dtype={'file_name': 'str'})\n        self.df.reset_index(drop=True, inplace=True)\n\n    def _create_input_arrays(self):\n        cols = ['file_name', 'x_coord', 'y_coord', 'tumor']\n        label_array = []\n        for file_name, x, y, tumor in self.df.loc[:, cols].itertuples(index=False, name=None):\n            path = f'{file_name}/patch_{file_name}_x_{x}_y_{y}.png'\n            self._input_array.append(path)\n            label_array.append(tumor)\n        self.y_array = torch.LongTensor(label_array)\n\n    def get_input(self, idx):\n        \"\"\"Returns x, mask_x for a given index(idx).\"\"\"\n        x = self._read_image(self.patches_dir, idx)\n        x = self._transform_image(x, self.transform)\n\n        return x\n\n    def _read_image(self, patches_dir, idx):\n        patch_path = patches_dir / self._input_array[idx]\n        x = Image.open(patch_path).convert('RGB')\n\n        return x\n\n    @staticmethod\n    def _transform_image(image, transform):\n        if transform is not None:\n            return transform(image)\n        return to_tensor(image)\n\n\n\n\ndef test__transform_image():\n    from torchvision.transforms import ToTensor, RandomHorizontalFlip, Compose\n\n    # Create some simple transformations\n    transform_1 = ToTensor()\n    transform_2 = Compose([ToTensor(), RandomHorizontalFlip(p=1)])\n    transform_3 = None\n\n    image = Image.new('RGB', (10, 10), color = 'red')\n\n    original_result_1 = BCSSDataset._transform_image(image, transform_1)\n    new_result_1 = BCSSDataset._transform_image_new_implementation(image, transform_1)\n    assert torch.equal(original_result_1, new_result_1), \"Test case 1 failed\"\n\n    original_result_2 = BCSSDataset._transform_image(image, transform_2)\n    new_result_2 = BCSSDataset._transform_image_new_implementation(image, transform_2)\n    assert torch.equal(original_result_2, new_result_2), \"Test case 2 failed\"\n\n    original_result_3 = BCSSDataset._transform_image(image, transform_3)\n    new_result_3 = BCSSDataset._transform_image_new_implementation(image, transform_3)\n    assert torch.equal(original_result_3, new_result_3), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test__transform_image()"
    },
    {
        "func_name": "calculate_variance",
        "idx": "20",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_variance.py",
        "orig_func": "def calculate_variance(data, ddof=0):\n    \"\"\"\n    Calculate the variance value of a time series with specified degrees of freedom.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the variance is to be calculated.\n    ddof : int, optional\n        The degrees of freedom to use when calculating the variance. Default is 0 (population variance).\n        \n    Returns\n    -------\n    float\n        The variance of the provided time series with specified degrees of freedom.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_variance(data)\n    2.5\n    >>> calculate_variance(data, ddof=0)  # Population variance\n    2.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    return np.var(data, ddof=ddof) if len(data) > 0 else np.nan",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_variance.py\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_variance(data, ddof=0):\n    \"\"\"\n    Calculate the variance value of a time series with specified degrees of freedom.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the variance is to be calculated.\n    ddof : int, optional\n        The degrees of freedom to use when calculating the variance. Default is 0 (population variance).\n        \n    Returns\n    -------\n    float\n        The variance of the provided time series with specified degrees of freedom.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_variance(data)\n    2.5\n    >>> calculate_variance(data, ddof=0)  # Population variance\n    2.0\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Calculate and return the variance with specified ddof, handling empty series by returning NaN\n    return np.var(data, ddof=ddof) if len(data) > 0 else np.nan\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n\n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n\n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n\n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Check data type\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Validation for Series or DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check for NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Check index type if DateTimeIndex is required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_variance(data, ddof=0):\n    \"\"\"\n    Calculate the variance value of a time series with specified degrees of freedom.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the variance is to be calculated.\n    ddof : int, optional\n        The degrees of freedom to use when calculating the variance. Default is 0 (population variance).\n\n    Returns\n    -------\n    float\n        The variance of the provided time series with specified degrees of freedom.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_variance(data)\n    2.5\n    >>> calculate_variance(data, ddof=0)  # Population variance\n    2.0\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Calculate and return the variance with specified ddof, handling empty series by returning NaN\n    return np.var(data, ddof=ddof) if len(data) > 0 else np.nan\n\n\n\ndef test_calculate_variance():\n    # Test case 1: Positive integers\n    data1 = pd.Series([1, 2, 3, 4, 5])\n    assert calculate_variance(data1) == calculate_variance_new_implementation(data1)\n    \n    # Test case 2: Floats with ddof\n    data2 = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n    assert calculate_variance(data2, ddof=1) == calculate_variance_new_implementation(data2, ddof=1)\n    \n    # Test case 3: Empty data\n    data3 = pd.Series([])\n    assert np.isnan(calculate_variance(data3)) == np.isnan(calculate_variance_new_implementation(data3))\n\nif __name__ == \"__main__\":\n    test_calculate_variance()"
    },
    {
        "func_name": "calculate_std_1st_der",
        "idx": "21",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_std_1st_der.py",
        "orig_func": "def calculate_std_1st_der(data):\n    \"\"\"\n    Calculate the standard deviation of the first derivative of a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the standard deviation of the first derivative is to be calculated.\n        \n    Returns\n    -------\n    float\n        The standard deviation of the first derivative of the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_std_1st_derivative(data)\n    0.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    return np.std(np.gradient(data)) if len(data) > 0 else np.nan",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_std_1st_der.py\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_std_1st_der(data):\n    \"\"\"\n    Calculate the standard deviation of the first derivative of a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the standard deviation of the first derivative is to be calculated.\n        \n    Returns\n    -------\n    float\n        The standard deviation of the first derivative of the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_std_1st_derivative(data)\n    0.0\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Calculate and return the standard deviation of the first derivative, handling empty series by returning NaN\n    return np.std(np.gradient(data)) if len(data) > 0 else np.nan\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_std_1st_der(data):\n    \"\"\"\n    Calculate the standard deviation of the first derivative of a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the standard deviation of the first derivative is to be calculated.\n        \n    Returns\n    -------\n    float\n        The standard deviation of the first derivative of the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_std_1st_derivative(data)\n    0.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    return np.std(np.gradient(data)) if len(data) > 0 else np.nan\n\n\n\ndef test_calculate_std_1st_der():\n    # Test case 1: Simple increasing series\n    data1 = pd.Series([1, 2, 3, 4, 5])\n    assert calculate_std_1st_der(data1) == calculate_std_1st_der_new_implementation(data1)\n    \n    # Test case 2: Constant series\n    data2 = pd.Series([5, 5, 5, 5, 5])\n    assert calculate_std_1st_der(data2) == calculate_std_1st_der_new_implementation(data2)\n    \n    # Test case 3: Negative values series\n    data3 = pd.Series([-1, -2, -3, -4, -5])\n    assert calculate_std_1st_der(data3) == calculate_std_1st_der_new_implementation(data3)\n    \ndef main():\n    test_calculate_std_1st_der()\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
        "func_name": "calculate_trough",
        "idx": "22",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_trough.py",
        "orig_func": "def calculate_trough(data, start=None, end=None):\n    \"\"\"\n    Calculate the local minimum of a time series within an optional range.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the minimum value is to be calculated.\n    start : int, str, or None, optional\n        The starting index, timestamp, or position for slicing the data.\n        If None, the series starts from the beginning.\n    end : int, str, or None, optional\n        The ending index, timestamp, or position for slicing the data.\n        If None, the series ends at the last value.\n\n    Returns\n    -------\n    float\n        The local minimum of the specified range in the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 5, 4, 3])\n    >>> calculate_trough(data)\n    1.0\n    >>> calculate_trough(data, start=1, end=3)\n    2.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    if start is not None or end is not None:\n        data = data[start:end]\n    return data.min() if len(data) > 0 else np.nan",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_trough.py\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_trough(data, start=None, end=None):\n    \"\"\"\n    Calculate the local minimum of a time series within an optional range.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the minimum value is to be calculated.\n    start : int, str, or None, optional\n        The starting index, timestamp, or position for slicing the data.\n        If None, the series starts from the beginning.\n    end : int, str, or None, optional\n        The ending index, timestamp, or position for slicing the data.\n        If None, the series ends at the last value.\n\n    Returns\n    -------\n    float\n        The local minimum of the specified range in the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 5, 4, 3])\n    >>> calculate_trough(data)\n    1.0\n    >>> calculate_trough(data, start=1, end=3)\n    2.0\n    \"\"\"\n\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n\n    # Slice the data based on start and end, if provided\n    if start is not None or end is not None:\n        data = data[start:end]\n\n    # Calculate and return the minimum, handling empty series by returning NaN\n    return data.min() if len(data) > 0 else np.nan\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n\n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n\n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n\n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n\n    # Check data type\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n\n    # Validation for Series or DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check for NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n\n        # Check index type if DateTimeIndex is required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n\n    return True\n\ndef calculate_trough(data, start=None, end=None):\n    \"\"\"\n    Calculate the local minimum of a time series within an optional range.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the minimum value is to be calculated.\n    start : int, str, or None, optional\n        The starting index, timestamp, or position for slicing the data.\n        If None, the series starts from the beginning.\n    end : int, str, or None, optional\n        The ending index, timestamp, or position for slicing the data.\n        If None, the series ends at the last value.\n\n    Returns\n    -------\n    float\n        The local minimum of the specified range in the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 5, 4, 3])\n    >>> calculate_trough(data)\n    1.0\n    >>> calculate_trough(data, start=1, end=3)\n    2.0\n    \"\"\"\n\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n\n    # Slice the data based on start and end, if provided\n    if start is not None or end is not None:\n        data = data[start:end]\n\n    # Calculate and return the minimum, handling empty series by returning NaN\n    return data.min() if len(data) > 0 else np.nan\n\n\n\ndef test_calculate_trough():\n    data_series = pd.Series([5, 2, 7, 1, 3, 4])\n    \n    # Test 1: complete series\n    assert calculate_trough(data_series) == calculate_trough_new_implementation(data_series)\n    \n    # Test 2: with start and end\n    assert calculate_trough(data_series, start=1, end=4) == calculate_trough_new_implementation(data_series, start=1, end=4)\n    \n    # Test 3: empty series\n    empty_series = pd.Series([])\n    assert np.isnan(calculate_trough(empty_series)) == np.isnan(calculate_trough_new_implementation(empty_series))\n\nif __name__ == \"__main__\":\n    test_calculate_trough()"
    },
    {
        "func_name": "calculate_spikeness",
        "idx": "23",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_spikeness.py",
        "orig_func": "def calculate_spikeness(data):\n    \"\"\"\n    Calculate the spikeness (skewness) of a time series.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the spikeness is to be calculated.\n\n    Returns\n    -------\n    float\n        The spikeness (skewness) of the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_spikeness(data)\n    0.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    if isinstance(data, np.ndarray):\n        data = pd.Series(data)\n    return data.skew() if len(data) > 0 else np.nan",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_spikeness.py\nimport pandas as pd\n\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_spikeness(data):\n    \"\"\"\n    Calculate the spikeness (skewness) of a time series.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the spikeness is to be calculated.\n\n    Returns\n    -------\n    float\n        The spikeness (skewness) of the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_spikeness(data)\n    0.0\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Ensure data is a pandas Series for compatibility with .skew()\n    if isinstance(data, np.ndarray):\n        data = pd.Series(data)\n    \n    # Return spikeness (skewness), handling empty series by returning NaN\n    return data.skew() if len(data) > 0 else np.nan\n\n```\n\n\n",
        "eval_script": "import pandas as pd\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Check the data type\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Validate for Series or DataFrame format\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check for NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Check index type if DateTimeIndex is required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_spikeness(data):\n    \"\"\"\n    Calculate the spikeness (skewness) of a time series.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the spikeness is to be calculated.\n\n    Returns\n    -------\n    float\n        The spikeness (skewness) of the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_spikeness(data)\n    0.0\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Ensure data is a pandas Series for compatibility with .skew()\n    if isinstance(data, np.ndarray):\n        data = pd.Series(data)\n    \n    # Return spikeness (skewness), handling empty series by returning NaN\n    return data.skew() if len(data) > 0 else np.nan\n\n\n\ndef test_calculate_spikeness():\n    # Test with a simple series\n    data1 = pd.Series([1, 2, 3, 4, 5])\n    assert calculate_spikeness(data1) == calculate_spikeness_new_implementation(data1)\n    \n    # Test with a numpy array with possible negative skewness\n    data2 = np.array([5, 5, 5, 4, 3, 2, 1])\n    assert calculate_spikeness(data2) == calculate_spikeness_new_implementation(data2)\n    \n    # Test with an edge case of empty series\n    data3 = pd.Series([])\n    assert np.isnan(calculate_spikeness(data3)) == np.isnan(calculate_spikeness_new_implementation(data3))\n\nif __name__ == \"__main__\":\n    test_calculate_spikeness()"
    },
    {
        "func_name": "calculate_peak",
        "idx": "24",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_peak.py",
        "orig_func": "def calculate_peak(data, start=None, end=None):\n    \"\"\"\n    Calculate the local maximum of a time series within an optional range.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the maximum value is to be calculated.\n    start : int, str, or None, optional\n        The starting index, timestamp, or position for slicing the data.\n        If None, the series starts from the beginning.\n    end : int, str, or None, optional\n        The ending index, timestamp, or position for slicing the data.\n        If None, the series ends at the last value.\n\n    Returns\n    -------\n    float\n        The local maximum of the specified range in the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 5, 4, 7])\n    >>> calculate_peak(data)\n    7.0\n    >>> calculate_peak(data, start=1, end=3)\n    5.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    if start is not None or end is not None:\n        data = data[start:end]\n    return data.max() if len(data) > 0 else np.nan",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_peak.py\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_peak(data, start=None, end=None):\n    \"\"\"\n    Calculate the local maximum of a time series within an optional range.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the maximum value is to be calculated.\n    start : int, str, or None, optional\n        The starting index, timestamp, or position for slicing the data.\n        If None, the series starts from the beginning.\n    end : int, str, or None, optional\n        The ending index, timestamp, or position for slicing the data.\n        If None, the series ends at the last value.\n\n    Returns\n    -------\n    float\n        The local maximum of the specified range in the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 5, 4, 7])\n    >>> calculate_peak(data)\n    7.0\n    >>> calculate_peak(data, start=1, end=3)\n    5.0\n    \"\"\"\n\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n\n    # Slice the data based on start and end, if provided\n    if start is not None or end is not None:\n        data = data[start:end]\n\n    # Calculate and return the maximum, handling empty series by returning NaN\n    return data.max() if len(data) > 0 else np.nan\n\n```\n\n\n",
        "eval_script": "import pandas as pd\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Check if data is of valid type\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Validate for Series or DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check for NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Check for DateTimeIndex if required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_peak(data, start=None, end=None):\n    \"\"\"\n    Calculate the local maximum of a time series within an optional range.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the maximum value is to be calculated.\n    start : int, str, or None, optional\n        The starting index, timestamp, or position for slicing the data.\n        If None, the series starts from the beginning.\n    end : int, str, or None, optional\n        The ending index, timestamp, or position for slicing the data.\n        If None, the series ends at the last value.\n\n    Returns\n    -------\n    float\n        The local maximum of the specified range in the provided time series.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 5, 4, 7])\n    >>> calculate_peak(data)\n    7.0\n    >>> calculate_peak(data, start=1, end=3)\n    5.0\n    \"\"\"\n\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n\n    # Slice the data based on start and end, if provided\n    if start is not None or end is not None:\n        data = data[start:end]\n\n    # Calculate and return the maximum, handling empty series by returning NaN\n    return data.max() if len(data) > 0 else np.nan\n\n\ndef test_calculate_peak():\n    # Test case with simple data\n    data = pd.Series([1, 3, 7, 2, 5])\n    assert calculate_peak(data) == calculate_peak_new_implementation(data)\n    \n    # Test case with start and end parameters\n    assert calculate_peak(data, start=1, end=4) == calculate_peak_new_implementation(data, start=1, end=4)\n    \n    # Test case with empty slice\n    result1 = calculate_peak(data, start=5, end=10)\n    result2 = calculate_peak_new_implementation(data, start=5, end=10)\n    assert (np.isnan(result1) and np.isnan(result2)) or result1 == result2\n\nif __name__ == \"__main__\":\n    test_calculate_peak()"
    },
    {
        "func_name": "calculate_mean",
        "idx": "25",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_mean.py",
        "orig_func": "def calculate_mean(data):\n    \"\"\"\n    Calculate the mean value of a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the mean value is to be calculated.\n        \n    Returns\n    -------\n    float\n        The mean value of the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_mean(data)\n    3.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    return data.mean() if len(data) > 0 else np.nan",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_mean.py\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_mean(data):\n    \"\"\"\n    Calculate the mean value of a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the mean value is to be calculated.\n        \n    Returns\n    -------\n    float\n        The mean value of the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_mean(data)\n    3.0\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Calculate and return the mean, handling empty series by returning NaN\n    return data.mean() if len(data) > 0 else np.nan\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_mean(data):\n    \"\"\"\n    Calculate the mean value of a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the mean value is to be calculated.\n        \n    Returns\n    -------\n    float\n        The mean value of the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_mean(data)\n    3.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    return data.mean() if len(data) > 0 else np.nan\n\n\n\ndef test_calculate_mean():\n    # Test case with a simple list\n    data_1 = np.array([1, 2, 3, 4, 5])\n    assert calculate_mean(data_1) == calculate_mean_new_implementation(data_1)\n\n    # Test case with pandas Series\n    data_2 = pd.Series([10, 20, 30, 40, 50])\n    assert calculate_mean(data_2) == calculate_mean_new_implementation(data_2)\n\n    # Test case with numpy array\n    data_3 = np.array([100, 200, 300])\n    assert calculate_mean(data_3) == calculate_mean_new_implementation(data_3)\n\nif __name__ == \"__main__\":\n    test_calculate_mean()"
    },
    {
        "func_name": "calculate_trend_strength",
        "idx": "26",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/trend_strength.py",
        "orig_func": "def calculate_trend_strength(data):\n    \"\"\"\n    Calculate the strength of the trend in a time series using linear regression.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the trend strength is to be calculated.\n        \n    Returns\n    -------\n    float\n        The R-squared value representing the strength of the trend (0 to 1).\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_trend_strength(data)\n    1.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    if len(data) < 2:\n        return np.nan\n    x = np.arange(len(data))\n    (slope, intercept, r_value, p_value, std_err) = linregress(x, data)\n    trend_strength = r_value ** 2\n    return trend_strength",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/trend_strength.py\nimport numpy as np\n\nfrom scipy.stats import linregress\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_trend_strength(data):\n    \"\"\"\n    Calculate the strength of the trend in a time series using linear regression.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the trend strength is to be calculated.\n        \n    Returns\n    -------\n    float\n        The R-squared value representing the strength of the trend (0 to 1).\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_trend_strength(data)\n    1.0\n    \"\"\"\n    # Validate the time series data\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Handle empty or insufficient data by returning NaN\n    if len(data) < 2:\n        return np.nan\n    \n    # Generate an index as a proxy for time\n    x = np.arange(len(data))\n    \n    # Fit a linear regression and calculate the R-squared value\n    slope, intercept, r_value, p_value, std_err = linregress(x, data)\n    \n    # R-squared value as trend strength\n    trend_strength = r_value ** 2\n    \n    return trend_strength\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    return True\n\ndef calculate_trend_strength(data):\n    validate_time_series_data(data, require_datetime_index=False)\n    if len(data) < 2:\n        return np.nan\n    x = np.arange(len(data))\n    slope, intercept, r_value, p_value, std_err = linregress(x, data)\n    trend_strength = r_value ** 2\n    return trend_strength\n\n\n\ndef test_calculate_trend_strength():\n    data1 = pd.Series([1, 2, 3, 4, 5])\n    data2 = pd.Series([10, 9, 4, 2, 0])\n    data3 = pd.Series([1, 1, 1, 1, 1])\n\n    # Test 1\n    assert calculate_trend_strength(data1) == calculate_trend_strength_new_implementation(data1), \"Test 1 Failed!\"\n    \n    # Test 2\n    assert calculate_trend_strength(data2) == calculate_trend_strength_new_implementation(data2), \"Test 2 Failed!\"\n    \n    # Test 3\n    assert calculate_trend_strength(data3) == calculate_trend_strength_new_implementation(data3), \"Test 3 Failed!\"\n\nif __name__ == \"__main__\":\n    test_calculate_trend_strength()"
    },
    {
        "func_name": "calculate_dominant",
        "idx": "27",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/histogram_dominant.py",
        "orig_func": "def calculate_dominant(data, bins=10, return_bin_center=False):\n    \"\"\"\n    Calculate the dominant value (mode) of a time series histogram.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the dominant value is to be calculated.\n    bins : int, optional\n        The number of bins to use for creating the histogram, by default 10.\n\n    Returns\n    -------\n    float\n        The dominant value (mode) of the histogram.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 1, 2, 3, 3, 3, 4, 5])\n    >>> calculate_dominant(data)\n    3.0\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    if len(data) == 0:\n        return np.nan\n    unique_values = np.unique(data)\n    bins = np.append(unique_values, unique_values[-1] + 1)\n    (counts, bin_edges) = np.histogram(data, bins=bins)\n    max_bin_index = np.argmax(counts)\n    if return_bin_center:\n        dominant_value = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2\n    else:\n        dominant_value = bin_edges[max_bin_index]\n    return dominant_value",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/histogram_dominant.py\nimport numpy as np\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_dominant(data, bins=10, return_bin_center=False):\n    \"\"\"\n    Calculate the dominant value (mode) of a time series histogram.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the dominant value is to be calculated.\n    bins : int, optional\n        The number of bins to use for creating the histogram, by default 10.\n\n    Returns\n    -------\n    float\n        The dominant value (mode) of the histogram.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 1, 2, 3, 3, 3, 4, 5])\n    >>> calculate_dominant(data)\n    3.0\n    \"\"\"\n    # Validate the time series data\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    if len(data) == 0:\n        return np.nan\n    \n    # Get unique values and use them as bin edges\n    unique_values = np.unique(data)\n    bins = np.append(unique_values, unique_values[-1] + 1)  # Add an extra edge to cover the last bin\n\n    # Calculate histogram with specific bin edges\n    counts, bin_edges = np.histogram(data, bins=bins)\n    max_bin_index = np.argmax(counts)\n\n    # Calculate the center or lower bound of the bin with the highest frequency\n    if return_bin_center:\n        dominant_value = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2\n    else:\n        dominant_value = bin_edges[max_bin_index]\n    \n    return dominant_value\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Check data type\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Validation for Series or DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check for NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Check index type if DateTimeIndex is required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_dominant(data, bins=10, return_bin_center=False):\n    \"\"\"\n    Calculate the dominant value (mode) of a time series histogram.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the dominant value is to be calculated.\n    bins : int, optional\n        The number of bins to use for creating the histogram, by default 10.\n\n    Returns\n    -------\n    float\n        The dominant value (mode) of the histogram.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 1, 2, 3, 3, 3, 4, 5])\n    >>> calculate_dominant(data)\n    3.0\n    \"\"\"\n    # Validate the time series data\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    if len(data) == 0:\n        return np.nan\n    \n    # Get unique values and use them as bin edges\n    unique_values = np.unique(data)\n    bins = np.append(unique_values, unique_values[-1] + 1)  # Add an extra edge to cover the last bin\n\n    # Calculate histogram with specific bin edges\n    counts, bin_edges = np.histogram(data, bins=bins)\n    max_bin_index = np.argmax(counts)\n\n    # Calculate the center or lower bound of the bin with the highest frequency\n    if return_bin_center:\n        dominant_value = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2\n    else:\n        dominant_value = bin_edges[max_bin_index]\n    \n    return dominant_value\n\n\n\ndef test_calculate_dominant():\n    data1 = pd.Series([1, 1, 2, 3, 3, 3, 4, 5])\n    assert calculate_dominant(data1) == calculate_dominant_new_implementation(data1), \"Test case 1 failed\"\n\n    data2 = np.array([5, 3, 3, 1, 1, 4, 5, 5])\n    assert calculate_dominant(data2) == calculate_dominant_new_implementation(data2), \"Test case 2 failed\"\n\n    data3 = pd.Series([10, 20, 20, 30, 20, 30, 30])\n    assert calculate_dominant(data3, return_bin_center=True) == calculate_dominant_new_implementation(data3, return_bin_center=True), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_calculate_dominant()"
    },
    {
        "func_name": "calculate_seasonality_strength",
        "idx": "28",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/seasonality_strength.py",
        "orig_func": "def calculate_seasonality_strength(data, period=2, max_lag=12):\n    \"\"\"\n    Calculate the strength of the seasonality in a time series based on autocorrelation.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the seasonality strength is to be calculated.\n    period : int, optional\n        The periodic interval to check for seasonality (default is 2).\n    max_lag : int, optional\n        The maximum number of lags to consider for autocorrelation (default is 12).\n\n    Returns\n    -------\n    float\n        The seasonality strength, ranging from 0 to 1, where 1 indicates strong seasonality.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values or is too short to calculate seasonality.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2], index=pd.date_range(\"2023-01-01\", periods=12, freq=\"M\"))\n    >>> calculate_seasonality_strength(data, period=3)\n    0.75\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    if len(data) < period + 1:\n        return np.nan\n    autocorr_values = acf(data, nlags=max(max_lag, period), fft=True)\n    seasonality_strength = autocorr_values[period] if period < len(autocorr_values) else 0.0\n    return seasonality_strength",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/seasonality_strength.py\nimport numpy as np\n\nfrom statsmodels.tsa.stattools import acf\n\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_seasonality_strength(data, period=2, max_lag=12):\n    \"\"\"\n    Calculate the strength of the seasonality in a time series based on autocorrelation.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the seasonality strength is to be calculated.\n    period : int, optional\n        The periodic interval to check for seasonality (default is 2).\n    max_lag : int, optional\n        The maximum number of lags to consider for autocorrelation (default is 12).\n\n    Returns\n    -------\n    float\n        The seasonality strength, ranging from 0 to 1, where 1 indicates strong seasonality.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values or is too short to calculate seasonality.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2], index=pd.date_range(\"2023-01-01\", periods=12, freq=\"M\"))\n    >>> calculate_seasonality_strength(data, period=3)\n    0.75\n    \"\"\"\n    # Validate the time series data\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Handle empty or insufficient data\n    if len(data) < period + 1:\n        return np.nan\n    \n    # Calculate the autocorrelation of the data\n    autocorr_values = acf(data, nlags=max(max_lag, period), fft=True)\n    \n    # The seasonality strength is based on the autocorrelation at the specified period\n    seasonality_strength = autocorr_values[period] if period < len(autocorr_values) else 0.0\n    \n    return seasonality_strength\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import acf\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Checks the type of data\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Validation for data in Series or DataFrame format\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Check index type if DateTimeIndex is required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\ndef calculate_seasonality_strength(data, period=2, max_lag=12):\n    \"\"\"\n    Calculate the strength of the seasonality in a time series based on autocorrelation.\n\n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the seasonality strength is to be calculated.\n    period : int, optional\n        The periodic interval to check for seasonality (default is 2).\n    max_lag : int, optional\n        The maximum number of lags to consider for autocorrelation (default is 12).\n\n    Returns\n    -------\n    float\n        The seasonality strength, ranging from 0 to 1, where 1 indicates strong seasonality.\n\n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values or is too short to calculate seasonality.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2], index=pd.date_range(\"2023-01-01\", periods=12, freq=\"M\"))\n    >>> calculate_seasonality_strength(data, period=3)\n    0.75\n    \"\"\"\n    # Validate the time series data\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Handle empty or insufficient data\n    if len(data) < period + 1:\n        return np.nan\n    \n    # Calculate the autocorrelation of the data\n    autocorr_values = acf(data, nlags=max(max_lag, period), fft=True)\n    \n    # The seasonality strength is based on the autocorrelation at the specified period\n    seasonality_strength = autocorr_values[period] if period < len(autocorr_values) else 0.0\n    \n    return seasonality_strength\n\n\n\ndef test_calculate_seasonality_strength():\n    data1 = pd.Series([1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2], index=pd.date_range(\"2023-01-01\", periods=12, freq=\"M\"))\n    data2 = np.array([1, 4, 2, 5, 1, 4, 2, 5, 1, 4, 2, 5])\n    data3 = pd.Series([5, 3, 4, 2, 5, 3, 4, 2, 5, 3, 4, 2])\n\n    assert calculate_seasonality_strength(data1, period=3) == calculate_seasonality_strength_new_implementation(data1, period=3)\n    assert calculate_seasonality_strength(data2, period=4) == calculate_seasonality_strength_new_implementation(data2, period=4)\n    assert calculate_seasonality_strength(data3, period=3, max_lag=10) == calculate_seasonality_strength_new_implementation(data3, period=3, max_lag=10)\n\nif __name__ == \"__main__\":\n    test_calculate_seasonality_strength()"
    },
    {
        "func_name": "calculate_length",
        "idx": "29",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/features/feature_length.py",
        "orig_func": "def calculate_length(data):\n    \"\"\"\n    Calculate the number of data points in a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the length feature is to be calculated.\n        \n    Returns\n    -------\n    int\n        The number of data points in the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_length(data)\n    5\n    \"\"\"\n    validate_time_series_data(data, require_datetime_index=False)\n    return len(data)",
        "orig_context": "```python\n## interpreTS/utils/data_validation.py\nimport pandas as pd\n\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Sprawdzenie typu danych\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Walidacja dla danych w formacie Series lub DataFrame\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Sprawdzenie warto\u015bci NaN\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Sprawdzenie typu indeksu, je\u015bli wymagany jest DateTimeIndex\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n```\n\n\n```python\n## interpreTS/core/features/feature_length.py\nfrom interpreTS.utils.data_validation import validate_time_series_data\n\ndef calculate_length(data):\n    \"\"\"\n    Calculate the number of data points in a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the length feature is to be calculated.\n        \n    Returns\n    -------\n    int\n        The number of data points in the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_length(data)\n    5\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Return the length of the data\n    return len(data)\n\n```\n\n\n",
        "eval_script": "import pandas as pd\nimport numpy as np\n\ndef validate_time_series_data(data, require_datetime_index=False):\n    \"\"\"\n    Validate if the input data is suitable for time series processing.\n    \n    Parameters\n    ----------\n    data : pd.Series, pd.DataFrame, or np.ndarray\n        The time series data to be validated.\n    require_datetime_index : bool, optional\n        If True, validation will ensure the data has a DateTime index (for time-based operations).\n        \n    Returns\n    -------\n    bool\n        True if the data is valid; raises an error otherwise.\n        \n    Raises\n    ------\n    TypeError\n        If data is not a pd.Series, pd.DataFrame, or np.ndarray.\n    ValueError\n        If the data contains NaN values or lacks a DateTime index when required.\n    \"\"\"\n    \n    # Validate data type\n    if not isinstance(data, (pd.Series, pd.DataFrame, np.ndarray)):\n        raise TypeError(\"Data must be a pandas Series, DataFrame, or numpy array.\")\n    \n    # Validate Series or DataFrame data\n    if isinstance(data, (pd.Series, pd.DataFrame)):\n        # Check for NaN values\n        if data.isnull().any().any():\n            raise ValueError(\"Data contains NaN values.\")\n        \n        # Check index type if DateTimeIndex is required\n        if require_datetime_index and not isinstance(data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for time-based operations.\")\n    \n    return True\n\n\ndef calculate_length(data):\n    \"\"\"\n    Calculate the number of data points in a time series.\n    \n    Parameters\n    ----------\n    data : pd.Series or np.ndarray\n        The time series data for which the length feature is to be calculated.\n        \n    Returns\n    -------\n    int\n        The number of data points in the provided time series.\n        \n    Raises\n    ------\n    TypeError\n        If the data is not a valid time series type.\n    ValueError\n        If the data contains NaN values.\n        \n    Examples\n    --------\n    >>> import pandas as pd\n    >>> data = pd.Series([1, 2, 3, 4, 5])\n    >>> calculate_length(data)\n    5\n    \"\"\"\n    # Validate the time series without requiring a DateTime index\n    validate_time_series_data(data, require_datetime_index=False)\n    \n    # Return the length of the data\n    return len(data)\n\n\n\n\n\ndef test_calculate_length():\n    data1 = pd.Series([1, 2, 3, 4, 5])\n    data2 = np.array([10, 20, 30, 40, 50])\n    data3 = pd.Series(range(100))\n    \n    assert calculate_length(data1) == calculate_length_new_implementation(data1)\n    assert calculate_length(data2) == calculate_length_new_implementation(data2)\n    assert calculate_length(data3) == calculate_length_new_implementation(data3)\n\n\nif __name__ == \"__main__\":\n    test_calculate_length()"
    },
    {
        "func_name": "TimeSeriesData.resample",
        "idx": "32",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/time_series_data.py",
        "orig_func": "def resample(self, interval):\n    \"\"\"\n        Resample the time series data to a specified interval.\n        \n        Parameters\n        ----------\n        interval : str\n            The interval to resample the data, e.g., 'D' for daily, 'H' for hourly.\n            \n        Returns\n        -------\n        TimeSeriesData\n            A new TimeSeriesData object with resampled data.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5], index=pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"))\n        >>> ts_data = TimeSeriesData(data)\n        >>> resampled_data = ts_data.resample(\"2D\")\n        \"\"\"\n    if not isinstance(self.data.index, pd.DatetimeIndex):\n        raise ValueError('Data must have a DateTime index for resampling.')\n    resampled_data = self.data.resample(interval).mean()\n    return TimeSeriesData(resampled_data)",
        "orig_context": "```python\n## interpreTS/core/time_series_data.py\nimport pandas as pd\n\nclass TimeSeriesData:\n    \"\"\"\n    A class to manage and process time series data.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize the TimeSeriesData with time series data.\n        \n        Parameters\n        ----------\n        data : pd.Series, pd.DataFrame, or np.ndarray\n            The time series data to be managed. If provided as a numpy array,\n            it will be converted to a pandas DataFrame for consistency.\n            \n        Examples\n        --------\n        >>> import pandas as pd\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        \"\"\"\n        \n        \n        if isinstance(data, (pd.Series, pd.DataFrame)):\n            self.data = data\n        else:\n            raise ValueError(\"Data must be a pandas Series or DataFrame.\")\n    \n    def resample(self, interval):\n        \"\"\"\n        Resample the time series data to a specified interval.\n        \n        Parameters\n        ----------\n        interval : str\n            The interval to resample the data, e.g., 'D' for daily, 'H' for hourly.\n            \n        Returns\n        -------\n        TimeSeriesData\n            A new TimeSeriesData object with resampled data.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5], index=pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"))\n        >>> ts_data = TimeSeriesData(data)\n        >>> resampled_data = ts_data.resample(\"2D\")\n        \"\"\"\n        \n        \n        if not isinstance(self.data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for resampling.\")\n\n        resampled_data = self.data.resample(interval).mean()\n        \n        return TimeSeriesData(resampled_data)\n\n    def split(self, train_size=0.7):\n        \"\"\"\n        Split the time series data into training and test sets.\n        \n        Parameters\n        ----------\n        train_size : float, optional\n            The proportion of the data to use for training, by default 0.7.\n            \n        Returns\n        -------\n        tuple of TimeSeriesData\n            A tuple containing the training and test sets as TimeSeriesData objects.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        >>> train, test = ts_data.split(0.6)\n        \"\"\"\n        \n        \n        split_index = int(len(self.data) * train_size)\n        train_data = self.data[:split_index]\n        test_data = self.data[split_index:]\n        \n        return TimeSeriesData(train_data), TimeSeriesData(test_data)\n\n```\n\n\n",
        "eval_script": "## interpreTS/core/time_series_data.py\nimport pandas as pd\n\nclass TimeSeriesData:\n    \"\"\"\n    A class to manage and process time series data.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize the TimeSeriesData with time series data.\n        \n        Parameters\n        ----------\n        data : pd.Series, pd.DataFrame, or np.ndarray\n            The time series data to be managed. If provided as a numpy array,\n            it will be converted to a pandas DataFrame for consistency.\n            \n        Examples\n        --------\n        >>> import pandas as pd\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        \"\"\"\n        \n        \n        if isinstance(data, (pd.Series, pd.DataFrame)):\n            self.data = data\n        else:\n            raise ValueError(\"Data must be a pandas Series or DataFrame.\")\n\n    def resample(self, interval):\n        \"\"\"\n        Resample the time series data to a specified interval.\n        \n        Parameters\n        ----------\n        interval : str\n            The interval to resample the data, e.g., 'D' for daily, 'H' for hourly.\n            \n        Returns\n        -------\n        TimeSeriesData\n            A new TimeSeriesData object with resampled data.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5], index=pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"))\n        >>> ts_data = TimeSeriesData(data)\n        >>> resampled_data = ts_data.resample(\"2D\")\n        \"\"\"\n        \n        \n        if not isinstance(self.data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for resampling.\")\n\n        resampled_data = self.data.resample(interval).mean()\n        \n        return TimeSeriesData(resampled_data)\n\n\n    \n\n    def split(self, train_size=0.7):\n        \"\"\"\n        Split the time series data into training and test sets.\n        \n        Parameters\n        ----------\n        train_size : float, optional\n            The proportion of the data to use for training, by default 0.7.\n            \n        Returns\n        -------\n        tuple of TimeSeriesData\n            A tuple containing the training and test sets as TimeSeriesData objects.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        >>> train, test = ts_data.split(0.6)\n        \"\"\"\n        \n        \n        split_index = int(len(self.data) * train_size)\n        train_data = self.data[:split_index]\n        test_data = self.data[split_index:]\n        \n        return TimeSeriesData(train_data), TimeSeriesData(test_data)\n\ndef test_resample():\n    # Sample data\n    data = pd.Series([1, 2, 3, 4, 5], index=pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"))\n    ts_data = TimeSeriesData(data)\n    \n    # Resample with original implementation\n    resampled_original = ts_data.resample(\"2D\").data\n    \n    # Resample with new implementation\n    resampled_new = ts_data.resample_new_implementation(\"2D\").data\n    \n    # Assertions\n    assert resampled_original.equals(resampled_new), \"Resampled data values do not match.\"\n    assert resampled_original.index.equals(resampled_new.index), \"Resampled data indices do not match.\"\n    assert len(resampled_original) == len(resampled_new), \"Resampled data lengths do not match.\"\n\nif __name__ == \"__main__\":\n    test_resample()"
    },
    {
        "func_name": "TimeSeriesData.split",
        "idx": "33",
        "repo_name": "ruleminer___InterpreTS",
        "func_path": "interpreTS/core/time_series_data.py",
        "orig_func": "def split(self, train_size=0.7):\n    \"\"\"\n        Split the time series data into training and test sets.\n        \n        Parameters\n        ----------\n        train_size : float, optional\n            The proportion of the data to use for training, by default 0.7.\n            \n        Returns\n        -------\n        tuple of TimeSeriesData\n            A tuple containing the training and test sets as TimeSeriesData objects.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        >>> train, test = ts_data.split(0.6)\n        \"\"\"\n    split_index = int(len(self.data) * train_size)\n    train_data = self.data[:split_index]\n    test_data = self.data[split_index:]\n    return (TimeSeriesData(train_data), TimeSeriesData(test_data))",
        "orig_context": "```python\n## interpreTS/core/time_series_data.py\nimport pandas as pd\n\nclass TimeSeriesData:\n    \"\"\"\n    A class to manage and process time series data.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize the TimeSeriesData with time series data.\n        \n        Parameters\n        ----------\n        data : pd.Series, pd.DataFrame, or np.ndarray\n            The time series data to be managed. If provided as a numpy array,\n            it will be converted to a pandas DataFrame for consistency.\n            \n        Examples\n        --------\n        >>> import pandas as pd\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        \"\"\"\n        \n        \n        if isinstance(data, (pd.Series, pd.DataFrame)):\n            self.data = data\n        else:\n            raise ValueError(\"Data must be a pandas Series or DataFrame.\")\n    \n    def resample(self, interval):\n        \"\"\"\n        Resample the time series data to a specified interval.\n        \n        Parameters\n        ----------\n        interval : str\n            The interval to resample the data, e.g., 'D' for daily, 'H' for hourly.\n            \n        Returns\n        -------\n        TimeSeriesData\n            A new TimeSeriesData object with resampled data.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5], index=pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"))\n        >>> ts_data = TimeSeriesData(data)\n        >>> resampled_data = ts_data.resample(\"2D\")\n        \"\"\"\n        \n        \n        if not isinstance(self.data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for resampling.\")\n\n        resampled_data = self.data.resample(interval).mean()\n        \n        return TimeSeriesData(resampled_data)\n\n    def split(self, train_size=0.7):\n        \"\"\"\n        Split the time series data into training and test sets.\n        \n        Parameters\n        ----------\n        train_size : float, optional\n            The proportion of the data to use for training, by default 0.7.\n            \n        Returns\n        -------\n        tuple of TimeSeriesData\n            A tuple containing the training and test sets as TimeSeriesData objects.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        >>> train, test = ts_data.split(0.6)\n        \"\"\"\n        \n        \n        split_index = int(len(self.data) * train_size)\n        train_data = self.data[:split_index]\n        test_data = self.data[split_index:]\n        \n        return TimeSeriesData(train_data), TimeSeriesData(test_data)\n\n```\n\n\n",
        "eval_script": "import pandas as pd\n\nclass TimeSeriesData:\n    \"\"\"\n    A class to manage and process time series data.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize the TimeSeriesData with time series data.\n        \n        Parameters\n        ----------\n        data : pd.Series, pd.DataFrame, or np.ndarray\n            The time series data to be managed. If provided as a numpy array,\n            it will be converted to a pandas DataFrame for consistency.\n            \n        Examples\n        --------\n        >>> import pandas as pd\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        \"\"\"\n        \n        \n        if isinstance(data, (pd.Series, pd.DataFrame)):\n            self.data = data\n        else:\n            raise ValueError(\"Data must be a pandas Series or DataFrame.\")\n    \n    def resample(self, interval):\n        \"\"\"\n        Resample the time series data to a specified interval.\n        \n        Parameters\n        ----------\n        interval : str\n            The interval to resample the data, e.g., 'D' for daily, 'H' for hourly.\n            \n        Returns\n        -------\n        TimeSeriesData\n            A new TimeSeriesData object with resampled data.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5], index=pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"))\n        >>> ts_data = TimeSeriesData(data)\n        >>> resampled_data = ts_data.resample(\"2D\")\n        \"\"\"\n        \n        \n        if not isinstance(self.data.index, pd.DatetimeIndex):\n            raise ValueError(\"Data must have a DateTime index for resampling.\")\n\n        resampled_data = self.data.resample(interval).mean()\n        \n        return TimeSeriesData(resampled_data)\n\n    def split(self, train_size=0.7):\n        \"\"\"\n        Split the time series data into training and test sets.\n        \n        Parameters\n        ----------\n        train_size : float, optional\n            The proportion of the data to use for training, by default 0.7.\n            \n        Returns\n        -------\n        tuple of TimeSeriesData\n            A tuple containing the training and test sets as TimeSeriesData objects.\n            \n        Examples\n        --------\n        >>> data = pd.Series([1, 2, 3, 4, 5])\n        >>> ts_data = TimeSeriesData(data)\n        >>> train, test = ts_data.split(0.6)\n        \"\"\"\n        \n        \n        split_index = int(len(self.data) * train_size)\n        train_data = self.data[:split_index]\n        test_data = self.data[split_index:]\n        \n        return TimeSeriesData(train_data), TimeSeriesData(test_data)\n\n\n\ndef test_split():\n    # Test case 1: Simple series\n    data1 = pd.Series([1, 2, 3, 4, 5])\n    ts_data1 = TimeSeriesData(data1)\n    \n    train1, test1 = ts_data1.split(0.6)\n    train2, test2 = ts_data1.split_new_implementation(0.6)\n    \n    assert train1.data.equals(train2.data), \"Training data mismatch for data1\"\n    assert test1.data.equals(test2.data), \"Test data mismatch for data1\"\n    \n    # Test case 2: Longer series with different split\n    data2 = pd.Series(range(10))\n    ts_data2 = TimeSeriesData(data2)\n    \n    train1, test1 = ts_data2.split(0.5)\n    train2, test2 = ts_data2.split_new_implementation(0.5)\n    \n    assert train1.data.equals(train2.data), \"Training data mismatch for data2\"\n    assert test1.data.equals(test2.data), \"Test data mismatch for data2\"\n    \n    # Test case 3: Dataframe input\n    data3 = pd.DataFrame({'values': [10, 20, 30, 40, 50, 60]})\n    ts_data3 = TimeSeriesData(data3)\n    \n    train1, test1 = ts_data3.split(0.7)\n    train2, test2 = ts_data3.split_new_implementation(0.7)\n    \n    assert train1.data.equals(train2.data), \"Training data mismatch for data3\"\n    assert test1.data.equals(test2.data), \"Test data mismatch for data3\"\n\nif __name__ == \"__main__\":\n    test_split()"
    },
    {
        "func_name": "collate_fn",
        "idx": "34",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/datasets/collate.py",
        "orig_func": "def collate_fn(dataset_items: list[dict]):\n    \"\"\"\n    Collate and pad fields in the dataset items.\n    Converts individual items into a batch.\n\n    Args:\n        dataset_items (list[dict]): list of objects from\n            dataset.__getitem__.\n    Returns:\n        result_batch (dict[Tensor]): dict, containing batch-version\n            of the tensors.\n    \"\"\"\n    result_batch = {'mix_audio': [], 's1_audio': [], 's2_audio': [], 's1_mouth': [], 's2_mouth': [], 'mix_audio_length': [], 's1_audio_length': [], 's2_audio_length': []}\n    for item in dataset_items:\n        result_batch['mix_audio'].append(item['mix_audio'].squeeze(0))\n        result_batch['s1_audio'].append(item['s1_audio'].squeeze(0))\n        result_batch['s2_audio'].append(item['s2_audio'].squeeze(0))\n        result_batch['mix_audio_length'].append(item['mix_audio_length'])\n        result_batch['s1_audio_length'].append(item['s1_audio_length'])\n        result_batch['s2_audio_length'].append(item['s2_audio_length'])\n\n    def pad_sequences(sequences):\n        return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n    result_batch['mix_audio'] = pad_sequences(result_batch['mix_audio'])\n    result_batch['s1_audio'] = pad_sequences(result_batch['s1_audio'])\n    result_batch['s2_audio'] = pad_sequences(result_batch['s2_audio'])\n    result_batch['mix_audio_length'] = torch.tensor(result_batch['mix_audio_length'])\n    result_batch['s1_audio_length'] = torch.tensor(result_batch['s1_audio_length'])\n    result_batch['s2_audio_length'] = torch.tensor(result_batch['s2_audio_length'])\n    return result_batch",
        "orig_context": "```python\n## src/datasets/collate.py\nimport torch\n\ndef collate_fn(dataset_items: list[dict]):\n    \"\"\"\n    Collate and pad fields in the dataset items.\n    Converts individual items into a batch.\n\n    Args:\n        dataset_items (list[dict]): list of objects from\n            dataset.__getitem__.\n    Returns:\n        result_batch (dict[Tensor]): dict, containing batch-version\n            of the tensors.\n    \"\"\"\n    result_batch = {\n        'mix_audio': [],\n        's1_audio': [],\n        's2_audio': [],\n        's1_mouth': [],\n        's2_mouth': [],\n        'mix_audio_length': [],\n        's1_audio_length': [],\n        's2_audio_length': []\n    }\n\n    for item in dataset_items:\n        result_batch['mix_audio'].append(item['mix_audio'].squeeze(0))\n        result_batch['s1_audio'].append(item['s1_audio'].squeeze(0))\n        result_batch['s2_audio'].append(item['s2_audio'].squeeze(0))\n        # result_batch['s1_mouth'].append(item['s1_mouth'])\n        # result_batch['s2_mouth'].append(item['s2_mouth'])\n        result_batch['mix_audio_length'].append(item['mix_audio_length'])\n        result_batch['s1_audio_length'].append(item['s1_audio_length'])\n        result_batch['s2_audio_length'].append(item['s2_audio_length'])\n\n    def pad_sequences(sequences):\n        return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n\n    result_batch['mix_audio'] = pad_sequences(result_batch['mix_audio'])\n    result_batch['s1_audio'] = pad_sequences(result_batch['s1_audio'])\n    result_batch['s2_audio'] = pad_sequences(result_batch['s2_audio'])\n\n    # result_batch['s1_mouth'] = torch.stack(result_batch['s1_mouth'])\n    # result_batch['s2_mouth'] = torch.stack(result_batch['s2_mouth'])\n    result_batch['mix_audio_length'] = torch.tensor(result_batch['mix_audio_length'])\n    result_batch['s1_audio_length'] = torch.tensor(result_batch['s1_audio_length'])\n    result_batch['s2_audio_length'] = torch.tensor(result_batch['s2_audio_length'])\n\n    return result_batch\n\n```\n\n\n",
        "eval_script": "## src/datasets/collate.py\nimport torch\n\ndef collate_fn(dataset_items: list[dict]):\n    \"\"\"\n    Collate and pad fields in the dataset items.\n    Converts individual items into a batch.\n\n    Args:\n        dataset_items (list[dict]): list of objects from\n            dataset.__getitem__.\n    Returns:\n        result_batch (dict[Tensor]): dict, containing batch-version\n            of the tensors.\n    \"\"\"\n    result_batch = {\n        'mix_audio': [],\n        's1_audio': [],\n        's2_audio': [],\n        's1_mouth': [],\n        's2_mouth': [],\n        'mix_audio_length': [],\n        's1_audio_length': [],\n        's2_audio_length': []\n    }\n\n    for item in dataset_items:\n        result_batch['mix_audio'].append(item['mix_audio'].squeeze(0))\n        result_batch['s1_audio'].append(item['s1_audio'].squeeze(0))\n        result_batch['s2_audio'].append(item['s2_audio'].squeeze(0))\n        # result_batch['s1_mouth'].append(item['s1_mouth'])\n        # result_batch['s2_mouth'].append(item['s2_mouth'])\n        result_batch['mix_audio_length'].append(item['mix_audio_length'])\n        result_batch['s1_audio_length'].append(item['s1_audio_length'])\n        result_batch['s2_audio_length'].append(item['s2_audio_length'])\n\n    def pad_sequences(sequences):\n        return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n\n    result_batch['mix_audio'] = pad_sequences(result_batch['mix_audio'])\n    result_batch['s1_audio'] = pad_sequences(result_batch['s1_audio'])\n    result_batch['s2_audio'] = pad_sequences(result_batch['s2_audio'])\n\n    # result_batch['s1_mouth'] = torch.stack(result_batch['s1_mouth'])\n    # result_batch['s2_mouth'] = torch.stack(result_batch['s2_mouth'])\n    result_batch['mix_audio_length'] = torch.tensor(result_batch['mix_audio_length'])\n    result_batch['s1_audio_length'] = torch.tensor(result_batch['s1_audio_length'])\n    result_batch['s2_audio_length'] = torch.tensor(result_batch['s2_audio_length'])\n\n    return result_batch\n\n\n\ndef test_collate_fn():\n    # Test input\n    dataset_items = [\n        {\n            'mix_audio': torch.tensor([1.0, 2.0]).unsqueeze(0),\n            's1_audio': torch.tensor([3.0, 4.0]).unsqueeze(0),\n            's2_audio': torch.tensor([5.0, 6.0]).unsqueeze(0),\n            'mix_audio_length': 2,\n            's1_audio_length': 2,\n            's2_audio_length': 2\n        },\n        {\n            'mix_audio': torch.tensor([7.0, 8.0, 9.0]).unsqueeze(0),\n            's1_audio': torch.tensor([10.0, 11.0]).unsqueeze(0),\n            's2_audio': torch.tensor([12.0, 13.0]).unsqueeze(0),\n            'mix_audio_length': 3,\n            's1_audio_length': 2,\n            's2_audio_length': 2\n        }\n    ]\n\n    # Expected results from both functions\n    result_old = collate_fn(dataset_items)\n    result_new = collate_fn_new_implementation(dataset_items)\n\n    # Assert the results match for both implementations\n    assert torch.equal(result_old['mix_audio'], result_new['mix_audio'])\n    assert torch.equal(result_old['s1_audio'], result_new['s1_audio'])\n    assert torch.equal(result_old['s2_audio'], result_new['s2_audio'])\n    assert torch.equal(result_old['mix_audio_length'], result_new['mix_audio_length'])\n    assert torch.equal(result_old['s1_audio_length'], result_new['s1_audio_length'])\n    assert torch.equal(result_old['s2_audio_length'], result_new['s2_audio_length'])\n\nif __name__ == \"__main__\":\n    test_collate_fn()"
    },
    {
        "func_name": "generate_id",
        "idx": "36",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/utils/init_utils.py",
        "orig_func": "def generate_id(length: int=8) -> str:\n    \"\"\"\n    Generate a random base-36 string of `length` digits.\n\n    Args:\n        length (int): length of a string.\n    Returns:\n        run_id (str): base-36 string with an experiment id.\n    \"\"\"\n    alphabet = string.ascii_lowercase + string.digits\n    return ''.join((secrets.choice(alphabet) for _ in range(length)))",
        "orig_context": "```python\n## src/utils/init_utils.py\nimport secrets\n\nimport string\n\ndef generate_id(length: int = 8) -> str:\n    \"\"\"\n    Generate a random base-36 string of `length` digits.\n\n    Args:\n        length (int): length of a string.\n    Returns:\n        run_id (str): base-36 string with an experiment id.\n    \"\"\"\n    # There are ~2.8T base-36 8-digit strings. If we generate 210k ids,\n    # we'll have a ~1% chance of collision.\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n\n```\n\n\n",
        "eval_script": "## src/utils/init_utils.py\nimport secrets\nimport string\nimport re\n\ndef generate_id(length: int = 8) -> str:\n    \"\"\"\n    Generate a random base-36 string of `length` digits.\n\n    Args:\n        length (int): length of a string.\n    Returns:\n        run_id (str): base-36 string with an experiment id.\n    \"\"\"\n    # There are ~2.8T base-36 8-digit strings. If we generate 210k ids,\n    # we'll have a ~1% chance of collision.\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n\n\n\ndef test_generate_id():\n    # Test that both functions return strings of the same length\n    assert len(generate_id(8)) == len(generate_id_new_implementation(8))\n    \n    # Test that both functions return strings containing only lowercase letters and digits\n    pattern = re.compile(\"^[a-z0-9]+$\")\n    assert pattern.match(generate_id(8)) is not None\n    assert pattern.match(generate_id_new_implementation(8)) is not None\n    \n    # Test that both functions return strings of different values in consecutive calls (checking randomness)\n    assert generate_id() != generate_id()\n    assert generate_id_new_implementation() != generate_id_new_implementation()\n\nif __name__ == \"__main__\":\n    test_generate_id()"
    },
    {
        "func_name": "resume_config",
        "idx": "37",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/utils/init_utils.py",
        "orig_func": "def resume_config(save_dir):\n    \"\"\"\n    Get run_id from resume config to continue logging\n    to the same experiment.\n\n    Args:\n        save_dir (Path): path to the directory with the run config.\n    Returns:\n        run_id (str): base-36 string with experiment id.\n    \"\"\"\n    saved_config = OmegaConf.load(save_dir / 'config.yaml')\n    run_id = saved_config.writer.run_id\n    print(f'Resuming training from run {run_id}...')\n    return run_id",
        "orig_context": "```python\n## src/utils/init_utils.py\nfrom omegaconf import OmegaConf\n\ndef resume_config(save_dir):\n    \"\"\"\n    Get run_id from resume config to continue logging\n    to the same experiment.\n\n    Args:\n        save_dir (Path): path to the directory with the run config.\n    Returns:\n        run_id (str): base-36 string with experiment id.\n    \"\"\"\n    saved_config = OmegaConf.load(save_dir / \"config.yaml\")\n    run_id = saved_config.writer.run_id\n    print(f\"Resuming training from run {run_id}...\")\n    return run_id\n\n```\n\n\n",
        "eval_script": "import os\nfrom omegaconf import OmegaConf\nfrom pathlib import Path\n\ndef resume_config(save_dir):\n    \"\"\"\n    Get run_id from resume config to continue logging\n    to the same experiment.\n\n    Args:\n        save_dir (Path): path to the directory with the run config.\n    Returns:\n        run_id (str): base-36 string with experiment id.\n    \"\"\"\n    saved_config = OmegaConf.load(save_dir / \"config.yaml\")\n    run_id = saved_config.writer.run_id\n    print(f\"Resuming training from run {run_id}...\")\n    return run_id\n\n\n\ndef test_resume_config():\n    \"\"\"Test to compare resume_config with resume_config_new_implementation.\"\"\"\n    # Create a mock setup for the test\n    mock_dir = Path(\"/home/user/tmp\")\n    os.makedirs(mock_dir, exist_ok=True)\n    mock_config_path = mock_dir / \"config.yaml\"\n    OmegaConf.save(config={\"writer\": {\"run_id\": \"abc123\"}}, f=mock_config_path)\n    \n    # Assertions\n    assert resume_config(mock_dir) == resume_config_new_implementation(mock_dir), \"Test case 1 failed\"\n    assert resume_config(mock_dir) == \"abc123\", \"Test case 2 failed\"\n    assert resume_config_new_implementation(mock_dir) == \"abc123\", \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_resume_config()"
    },
    {
        "func_name": "setup_saving_and_logging",
        "idx": "38",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/utils/init_utils.py",
        "orig_func": "def setup_saving_and_logging(config):\n    \"\"\"\n    Initialize the logger, writer, and saving directory.\n    The saving directory is defined by the run_name and save_dir\n    arguments of config.writer and config.trainer, respectfully.\n\n    Args:\n        config (DictConfig): hydra config for the current experiment.\n    Returns:\n        logger (Logger): logger that logs output.\n    \"\"\"\n    save_dir = ROOT_PATH / config.trainer.save_dir / config.writer.run_name\n    saving_init(save_dir, config)\n    if config.trainer.get('resume_from') is not None:\n        setup_logging(save_dir, append=True)\n    else:\n        setup_logging(save_dir, append=False)\n    logger = logging.getLogger('train')\n    logger.setLevel(logging.DEBUG)\n    return logger",
        "orig_context": "```python\n## src/utils/io_utils.py\nimport json\n\nfrom collections import OrderedDict\n\nfrom pathlib import Path\n\nROOT_PATH = Path(__file__).absolute().resolve().parent.parent.parent\n\ndef read_json(fname):\n    \"\"\"\n    Read the given json file.\n\n    Args:\n        fname (str): filename of the json file.\n    Returns:\n        json (list[OrderedDict] | OrderedDict): loaded json.\n    \"\"\"\n    fname = Path(fname)\n    with fname.open(\"rt\") as handle:\n        return json.load(handle, object_hook=OrderedDict)\n\n```\n\n\n```python\n## src/logger/logger.py\nimport logging.config\n\nfrom pathlib import Path\n\nfrom src.utils.io_utils import ROOT_PATH, read_json\n\ndef setup_logging(save_dir, log_config=None, default_level=logging.INFO, append=False):\n    \"\"\"\n    Setup logging configuration.\n\n    Args:\n        save_dir (Path): path to directory, where all logs and\n            checkpoints should be saved.\n        log_config (str | None): path to logger config. If none\n            'logger_config.json' from the src.logger directory is used.\n        default_level (int): default logging level.\n        append (bool): if True, append file instead of overwriting.\n    \"\"\"\n    if log_config is None:\n        log_config = str(ROOT_PATH / \"src\" / \"logger\" / \"logger_config.json\")\n    log_config = Path(log_config)\n    if log_config.is_file():\n        config = read_json(log_config)\n        # modify logging paths based on run config\n        for _, handler in config[\"handlers\"].items():\n            if \"filename\" in handler:\n                handler[\"filename\"] = str(save_dir / handler[\"filename\"])\n\n        logging.config.dictConfig(config)\n    else:\n        print(f\"Warning: logging configuration file is not found in {log_config}.\")\n        logging.basicConfig(level=default_level, filemode=\"a\" if append else \"w\")\n\n```\n\n\n```python\n## src/utils/init_utils.py\nimport logging\n\nimport secrets\n\nimport shutil\n\nimport string\n\nimport subprocess\n\nfrom omegaconf import OmegaConf\n\nfrom src.logger.logger import setup_logging\n\nfrom src.utils.io_utils import ROOT_PATH\n\ndef generate_id(length: int = 8) -> str:\n    \"\"\"\n    Generate a random base-36 string of `length` digits.\n\n    Args:\n        length (int): length of a string.\n    Returns:\n        run_id (str): base-36 string with an experiment id.\n    \"\"\"\n    # There are ~2.8T base-36 8-digit strings. If we generate 210k ids,\n    # we'll have a ~1% chance of collision.\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n\ndef log_git_commit_and_patch(save_dir):\n    \"\"\"\n    Log current git commit and patch to save dir.\n    Improves reproducibility by allowing to run the same code version:\n        git checkout commit_hash_from_commit_path\n        git apply patch_path\n\n    If you created new files and want to have them in patch,\n    stage them via git add before running the script.\n\n    Patch can be applied via the following command:\n        git apply patch_path\n\n    Args:\n        save_dir (Path): directory to save patch and commit in\n    \"\"\"\n    print(\"Logging git commit and patch...\")\n    commit_path = save_dir / \"git_commit.txt\"\n    patch_path = save_dir / \"git_diff.patch\"\n    with commit_path.open(\"w\") as f:\n        subprocess.call([\"git\", \"rev-parse\", \"HEAD\"], stdout=f)\n    with patch_path.open(\"w\") as f:\n        subprocess.call([\"git\", \"diff\", \"HEAD\"], stdout=f)\n\ndef resume_config(save_dir):\n    \"\"\"\n    Get run_id from resume config to continue logging\n    to the same experiment.\n\n    Args:\n        save_dir (Path): path to the directory with the run config.\n    Returns:\n        run_id (str): base-36 string with experiment id.\n    \"\"\"\n    saved_config = OmegaConf.load(save_dir / \"config.yaml\")\n    run_id = saved_config.writer.run_id\n    print(f\"Resuming training from run {run_id}...\")\n    return run_id\n\ndef saving_init(save_dir, config):\n    \"\"\"\n    Initialize saving by getting run_id.\n\n    Args:\n        save_dir (Path): path to the directory to log everything:\n            logs, checkpoints, config, etc.\n        config (DictConfig): hydra config for the current experiment.\n    \"\"\"\n    run_id = None\n\n    if save_dir.exists():\n        if config.trainer.get(\"resume_from\") is not None:\n            run_id = resume_config(save_dir)\n        elif config.trainer.override:\n            print(f\"Overriding save directory '{save_dir}'...\")\n            shutil.rmtree(str(save_dir))\n        elif not config.trainer.override:\n            raise ValueError(\n                \"Save directory exists. Change the name or set override=True\"\n            )\n\n    save_dir.mkdir(exist_ok=True, parents=True)\n\n    if run_id is None:\n        run_id = generate_id(length=config.writer.id_length)\n\n    OmegaConf.set_struct(config, False)\n    config.writer.run_id = run_id\n    OmegaConf.set_struct(config, True)\n\n    OmegaConf.save(config, save_dir / \"config.yaml\")\n\n    log_git_commit_and_patch(save_dir)\n\ndef setup_saving_and_logging(config):\n    \"\"\"\n    Initialize the logger, writer, and saving directory.\n    The saving directory is defined by the run_name and save_dir\n    arguments of config.writer and config.trainer, respectfully.\n\n    Args:\n        config (DictConfig): hydra config for the current experiment.\n    Returns:\n        logger (Logger): logger that logs output.\n    \"\"\"\n    save_dir = ROOT_PATH / config.trainer.save_dir / config.writer.run_name\n    saving_init(save_dir, config)\n\n    if config.trainer.get(\"resume_from\") is not None:\n        setup_logging(save_dir, append=True)\n    else:\n        setup_logging(save_dir, append=False)\n    logger = logging.getLogger(\"train\")\n    logger.setLevel(logging.DEBUG)\n\n    return logger\n\n```\n\n\n",
        "eval_script": "import logging\nimport secrets\nimport shutil\nimport string\nimport subprocess\nfrom pathlib import Path\nfrom collections import OrderedDict\nimport json\nimport os\nfrom omegaconf import OmegaConf, DictConfig\n\nROOT_PATH = Path('/home/user/tmp')  # Mock the ROOT_PATH as necessary\n\ndef read_json(fname):\n    \"\"\"\n    Read the given json file.\n    Args:\n        fname (str): filename of the json file.\n    Returns:\n        json (list[OrderedDict] | OrderedDict): loaded json.\n    \"\"\"\n    fname = Path(fname)\n    with fname.open(\"rt\") as handle:\n        return json.load(handle, object_hook=OrderedDict)\n\n\ndef setup_logging(save_dir, log_config=None, default_level=logging.INFO, append=False):\n    \"\"\"\n    Setup logging configuration.\n    Args:\n        save_dir (Path): path to directory, where all logs and\n            checkpoints should be saved.\n        log_config (str | None): path to logger config. If none\n            'logger_config.json' from the src.logger directory is used.\n        default_level (int): default logging level.\n        append (bool): if True, append file instead of overwriting.\n    \"\"\"\n    if log_config is None:\n        log_config = str(ROOT_PATH / \"src\" / \"logger\" / \"logger_config.json\")\n    log_config = Path(log_config)\n    if log_config.is_file():\n        config = read_json(log_config)\n        # modify logging paths based on run config\n        for _, handler in config[\"handlers\"].items():\n            if \"filename\" in handler:\n                handler[\"filename\"] = str(save_dir / handler[\"filename\"])\n        logging.config.dictConfig(config)\n    else:\n        print(f\"Warning: logging configuration file is not found in {log_config}.\")\n        logging.basicConfig(level=default_level, filemode=\"a\" if append else \"w\")\n\ndef generate_id(length: int = 8) -> str:\n    \"\"\"\n    Generate a random base-36 string of `length` digits.\n    Args:\n        length (int): length of a string.\n    Returns:\n        run_id (str): base-36 string with an experiment id.\n    \"\"\"\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n\ndef log_git_commit_and_patch(save_dir):\n    \"\"\"\n    Log current git commit and patch to save dir.\n    Args:\n        save_dir (Path): directory to save patch and commit in\n    \"\"\"\n    try:\n        print(\"Logging git commit and patch...\")\n        commit_path = save_dir / \"git_commit.txt\"\n        patch_path = save_dir / \"git_diff.patch\"\n        with commit_path.open(\"w\") as f:\n            subprocess.check_call([\"git\", \"rev-parse\", \"HEAD\"], stdout=f)\n        with patch_path.open(\"w\") as f:\n            subprocess.check_call([\"git\", \"diff\", \"HEAD\"], stdout=f)\n    except subprocess.CalledProcessError:\n        print(\"Failed to log git commit and patch. Not a git repository.\")\n\ndef resume_config(save_dir):\n    \"\"\"\n    Get run_id from resume config to continue logging\n    to the same experiment.\n    Args:\n        save_dir (Path): path to the directory with the run config.\n    Returns:\n        run_id (str): base-36 string with experiment id.\n    \"\"\"\n    saved_config = OmegaConf.load(save_dir / \"config.yaml\")\n    run_id = saved_config.writer.run_id\n    print(f\"Resuming training from run {run_id}...\")\n    return run_id\n\ndef saving_init(save_dir, config):\n    \"\"\"\n    Initialize saving by getting run_id.\n    Args:\n        save_dir (Path): path to the directory to log everything:\n            logs, checkpoints, config, etc.\n        config (DictConfig): hydra config for the current experiment.\n    \"\"\"\n    run_id = None\n\n    if save_dir.exists():\n        if config.trainer.get(\"resume_from\") is not None:\n            run_id = resume_config(save_dir)\n        elif config.trainer.override:\n            print(f\"Overriding save directory '{save_dir}'...\")\n            shutil.rmtree(str(save_dir))\n        elif not config.trainer.override:\n            raise ValueError(\n                \"Save directory exists. Change the name or set override=True\"\n            )\n\n    save_dir.mkdir(exist_ok=True, parents=True)\n\n    if run_id is None:\n        run_id = generate_id(length=config.writer.id_length)\n\n    OmegaConf.set_struct(config, False)\n    config.writer.run_id = run_id\n    OmegaConf.set_struct(config, True)\n\n    OmegaConf.save(config, save_dir / \"config.yaml\")\n\n    log_git_commit_and_patch(save_dir)\n\ndef setup_saving_and_logging(config):\n    \"\"\"\n    Initialize the logger, writer, and saving directory.\n    The saving directory is defined by the run_name and save_dir\n    arguments of config.writer and config.trainer, respectfully.\n    Args:\n        config (DictConfig): hydra config for the current experiment.\n    Returns:\n        logger (Logger): logger that logs output.\n    \"\"\"\n    save_dir = ROOT_PATH / config.trainer.save_dir / config.writer.run_name\n    saving_init(save_dir, config)\n\n    if config.trainer.get(\"resume_from\") is not None:\n        setup_logging(save_dir, append=True)\n    else:\n        setup_logging(save_dir, append=False)\n    logger = logging.getLogger(\"train\")\n    logger.setLevel(logging.DEBUG)\n\n    return logger\n\n\ndef test_setup_saving_and_logging():\n    config = OmegaConf.create({\n        \"trainer\": {\n            \"save_dir\": \"mock_save_dir\",\n            \"resume_from\": None,\n            \"override\": True\n        },\n        \"writer\": {\n            \"run_name\": \"test_run\",\n            \"id_length\": 8\n        }\n    })\n\n    # Make sure temporary directory exists for saving experiment data\n    os.makedirs(ROOT_PATH / config.trainer.save_dir / config.writer.run_name, exist_ok=True)\n\n    logger_orig = setup_saving_and_logging(config)\n    logger_new = setup_saving_and_logging_new_implementation(config)\n\n    assert logger_orig.name == logger_new.name, \"Logger names do not match.\"\n    assert logger_orig.level == logger_new.level, \"Logger levels do not match.\"\n    # Check if the configurations are identical    \n    assert OmegaConf.to_container(config) == OmegaConf.to_container(config), \"Configurations do not match.\"\n\nif __name__ == \"__main__\":\n    test_setup_saving_and_logging()"
    },
    {
        "func_name": "compute_metric",
        "idx": "39",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/metrics/utils.py",
        "orig_func": "def compute_metric(metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio):\n    m_s1_s1 = metric(s1_audio, speaker_1)\n    m_s1_s2 = metric(s1_audio, speaker_2)\n    m_s2_s1 = metric(s2_audio, speaker_1)\n    m_s2_s2 = metric(s2_audio, speaker_2)\n    m_mix_s1 = metric(s1_audio, mix_audio)\n    m_mix_s2 = metric(s2_audio, mix_audio)\n    mi_s1 = m_s1_s1 - m_mix_s1\n    mi_s2 = m_s2_s2 - m_mix_s2\n    mean_mi_perm1 = (mi_s1 + mi_s2) / 2\n    mi_perm2_s1 = m_s1_s2 - m_mix_s1\n    mi_perm2_s2 = m_s2_s1 - m_mix_s2\n    mean_mi_perm2 = (mi_perm2_s1 + mi_perm2_s2) / 2\n    result_metric = torch.maximum(mean_mi_perm1, mean_mi_perm2)\n    return result_metric.mean()",
        "orig_context": "```python\n## src/metrics/utils.py\nimport torch\n\ndef compute_metric(metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio):\n    m_s1_s1 = metric(s1_audio, speaker_1)\n    m_s1_s2 = metric(s1_audio, speaker_2)\n    m_s2_s1 = metric(s2_audio, speaker_1)\n    m_s2_s2 = metric(s2_audio, speaker_2)\n\n    m_mix_s1 = metric(s1_audio, mix_audio)\n    m_mix_s2 = metric(s2_audio, mix_audio)\n\n    mi_s1 = m_s1_s1 - m_mix_s1\n    mi_s2 = m_s2_s2 - m_mix_s2\n    mean_mi_perm1 = (mi_s1 + mi_s2) / 2\n\n    mi_perm2_s1 = m_s1_s2 - m_mix_s1\n    mi_perm2_s2 = m_s2_s1 - m_mix_s2\n    mean_mi_perm2 = (mi_perm2_s1 + mi_perm2_s2) / 2\n\n    result_metric = torch.maximum(mean_mi_perm1, mean_mi_perm2)\n    return result_metric.mean()\n\n```\n\n\n",
        "eval_script": "import torch\n\ndef compute_metric(metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio):\n    m_s1_s1 = metric(s1_audio, speaker_1)\n    m_s1_s2 = metric(s1_audio, speaker_2)\n    m_s2_s1 = metric(s2_audio, speaker_1)\n    m_s2_s2 = metric(s2_audio, speaker_2)\n\n    m_mix_s1 = metric(s1_audio, mix_audio)\n    m_mix_s2 = metric(s2_audio, mix_audio)\n\n    mi_s1 = m_s1_s1 - m_mix_s1\n    mi_s2 = m_s2_s2 - m_mix_s2\n    mean_mi_perm1 = (mi_s1 + mi_s2) / 2\n\n    mi_perm2_s1 = m_s1_s2 - m_mix_s1\n    mi_perm2_s2 = m_s2_s1 - m_mix_s2\n    mean_mi_perm2 = (mi_perm2_s1 + mi_perm2_s2) / 2\n\n    result_metric = torch.maximum(mean_mi_perm1, mean_mi_perm2)\n    return result_metric.mean()\n\n# Dummy implementation for demonstration; replace with actual new implementation.\n\n\n# Test function\ndef test_compute_metric():\n    # Dummy metric for testing purposes\n    def dummy_metric(x, y):\n        return (x - y).abs().sum()\n\n    # Test case 1\n    s1_audio = torch.tensor([1.0, 2.0, 3.0])\n    s2_audio = torch.tensor([1.0, 0.0, 1.0])\n    speaker_1 = torch.tensor([1.0, 1.0, 1.0])\n    speaker_2 = torch.tensor([0.0, 0.0, 0.0])\n    mix_audio = torch.tensor([0.5, 1.0, 1.5])\n    assert compute_metric(dummy_metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio) == compute_metric_new_implementation(dummy_metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio)\n\n    # Test case 2\n    s1_audio = torch.tensor([0.0, 0.0, 0.0])\n    s2_audio = torch.tensor([1.0, 1.0, 1.0])\n    speaker_1 = torch.tensor([0.0, -1.0, 2.0])\n    speaker_2 = torch.tensor([2.0, 1.0, 0.0])\n    mix_audio = torch.tensor([0.0, 0.0, 0.0])\n    assert compute_metric(dummy_metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio) == compute_metric_new_implementation(dummy_metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio)\n\n    # Test case 3\n    s1_audio = torch.tensor([-1.0, -2.0, -3.0])\n    s2_audio = torch.tensor([4.0, 5.0, 6.0])\n    speaker_1 = torch.tensor([7.0, 8.0, 9.0])\n    speaker_2 = torch.tensor([-1.0, -1.0, -1.0])\n    mix_audio = torch.tensor([1.0, 2.0, 3.0])\n    assert compute_metric(dummy_metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio) == compute_metric_new_implementation(dummy_metric, s1_audio, s2_audio, speaker_1, speaker_2, mix_audio)\n\nif __name__ == \"__main__\":\n    test_compute_metric()"
    },
    {
        "func_name": "WandBWriter.add_audio",
        "idx": "41",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/logger/wandb.py",
        "orig_func": "def add_audio(self, audio_name, audio, sample_rate=None):\n    \"\"\"\n        Log an audio to the experiment tracker.\n\n        Args:\n            audio_name (str): name of the audio to use in the tracker.\n            audio (Path | ndarray): audio in the WandB-friendly format.\n            sample_rate (int): audio sample rate.\n        \"\"\"\n    audio = audio.detach().cpu().numpy().T\n    self.wandb.log({self._object_name(audio_name): self.wandb.Audio(audio, sample_rate=sample_rate)}, step=self.step)",
        "orig_context": "```python\n## src/logger/wandb.py\nfrom datetime import datetime\n\nimport numpy as np\n\nimport pandas as pd\n\nclass WandBWriter:\n    \"\"\"\n    Class for experiment tracking via WandB.\n\n    See https://docs.wandb.ai/.\n    \"\"\"\n\n    def __init__(\n        self,\n        logger,\n        project_config,\n        project_name,\n        entity=None,\n        run_id=None,\n        run_name=None,\n        mode=\"online\",\n        **kwargs,\n    ):\n        \"\"\"\n        API key is expected to be provided by the user in the terminal.\n\n        Args:\n            logger (Logger): logger that logs output.\n            project_config (dict): config for the current experiment.\n            project_name (str): name of the project inside experiment tracker.\n            entity (str | None): name of the entity inside experiment\n                tracker. Used if you work in a team.\n            run_id (str | None): the id of the current run.\n            run_name (str | None): the name of the run. If None, random name\n                is given.\n            mode (str): if online, log data to the remote server. If\n                offline, log locally.\n        \"\"\"\n        try:\n            import wandb\n\n            wandb.login()\n\n            self.run_id = run_id\n\n            wandb.init(\n                project=project_name,\n                entity=entity,\n                config=project_config,\n                name=run_name,\n                resume=\"allow\",  # resume the run if run_id existed\n                id=self.run_id,\n                mode=mode,\n                save_code=kwargs.get(\"save_code\", False),\n            )\n            self.wandb = wandb\n\n        except ImportError:\n            logger.warning(\"For use wandb install it via \\n\\t pip install wandb\")\n\n        self.step = 0\n        # the mode is usually equal to the current partition name\n        # used to separate Partition1 and Partition2 metrics\n        self.mode = \"\"\n        self.timer = datetime.now()\n\n    def set_step(self, step, mode=\"train\"):\n        \"\"\"\n        Define current step and mode for the tracker.\n\n        Calculates the difference between method calls to monitor\n        training/evaluation speed.\n\n        Args:\n            step (int): current step.\n            mode (str): current mode (partition name).\n        \"\"\"\n        self.mode = mode\n        previous_step = self.step\n        self.step = step\n        if step == 0:\n            self.timer = datetime.now()\n        else:\n            duration = datetime.now() - self.timer\n            self.add_scalar(\n                \"steps_per_sec\", (self.step - previous_step) / duration.total_seconds()\n            )\n            self.timer = datetime.now()\n\n    def _object_name(self, object_name):\n        \"\"\"\n        Update object_name (scalar, image, etc.) with the\n        current mode (partition name). Used to separate metrics\n        from different partitions.\n\n        Args:\n            object_name (str): current object name.\n        Returns:\n            object_name (str): updated object name.\n        \"\"\"\n        return f\"{object_name}_{self.mode}\"\n\n    def add_checkpoint(self, checkpoint_path, save_dir):\n        \"\"\"\n        Log checkpoints to the experiment tracker.\n\n        The checkpoints will be available in the files section\n        inside the run_name dir.\n\n        Args:\n            checkpoint_path (str): path to the checkpoint file.\n            save_dir (str): path to the dir, where checkpoint is saved.\n        \"\"\"\n        self.wandb.save(checkpoint_path, base_path=save_dir)\n\n    def add_scalar(self, scalar_name, scalar):\n        \"\"\"\n        Log a scalar to the experiment tracker.\n\n        Args:\n            scalar_name (str): name of the scalar to use in the tracker.\n            scalar (float): value of the scalar.\n        \"\"\"\n        self.wandb.log(\n            {\n                self._object_name(scalar_name): scalar,\n            },\n            step=self.step,\n        )\n\n    def add_scalars(self, scalars):\n        \"\"\"\n        Log several scalars to the experiment tracker.\n\n        Args:\n            scalars (dict): dict, containing scalar name and value.\n        \"\"\"\n        self.wandb.log(\n            {\n                self._object_name(scalar_name): scalar\n                for scalar_name, scalar in scalars.items()\n            },\n            step=self.step,\n        )\n\n    def add_image(self, image_name, image):\n        \"\"\"\n        Log an image to the experiment tracker.\n\n        Args:\n            image_name (str): name of the image to use in the tracker.\n            image (Path | ndarray | Image): image in the WandB-friendly\n                format.\n        \"\"\"\n        self.wandb.log(\n            {self._object_name(image_name): self.wandb.Image(image)}, step=self.step\n        )\n\n    def add_audio(self, audio_name, audio, sample_rate=None):\n        \"\"\"\n        Log an audio to the experiment tracker.\n\n        Args:\n            audio_name (str): name of the audio to use in the tracker.\n            audio (Path | ndarray): audio in the WandB-friendly format.\n            sample_rate (int): audio sample rate.\n        \"\"\"\n        audio = audio.detach().cpu().numpy().T\n        self.wandb.log(\n            {\n                self._object_name(audio_name): self.wandb.Audio(\n                    audio, sample_rate=sample_rate\n                )\n            },\n            step=self.step,\n        )\n\n    def add_text(self, text_name, text):\n        \"\"\"\n        Log text to the experiment tracker.\n\n        Args:\n            text_name (str): name of the text to use in the tracker.\n            text (str): text content.\n        \"\"\"\n        self.wandb.log(\n            {self._object_name(text_name): self.wandb.Html(text)}, step=self.step\n        )\n\n    def add_histogram(self, hist_name, values_for_hist, bins=None):\n        \"\"\"\n        Log histogram to the experiment tracker.\n\n        Args:\n            hist_name (str): name of the histogram to use in the tracker.\n            values_for_hist (Tensor): array of values to calculate\n                histogram of.\n            bins (int | str): the definition of bins for the histogram.\n        \"\"\"\n        values_for_hist = values_for_hist.detach().cpu().numpy()\n        np_hist = np.histogram(values_for_hist, bins=bins)\n        if np_hist[0].shape[0] > 512:\n            np_hist = np.histogram(values_for_hist, bins=512)\n\n        hist = self.wandb.Histogram(np_histogram=np_hist)\n\n        self.wandb.log({self._object_name(hist_name): hist}, step=self.step)\n\n    def add_table(self, table_name, table: pd.DataFrame):\n        \"\"\"\n        Log table to the experiment tracker.\n\n        Args:\n            table_name (str): name of the table to use in the tracker.\n            table (DataFrame): table content.\n        \"\"\"\n        self.wandb.log(\n            {self._object_name(table_name): self.wandb.Table(dataframe=table)},\n            step=self.step,\n        )\n\n    def add_images(self, image_names, images):\n        raise NotImplementedError()\n\n    def add_pr_curve(self, curve_name, curve):\n        raise NotImplementedError()\n\n    def add_embedding(self, embedding_name, embedding):\n        raise NotImplementedError()\n\n```\n\n\n",
        "eval_script": "## src/logger/wandb.py\nfrom datetime import datetime\n\nimport numpy as np\n\nimport pandas as pd\n\nclass MockWandB:\n    class Audio:\n        def __init__(self, data, sample_rate=None):\n            self.data = data\n            self.sample_rate = sample_rate\n\n    class Image:\n        def __init__(self, data):\n            self.data = data\n\n    class Html:\n        def __init__(self, data):\n            self.data = data\n\n    class Histogram:\n        def __init__(self, np_histogram):\n            self.np_histogram = np_histogram\n\n    class Table:\n        def __init__(self, dataframe):\n            self.dataframe = dataframe\n\n    @staticmethod\n    def log(data, step=None):\n        print(f\"Log data: {data} at step {step}\")\n\n    @staticmethod\n    def save(checkpoint_path, base_path):\n        print(f\"Save checkpoint from {checkpoint_path} to {base_path}\")\n\n    @staticmethod\n    def init(*args, **kwargs):\n        print(\"WandB initialized with:\", kwargs)\n\n    @staticmethod\n    def login():\n        print(\"Mock WandB login\")\n\nclass WandBWriter:\n    \"\"\"\n    Class for experiment tracking via WandB.\n\n    See https://docs.wandb.ai/.\n    \"\"\"\n\n    def __init__(\n        self,\n        logger,\n        project_config,\n        project_name,\n        entity=None,\n        run_id=None,\n        run_name=None,\n        mode=\"online\",\n        **kwargs,\n    ):\n        \"\"\"\n        API key is expected to be provided by the user in the terminal.\n\n        Args:\n            logger (Logger): logger that logs output.\n            project_config (dict): config for the current experiment.\n            project_name (str): name of the project inside experiment tracker.\n            entity (str | None): name of the entity inside experiment\n                tracker. Used if you work in a team.\n            run_id (str | None): the id of the current run.\n            run_name (str | None): the name of the run. If None, random name\n                is given.\n            mode (str): if online, log data to the remote server. If\n                offline, log locally.\n        \"\"\"\n        if logger is None:\n            self.wandb = MockWandB\n        else:\n            try:\n                import wandb\n\n                wandb.login()\n\n                self.wandb = wandb\n\n            except ImportError:\n                logger.warning(\"For use wandb install it via \\n\\t pip install wandb\")\n                self.wandb = MockWandB\n\n        self.run_id = run_id\n\n        self.wandb.init(\n            project=project_name,\n            entity=entity,\n            config=project_config,\n            name=run_name,\n            resume=\"allow\",  # resume the run if run_id existed\n            id=self.run_id,\n            mode=mode,\n            save_code=kwargs.get(\"save_code\", False),\n        )\n\n        self.step = 0\n        # the mode is usually equal to the current partition name\n        # used to separate Partition1 and Partition2 metrics\n        self.mode = \"\"\n        self.timer = datetime.now()\n\n    def set_step(self, step, mode=\"train\"):\n        \"\"\"\n        Define current step and mode for the tracker.\n\n        Calculates the difference between method calls to monitor\n        training/evaluation speed.\n\n        Args:\n            step (int): current step.\n            mode (str): current mode (partition name).\n        \"\"\"\n        self.mode = mode\n        previous_step = self.step\n        self.step = step\n        if step == 0:\n            self.timer = datetime.now()\n        else:\n            duration = datetime.now() - self.timer\n            self.add_scalar(\n                \"steps_per_sec\", (self.step - previous_step) / duration.total_seconds()\n            )\n            self.timer = datetime.now()\n\n    def _object_name(self, object_name):\n        \"\"\"\n        Update object_name (scalar, image, etc.) with the\n        current mode (partition name). Used to separate metrics\n        from different partitions.\n\n        Args:\n            object_name (str): current object name.\n        Returns:\n            object_name (str): updated object name.\n        \"\"\"\n        return f\"{object_name}_{self.mode}\"\n\n    def add_checkpoint(self, checkpoint_path, save_dir):\n        \"\"\"\n        Log checkpoints to the experiment tracker.\n\n        The checkpoints will be available in the files section\n        inside the run_name dir.\n\n        Args:\n            checkpoint_path (str): path to the checkpoint file.\n            save_dir (str): path to the dir, where checkpoint is saved.\n        \"\"\"\n        self.wandb.save(checkpoint_path, base_path=save_dir)\n\n    def add_scalar(self, scalar_name, scalar):\n        \"\"\"\n        Log a scalar to the experiment tracker.\n\n        Args:\n            scalar_name (str): name of the scalar to use in the tracker.\n            scalar (float): value of the scalar.\n        \"\"\"\n        self.wandb.log(\n            {\n                self._object_name(scalar_name): scalar,\n            },\n            step=self.step,\n        )\n\n    def add_scalars(self, scalars):\n        \"\"\"\n        Log several scalars to the experiment tracker.\n\n        Args:\n            scalars (dict): dict, containing scalar name and value.\n        \"\"\"\n        self.wandb.log(\n            {\n                self._object_name(scalar_name): scalar\n                for scalar_name, scalar in scalars.items()\n            },\n            step=self.step,\n        )\n\n    def add_image(self, image_name, image):\n        \"\"\"\n        Log an image to the experiment tracker.\n\n        Args:\n            image_name (str): name of the image to use in the tracker.\n            image (Path | ndarray | Image): image in the WandB-friendly\n                format.\n        \"\"\"\n        self.wandb.log(\n            {self._object_name(image_name): self.wandb.Image(image)}, step=self.step\n        )\n\n    def add_audio(self, audio_name, audio, sample_rate=None):\n        \"\"\"\n        Log an audio to the experiment tracker.\n\n        Args:\n            audio_name (str): name of the audio to use in the tracker.\n            audio (Path | ndarray): audio in the WandB-friendly format.\n            sample_rate (int): audio sample rate.\n        \"\"\"\n        if hasattr(audio, \"detach\"):\n            audio = audio.detach().cpu().numpy().T\n        self.wandb.log(\n            {\n                self._object_name(audio_name): self.wandb.Audio(\n                    audio, sample_rate=sample_rate\n                )\n            },\n            step=self.step,\n        )\n\n\n    def add_text(self, text_name, text):\n        \"\"\"\n        Log text to the experiment tracker.\n\n        Args:\n            text_name (str): name of the text to use in the tracker.\n            text (str): text content.\n        \"\"\"\n        self.wandb.log(\n            {self._object_name(text_name): self.wandb.Html(text)}, step=self.step\n        )\n\n    def add_histogram(self, hist_name, values_for_hist, bins=None):\n        \"\"\"\n        Log histogram to the experiment tracker.\n\n        Args:\n            hist_name (str): name of the histogram to use in the tracker.\n            values_for_hist (Tensor): array of values to calculate\n                histogram of.\n            bins (int | str): the definition of bins for the histogram.\n        \"\"\"\n        values_for_hist = values_for_hist.detach().cpu().numpy()\n        np_hist = np.histogram(values_for_hist, bins=bins)\n        if np_hist[0].shape[0] > 512:\n            np_hist = np.histogram(values_for_hist, bins=512)\n\n        hist = self.wandb.Histogram(np_histogram=np_hist)\n\n        self.wandb.log({self._object_name(hist_name): hist}, step=self.step)\n\n    def add_table(self, table_name, table: pd.DataFrame):\n        \"\"\"\n        Log table to the experiment tracker.\n\n        Args:\n            table_name (str): name of the table to use in the tracker.\n            table (DataFrame): table content.\n        \"\"\"\n        self.wandb.log(\n            {self._object_name(table_name): self.wandb.Table(dataframe=table)},\n            step=self.step,\n        )\n\n    def add_images(self, image_names, images):\n        raise NotImplementedError()\n\n    def add_pr_curve(self, curve_name, curve):\n        raise NotImplementedError()\n\n    def add_embedding(self, embedding_name, embedding):\n        raise NotImplementedError()\n\n\ndef test_add_audio():\n    # Create instances of WandBWriter and MockWandB for testing\n    logger = None  # Assuming a logger is passed; mocked here for the example\n    config = {}\n    writer = WandBWriter(logger, config, \"test_project\")\n\n    # Mock audio data\n    audio_data = np.array([0.1, 0.2, 0.3, 0.4])\n    sample_rate = 44100\n    \n    # Log with the original implementation\n    writer.add_audio(\"test_audio\", audio_data, sample_rate=sample_rate)\n    \n    # Log with the new implementation\n    writer.add_audio_new_implementation(\"test_audio\", audio_data, sample_rate=sample_rate)\n\n    # Since we only mock log functionality, we simulate the comparison\n    assert writer._object_name(\"test_audio\") == writer._object_name(\"test_audio\")\n    assert np.array_equal(writer.wandb.Audio(audio_data, sample_rate).data, audio_data)\n    assert writer.wandb.Audio(audio_data, sample_rate).sample_rate == sample_rate\n\nif __name__ == \"__main__\":\n    test_add_audio()"
    },
    {
        "func_name": "BaseDataset._assert_index_is_valid",
        "idx": "43",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/datasets/base_dataset.py",
        "orig_func": "@staticmethod\ndef _assert_index_is_valid(index):\n    \"\"\"\n        Check the structure of the index and ensure it satisfies the desired\n        conditions.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n        \"\"\"\n    for entry in index:\n        assert 'path' in entry, \"Each dataset item should include field 'path' - path to audio file.\"\n        assert 'text' in entry, \"Each dataset item should include field 'text' - object ground-truth transcription.\"\n        assert 'audio_len' in entry, \"Each dataset item should include field 'audio_len' - length of the audio.\"",
        "orig_context": "```python\n## src/text_encoder/ctc_text_encoder.py\nimport re\n\nfrom string import ascii_lowercase\n\nimport torch\n\nclass CTCTextEncoder:\n    EMPTY_TOK = \"\"\n\n    def __init__(self, alphabet=None, **kwargs):\n        \"\"\"\n        Args:\n            alphabet (list): alphabet for language. If None, it will be\n                set to ascii\n        \"\"\"\n\n        if alphabet is None:\n            alphabet = list(ascii_lowercase + \" \")\n\n        self.alphabet = alphabet\n        self.vocab = [self.EMPTY_TOK] + list(self.alphabet)\n\n        self.ind2char = dict(enumerate(self.vocab))\n        self.char2ind = {v: k for k, v in self.ind2char.items()}\n\n    def __len__(self):\n        return len(self.vocab)\n\n    def __getitem__(self, item: int):\n        assert type(item) is int\n        return self.ind2char[item]\n\n    def encode(self, text) -> torch.Tensor:\n        text = self.normalize_text(text)\n        try:\n            return torch.Tensor([self.char2ind[char] for char in text]).unsqueeze(0)\n        except KeyError:\n            unknown_chars = set([char for char in text if char not in self.char2ind])\n            raise Exception(\n                f\"Can't encode text '{text}'. Unknown chars: '{' '.join(unknown_chars)}'\"\n            )\n\n    def decode(self, inds) -> str:\n        \"\"\"\n        Raw decoding without CTC.\n        Used to validate the CTC decoding implementation.\n\n        Args:\n            inds (list): list of tokens.\n        Returns:\n            raw_text (str): raw text with empty tokens and repetitions.\n        \"\"\"\n        return \"\".join([self.ind2char[int(ind)] for ind in inds]).strip()\n\n    def ctc_decode(self, inds) -> str:\n        pass  # TODO\n\n    @staticmethod\n    def normalize_text(text: str):\n        text = text.lower()\n        text = re.sub(r\"[^a-z ]\", \"\", text)\n        return text\n\n```\n\n\n```python\n## src/text_encoder/__init__.py\nfrom src.text_encoder.ctc_text_encoder import CTCTextEncoder\n\n```\n\n\n```python\n## src/datasets/base_dataset.py\nimport logging\n\nimport random\n\nimport numpy as np\n\nimport torchaudio\n\nfrom torch.utils.data import Dataset\n\nfrom src.text_encoder import CTCTextEncoder\n\nlogger = logging.getLogger(__name__)\n\nclass BaseDataset(Dataset):\n    \"\"\"\n    Base class for the datasets.\n\n    Given a proper index (list[dict]), allows to process different datasets\n    for the same task in the identical manner. Therefore, to work with\n    several datasets, the user only have to define index in a nested class.\n    \"\"\"\n\n    def __init__(\n        self,\n        index,\n        text_encoder=None,\n        target_sr=16000,\n        limit=None,\n        max_audio_length=None,\n        max_text_length=None,\n        shuffle_index=False,\n        instance_transforms=None,\n    ):\n        \"\"\"\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            text_encoder (CTCTextEncoder): text encoder.\n            target_sr (int): supported sample rate.\n            limit (int | None): if not None, limit the total number of elements\n                in the dataset to 'limit' elements.\n            max_audio_length (int): maximum allowed audio length.\n            max_test_length (int): maximum allowed text length.\n            shuffle_index (bool): if True, shuffle the index. Uses python\n                random package with seed 42.\n            instance_transforms (dict[Callable] | None): transforms that\n                should be applied on the instance. Depend on the\n                tensor name.\n        \"\"\"\n        # self._assert_index_is_valid(index)\n\n        index = self._filter_records_from_dataset(\n            index, max_audio_length, max_text_length\n        )\n        index = self._shuffle_and_limit_index(index, limit, shuffle_index)\n\n        # if not shuffle_index:\n        #     index = self._sort_index(index)\n\n        self._index: list[dict] = index\n\n        self.text_encoder = text_encoder\n        self.target_sr = target_sr\n        self.instance_transforms = instance_transforms\n\n    def __getitem__(self, ind):\n        \"\"\"\n        Get element from the index, preprocess it, and combine it\n        into a dict.\n\n        Notice that the choice of key names is defined by the template user.\n        However, they should be consistent across dataset getitem, collate_fn,\n        loss_function forward method, and model forward method.\n\n        Args:\n            ind (int): index in the self.index list.\n        Returns:\n            instance_data (dict): dict, containing instance\n                (a single dataset element).\n        \"\"\"\n        data_dict = self._index[ind]\n\n        mix_path = data_dict['mix_path']\n        s1_path = data_dict['s1_path']\n        s2_path = data_dict['s2_path']\n        s1_mouth_path = data_dict['s1_mouth_path']\n        s2_mouth_path = data_dict['s2_mouth_path']\n        mix_audio_length = data_dict['mix_audio_length']\n        s1_audio_length = data_dict['s1_audio_length']\n        s2_audio_length = data_dict['s2_audio_length']\n\n        mix_audio = self.load_audio(mix_path)\n        s1_audio = self.load_audio(s1_path)\n        s2_audio = self.load_audio(s2_path)\n\n        # s1_mouth = np.load(s1_mouth_path)\n        # s2_mouth = np.load(s2_mouth_path)\n\n        instance_data = {\n            'mix_path': mix_path,\n            's1_path': s1_path,\n            's2_path': s2_path,\n            's1_mouth_path': s1_mouth_path,\n            's2_mouth_path': s2_mouth_path,\n            'mix_audio_length': mix_audio_length,\n            's1_audio_length': s1_audio_length,\n            's2_audio_length': s2_audio_length,\n            'mix_audio': mix_audio,\n            's1_audio': s1_audio,\n            's2_audio': s2_audio,\n            # 's1_mouth': s1_mouth,\n            # 's2_mouth': s2_mouth\n        }\n\n        # TODO: \u043f\u043e\u0434\u0443\u043c\u0430\u0442\u044c \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u043a\u0430\u043a-\u0442\u043e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u044d\u0442\u043e \u0432\u0441\u0435\n        # instance_data = self.preprocess_data(instance_data)\n\n        return instance_data\n\n    def __len__(self):\n        \"\"\"\n        Get length of the dataset (length of the index).\n        \"\"\"\n        return len(self._index)\n\n    def load_audio(self, path):\n        audio_tensor, sr = torchaudio.load(path)\n        audio_tensor = audio_tensor[0:1, :]  # remove all channels but the first\n        target_sr = self.target_sr\n        if sr != target_sr:\n            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)\n        return audio_tensor\n\n    def get_spectrogram(self, audio):\n        \"\"\"\n        Special instance transform with a special key to\n        get spectrogram from audio.\n\n        Args:\n            audio (Tensor): original audio.\n        Returns:\n            spectrogram (Tensor): spectrogram for the audio.\n        \"\"\"\n        return self.instance_transforms[\"get_spectrogram\"](audio)\n\n    def preprocess_data(self, instance_data):\n        \"\"\"\n        Preprocess data with instance transforms.\n\n        Each tensor in a dict undergoes its own transform defined by the key.\n\n        Args:\n            instance_data (dict): dict, containing instance\n                (a single dataset element).\n        Returns:\n            instance_data (dict): dict, containing instance\n                (a single dataset element) (possibly transformed via\n                instance transform).\n        \"\"\"\n        if self.instance_transforms is not None:\n            for transform_name in self.instance_transforms.keys():\n                if transform_name == \"get_spectrogram\":\n                    continue  # skip special key\n                instance_data[transform_name] = self.instance_transforms[\n                    transform_name\n                ](instance_data[transform_name])\n        return instance_data\n\n    @staticmethod\n    def _filter_records_from_dataset(\n        index: list,\n        max_audio_length,\n        max_text_length,\n    ) -> list:\n        \"\"\"\n        Filter some of the elements from the dataset depending on\n        the desired max_test_length or max_audio_length.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            max_audio_length (int): maximum allowed audio length.\n            max_test_length (int): maximum allowed text length.\n        Returns:\n            index (list[dict]): list, containing dict for each element of\n                the dataset that satisfied the condition. The dict has\n                required metadata information, such as label and object path.\n        \"\"\"\n        initial_size = len(index)\n        if max_audio_length is not None:\n            exceeds_audio_length = (\n                np.array([el[\"audio_len\"] for el in index]) >= max_audio_length\n            )\n            _total = exceeds_audio_length.sum()\n            logger.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_audio_length} seconds. Excluding them.\"\n            )\n        else:\n            exceeds_audio_length = False\n\n        initial_size = len(index)\n        if max_text_length is not None:\n            exceeds_text_length = (\n                np.array(\n                    [len(CTCTextEncoder.normalize_text(el[\"text\"])) for el in index]\n                )\n                >= max_text_length\n            )\n            _total = exceeds_text_length.sum()\n            logger.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_text_length} characters. Excluding them.\"\n            )\n        else:\n            exceeds_text_length = False\n\n        records_to_filter = exceeds_text_length | exceeds_audio_length\n\n        if records_to_filter is not False and records_to_filter.any():\n            _total = records_to_filter.sum()\n            index = [el for el, exclude in zip(index, records_to_filter) if not exclude]\n            logger.info(\n                f\"Filtered {_total} ({_total / initial_size:.1%}) records  from dataset\"\n            )\n\n        return index\n\n    @staticmethod\n    def _assert_index_is_valid(index):\n        \"\"\"\n        Check the structure of the index and ensure it satisfies the desired\n        conditions.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n        \"\"\"\n        for entry in index:\n            assert \"path\" in entry, (\n                \"Each dataset item should include field 'path'\" \" - path to audio file.\"\n            )\n            assert \"text\" in entry, (\n                \"Each dataset item should include field 'text'\"\n                \" - object ground-truth transcription.\"\n            )\n            assert \"audio_len\" in entry, (\n                \"Each dataset item should include field 'audio_len'\"\n                \" - length of the audio.\"\n            )\n\n    @staticmethod\n    def _sort_index(index):\n        \"\"\"\n        Sort index by audio length.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n        Returns:\n            index (list[dict]): sorted list, containing dict for each element\n                of the dataset. The dict has required metadata information,\n                such as label and object path.\n        \"\"\"\n        return sorted(index, key=lambda x: x[\"audio_len\"])\n\n    @staticmethod\n    def _shuffle_and_limit_index(index, limit, shuffle_index):\n        \"\"\"\n        Shuffle elements in index and limit the total number of elements.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            limit (int | None): if not None, limit the total number of elements\n                in the dataset to 'limit' elements.\n            shuffle_index (bool): if True, shuffle the index. Uses python\n                random package with seed 42.\n        \"\"\"\n        if shuffle_index:\n            random.seed(42)\n            random.shuffle(index)\n\n        if limit is not None:\n            index = index[:limit]\n        return index\n\n```\n\n\n",
        "eval_script": "import logging\nimport random\nimport numpy as np\nimport torchaudio\nfrom torch.utils.data import Dataset\n\n# CTCTextEncoder definition imported from src.text_encoder.ctc_text_encoder\nimport re\nfrom string import ascii_lowercase\nimport torch\n\nclass CTCTextEncoder:\n    EMPTY_TOK = \"\"\n\n    def __init__(self, alphabet=None, **kwargs):\n        if alphabet is None:\n            alphabet = list(ascii_lowercase + \" \")\n\n        self.alphabet = alphabet\n        self.vocab = [self.EMPTY_TOK] + list(self.alphabet)\n\n        self.ind2char = dict(enumerate(self.vocab))\n        self.char2ind = {v: k for k, v in self.ind2char.items()}\n\n    def __len__(self):\n        return len(self.vocab)\n\n    def __getitem__(self, item: int):\n        assert type(item) is int\n        return self.ind2char[item]\n\n    def encode(self, text) -> torch.Tensor:\n        text = self.normalize_text(text)\n        try:\n            return torch.Tensor([self.char2ind[char] for char in text]).unsqueeze(0)\n        except KeyError:\n            unknown_chars = set([char for char in text if char not in self.char2ind])\n            raise Exception(\n                f\"Can't encode text '{text}'. Unknown chars: '{' '.join(unknown_chars)}'\"\n            )\n\n    def decode(self, inds) -> str:\n        return \"\".join([self.ind2char[int(ind)] for ind in inds]).strip()\n\n    def ctc_decode(self, inds) -> str:\n        pass  # TODO\n\n    @staticmethod\n    def normalize_text(text: str):\n        text = text.lower()\n        text = re.sub(r\"[^a-z ]\", \"\", text)\n        return text\n\n# Actual Dataset implementation\nlogger = logging.getLogger(__name__)\n\nclass BaseDataset(Dataset):\n    def __init__(\n        self,\n        index,\n        text_encoder=None,\n        target_sr=16000,\n        limit=None,\n        max_audio_length=None,\n        max_text_length=None,\n        shuffle_index=False,\n        instance_transforms=None,\n    ):\n        index = self._filter_records_from_dataset(\n            index, max_audio_length, max_text_length\n        )\n        index = self._shuffle_and_limit_index(index, limit, shuffle_index)\n\n        self._index: list[dict] = index\n        self.text_encoder = text_encoder\n        self.target_sr = target_sr\n        self.instance_transforms = instance_transforms\n\n    def __getitem__(self, ind):\n        data_dict = self._index[ind]\n\n        mix_audio = self.load_audio(data_dict['mix_path'])\n        s1_audio = self.load_audio(data_dict['s1_path'])\n        s2_audio = self.load_audio(data_dict['s2_path'])\n\n        instance_data = {\n            'mix_path': data_dict['mix_path'],\n            's1_path': data_dict['s1_path'],\n            's2_path': data_dict['s2_path'],\n            's1_mouth_path': data_dict['s1_mouth_path'],\n            's2_mouth_path': data_dict['s2_mouth_path'],\n            'mix_audio_length': data_dict['mix_audio_length'],\n            's1_audio_length': data_dict['s1_audio_length'],\n            's2_audio_length': data_dict['s2_audio_length'],\n            'mix_audio': mix_audio,\n            's1_audio': s1_audio,\n            's2_audio': s2_audio,\n        }\n\n        return instance_data\n\n    def __len__(self):\n        return len(self._index)\n\n    def load_audio(self, path):\n        audio_tensor, sr = torchaudio.load(path)\n        audio_tensor = audio_tensor[0:1, :]  # remove all channels but the first\n        target_sr = self.target_sr\n        if sr != target_sr:\n            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)\n        return audio_tensor\n\n    def get_spectrogram(self, audio):\n        return self.instance_transforms[\"get_spectrogram\"](audio)\n\n    def preprocess_data(self, instance_data):\n        if self.instance_transforms is not None:\n            for transform_name in self.instance_transforms.keys():\n                if transform_name == \"get_spectrogram\":\n                    continue  # skip special key\n                instance_data[transform_name] = self.instance_transforms[\n                    transform_name\n                ](instance_data[transform_name])\n        return instance_data\n\n    @staticmethod\n    def _filter_records_from_dataset(\n        index: list,\n        max_audio_length,\n        max_text_length,\n    ) -> list:\n        initial_size = len(index)\n        if max_audio_length is not None:\n            exceeds_audio_length = (\n                np.array([el[\"audio_len\"] for el in index]) >= max_audio_length\n            )\n            _total = exceeds_audio_length.sum()\n            logger.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_audio_length} seconds. Excluding them.\"\n            )\n        else:\n            exceeds_audio_length = False\n\n        initial_size = len(index)\n        if max_text_length is not None:\n            exceeds_text_length = (\n                np.array(\n                    [len(CTCTextEncoder.normalize_text(el[\"text\"])) for el in index]\n                )\n                >= max_text_length\n            )\n            _total = exceeds_text_length.sum()\n            logger.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_text_length} characters. Excluding them.\"\n            )\n        else:\n            exceeds_text_length = False\n\n        records_to_filter = exceeds_text_length | exceeds_audio_length\n\n        if records_to_filter is not False and records_to_filter.any():\n            _total = records_to_filter.sum()\n            index = [el for el, exclude in zip(index, records_to_filter) if not exclude]\n            logger.info(\n                f\"Filtered {_total} ({_total / initial_size:.1%}) records  from dataset\"\n            )\n\n        return index\n\n    @staticmethod\n    def _assert_index_is_valid(index):\n        for entry in index:\n            assert \"path\" in entry, (\n                \"Each dataset item should include field 'path'\" \" - path to audio file.\"\n            )\n            assert \"text\" in entry, (\n                \"Each dataset item should include field 'text'\"\n                \" - object ground-truth transcription.\"\n            )\n            assert \"audio_len\" in entry, (\n                \"Each dataset item should include field 'audio_len'\"\n                \" - length of the audio.\"\n            )\n\n    @staticmethod\n\n\n    @staticmethod\n    def _sort_index(index):\n        return sorted(index, key=lambda x: x[\"audio_len\"])\n\n    @staticmethod\n    def _shuffle_and_limit_index(index, limit, shuffle_index):\n        if shuffle_index:\n            random.seed(42)\n            random.shuffle(index)\n\n        if limit is not None:\n            index = index[:limit]\n        return index\n\ndef test__assert_index_is_valid():\n    # Define test cases\n    valid_index = [\n        {\"path\": \"audio1.wav\", \"text\": \"hello\", \"audio_len\": 3.5},\n        {\"path\": \"audio2.wav\", \"text\": \"world\", \"audio_len\": 2.2}\n    ]\n    \n    invalid_index_missing_path = [\n        {\"text\": \"hello\", \"audio_len\": 3.5},\n    ]\n    \n    invalid_index_missing_audio_len = [\n        {\"path\": \"audio3.wav\", \"text\": \"test\"},\n    ]\n\n    # Case 1: Valid index should pass without exception\n    BaseDataset._assert_index_is_valid(valid_index)\n    BaseDataset._assert_index_is_valid_new_implementation(valid_index)\n\n    # Case 2: Invalid index should raise assertion error\n    try:\n        BaseDataset._assert_index_is_valid(invalid_index_missing_path)\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(\"Original implementation did not raise error on invalid index\")\n    \n    try:\n        BaseDataset._assert_index_is_valid_new_implementation(invalid_index_missing_path)\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(\"New implementation did not raise error on invalid index\")\n    \n    # Case 3: Another invalid index should also raise an assertion error\n    try:\n        BaseDataset._assert_index_is_valid(invalid_index_missing_audio_len)\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(\"Original implementation did not raise error on invalid index\")\n    \n    try:\n        BaseDataset._assert_index_is_valid_new_implementation(invalid_index_missing_audio_len)\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(\"New implementation did not raise error on invalid index\")\n\nif __name__ == \"__main__\":\n    test__assert_index_is_valid()"
    },
    {
        "func_name": "BaseDataset._shuffle_and_limit_index",
        "idx": "46",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/datasets/base_dataset.py",
        "orig_func": "@staticmethod\ndef _shuffle_and_limit_index(index, limit, shuffle_index):\n    \"\"\"\n        Shuffle elements in index and limit the total number of elements.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            limit (int | None): if not None, limit the total number of elements\n                in the dataset to 'limit' elements.\n            shuffle_index (bool): if True, shuffle the index. Uses python\n                random package with seed 42.\n        \"\"\"\n    if shuffle_index:\n        random.seed(42)\n        random.shuffle(index)\n    if limit is not None:\n        index = index[:limit]\n    return index",
        "orig_context": "```python\n## src/text_encoder/ctc_text_encoder.py\nimport re\n\nfrom string import ascii_lowercase\n\nimport torch\n\nclass CTCTextEncoder:\n    EMPTY_TOK = \"\"\n\n    def __init__(self, alphabet=None, **kwargs):\n        \"\"\"\n        Args:\n            alphabet (list): alphabet for language. If None, it will be\n                set to ascii\n        \"\"\"\n\n        if alphabet is None:\n            alphabet = list(ascii_lowercase + \" \")\n\n        self.alphabet = alphabet\n        self.vocab = [self.EMPTY_TOK] + list(self.alphabet)\n\n        self.ind2char = dict(enumerate(self.vocab))\n        self.char2ind = {v: k for k, v in self.ind2char.items()}\n\n    def __len__(self):\n        return len(self.vocab)\n\n    def __getitem__(self, item: int):\n        assert type(item) is int\n        return self.ind2char[item]\n\n    def encode(self, text) -> torch.Tensor:\n        text = self.normalize_text(text)\n        try:\n            return torch.Tensor([self.char2ind[char] for char in text]).unsqueeze(0)\n        except KeyError:\n            unknown_chars = set([char for char in text if char not in self.char2ind])\n            raise Exception(\n                f\"Can't encode text '{text}'. Unknown chars: '{' '.join(unknown_chars)}'\"\n            )\n\n    def decode(self, inds) -> str:\n        \"\"\"\n        Raw decoding without CTC.\n        Used to validate the CTC decoding implementation.\n\n        Args:\n            inds (list): list of tokens.\n        Returns:\n            raw_text (str): raw text with empty tokens and repetitions.\n        \"\"\"\n        return \"\".join([self.ind2char[int(ind)] for ind in inds]).strip()\n\n    def ctc_decode(self, inds) -> str:\n        pass  # TODO\n\n    @staticmethod\n    def normalize_text(text: str):\n        text = text.lower()\n        text = re.sub(r\"[^a-z ]\", \"\", text)\n        return text\n\n```\n\n\n```python\n## src/text_encoder/__init__.py\nfrom src.text_encoder.ctc_text_encoder import CTCTextEncoder\n\n```\n\n\n```python\n## src/datasets/base_dataset.py\nimport logging\n\nimport random\n\nimport numpy as np\n\nimport torchaudio\n\nfrom torch.utils.data import Dataset\n\nfrom src.text_encoder import CTCTextEncoder\n\nlogger = logging.getLogger(__name__)\n\nclass BaseDataset(Dataset):\n    \"\"\"\n    Base class for the datasets.\n\n    Given a proper index (list[dict]), allows to process different datasets\n    for the same task in the identical manner. Therefore, to work with\n    several datasets, the user only have to define index in a nested class.\n    \"\"\"\n\n    def __init__(\n        self,\n        index,\n        text_encoder=None,\n        target_sr=16000,\n        limit=None,\n        max_audio_length=None,\n        max_text_length=None,\n        shuffle_index=False,\n        instance_transforms=None,\n    ):\n        \"\"\"\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            text_encoder (CTCTextEncoder): text encoder.\n            target_sr (int): supported sample rate.\n            limit (int | None): if not None, limit the total number of elements\n                in the dataset to 'limit' elements.\n            max_audio_length (int): maximum allowed audio length.\n            max_test_length (int): maximum allowed text length.\n            shuffle_index (bool): if True, shuffle the index. Uses python\n                random package with seed 42.\n            instance_transforms (dict[Callable] | None): transforms that\n                should be applied on the instance. Depend on the\n                tensor name.\n        \"\"\"\n        # self._assert_index_is_valid(index)\n\n        index = self._filter_records_from_dataset(\n            index, max_audio_length, max_text_length\n        )\n        index = self._shuffle_and_limit_index(index, limit, shuffle_index)\n\n        # if not shuffle_index:\n        #     index = self._sort_index(index)\n\n        self._index: list[dict] = index\n\n        self.text_encoder = text_encoder\n        self.target_sr = target_sr\n        self.instance_transforms = instance_transforms\n\n    def __getitem__(self, ind):\n        \"\"\"\n        Get element from the index, preprocess it, and combine it\n        into a dict.\n\n        Notice that the choice of key names is defined by the template user.\n        However, they should be consistent across dataset getitem, collate_fn,\n        loss_function forward method, and model forward method.\n\n        Args:\n            ind (int): index in the self.index list.\n        Returns:\n            instance_data (dict): dict, containing instance\n                (a single dataset element).\n        \"\"\"\n        data_dict = self._index[ind]\n\n        mix_path = data_dict['mix_path']\n        s1_path = data_dict['s1_path']\n        s2_path = data_dict['s2_path']\n        s1_mouth_path = data_dict['s1_mouth_path']\n        s2_mouth_path = data_dict['s2_mouth_path']\n        mix_audio_length = data_dict['mix_audio_length']\n        s1_audio_length = data_dict['s1_audio_length']\n        s2_audio_length = data_dict['s2_audio_length']\n\n        mix_audio = self.load_audio(mix_path)\n        s1_audio = self.load_audio(s1_path)\n        s2_audio = self.load_audio(s2_path)\n\n        # s1_mouth = np.load(s1_mouth_path)\n        # s2_mouth = np.load(s2_mouth_path)\n\n        instance_data = {\n            'mix_path': mix_path,\n            's1_path': s1_path,\n            's2_path': s2_path,\n            's1_mouth_path': s1_mouth_path,\n            's2_mouth_path': s2_mouth_path,\n            'mix_audio_length': mix_audio_length,\n            's1_audio_length': s1_audio_length,\n            's2_audio_length': s2_audio_length,\n            'mix_audio': mix_audio,\n            's1_audio': s1_audio,\n            's2_audio': s2_audio,\n            # 's1_mouth': s1_mouth,\n            # 's2_mouth': s2_mouth\n        }\n\n        # TODO: \u043f\u043e\u0434\u0443\u043c\u0430\u0442\u044c \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u043a\u0430\u043a-\u0442\u043e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u044d\u0442\u043e \u0432\u0441\u0435\n        # instance_data = self.preprocess_data(instance_data)\n\n        return instance_data\n\n    def __len__(self):\n        \"\"\"\n        Get length of the dataset (length of the index).\n        \"\"\"\n        return len(self._index)\n\n    def load_audio(self, path):\n        audio_tensor, sr = torchaudio.load(path)\n        audio_tensor = audio_tensor[0:1, :]  # remove all channels but the first\n        target_sr = self.target_sr\n        if sr != target_sr:\n            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)\n        return audio_tensor\n\n    def get_spectrogram(self, audio):\n        \"\"\"\n        Special instance transform with a special key to\n        get spectrogram from audio.\n\n        Args:\n            audio (Tensor): original audio.\n        Returns:\n            spectrogram (Tensor): spectrogram for the audio.\n        \"\"\"\n        return self.instance_transforms[\"get_spectrogram\"](audio)\n\n    def preprocess_data(self, instance_data):\n        \"\"\"\n        Preprocess data with instance transforms.\n\n        Each tensor in a dict undergoes its own transform defined by the key.\n\n        Args:\n            instance_data (dict): dict, containing instance\n                (a single dataset element).\n        Returns:\n            instance_data (dict): dict, containing instance\n                (a single dataset element) (possibly transformed via\n                instance transform).\n        \"\"\"\n        if self.instance_transforms is not None:\n            for transform_name in self.instance_transforms.keys():\n                if transform_name == \"get_spectrogram\":\n                    continue  # skip special key\n                instance_data[transform_name] = self.instance_transforms[\n                    transform_name\n                ](instance_data[transform_name])\n        return instance_data\n\n    @staticmethod\n    def _filter_records_from_dataset(\n        index: list,\n        max_audio_length,\n        max_text_length,\n    ) -> list:\n        \"\"\"\n        Filter some of the elements from the dataset depending on\n        the desired max_test_length or max_audio_length.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            max_audio_length (int): maximum allowed audio length.\n            max_test_length (int): maximum allowed text length.\n        Returns:\n            index (list[dict]): list, containing dict for each element of\n                the dataset that satisfied the condition. The dict has\n                required metadata information, such as label and object path.\n        \"\"\"\n        initial_size = len(index)\n        if max_audio_length is not None:\n            exceeds_audio_length = (\n                np.array([el[\"audio_len\"] for el in index]) >= max_audio_length\n            )\n            _total = exceeds_audio_length.sum()\n            logger.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_audio_length} seconds. Excluding them.\"\n            )\n        else:\n            exceeds_audio_length = False\n\n        initial_size = len(index)\n        if max_text_length is not None:\n            exceeds_text_length = (\n                np.array(\n                    [len(CTCTextEncoder.normalize_text(el[\"text\"])) for el in index]\n                )\n                >= max_text_length\n            )\n            _total = exceeds_text_length.sum()\n            logger.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_text_length} characters. Excluding them.\"\n            )\n        else:\n            exceeds_text_length = False\n\n        records_to_filter = exceeds_text_length | exceeds_audio_length\n\n        if records_to_filter is not False and records_to_filter.any():\n            _total = records_to_filter.sum()\n            index = [el for el, exclude in zip(index, records_to_filter) if not exclude]\n            logger.info(\n                f\"Filtered {_total} ({_total / initial_size:.1%}) records  from dataset\"\n            )\n\n        return index\n\n    @staticmethod\n    def _assert_index_is_valid(index):\n        \"\"\"\n        Check the structure of the index and ensure it satisfies the desired\n        conditions.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n        \"\"\"\n        for entry in index:\n            assert \"path\" in entry, (\n                \"Each dataset item should include field 'path'\" \" - path to audio file.\"\n            )\n            assert \"text\" in entry, (\n                \"Each dataset item should include field 'text'\"\n                \" - object ground-truth transcription.\"\n            )\n            assert \"audio_len\" in entry, (\n                \"Each dataset item should include field 'audio_len'\"\n                \" - length of the audio.\"\n            )\n\n    @staticmethod\n    def _sort_index(index):\n        \"\"\"\n        Sort index by audio length.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n        Returns:\n            index (list[dict]): sorted list, containing dict for each element\n                of the dataset. The dict has required metadata information,\n                such as label and object path.\n        \"\"\"\n        return sorted(index, key=lambda x: x[\"audio_len\"])\n\n    @staticmethod\n    def _shuffle_and_limit_index(index, limit, shuffle_index):\n        \"\"\"\n        Shuffle elements in index and limit the total number of elements.\n\n        Args:\n            index (list[dict]): list, containing dict for each element of\n                the dataset. The dict has required metadata information,\n                such as label and object path.\n            limit (int | None): if not None, limit the total number of elements\n                in the dataset to 'limit' elements.\n            shuffle_index (bool): if True, shuffle the index. Uses python\n                random package with seed 42.\n        \"\"\"\n        if shuffle_index:\n            random.seed(42)\n            random.shuffle(index)\n\n        if limit is not None:\n            index = index[:limit]\n        return index\n\n```\n\n\n",
        "eval_script": "# Import necessary modules and classes\nimport logging\nimport random\nimport numpy as np\nimport torchaudio\nfrom torch.utils.data import Dataset\nfrom string import ascii_lowercase\nimport re\nimport torch\n\n# Define the CTCTextEncoder class\nclass CTCTextEncoder:\n    EMPTY_TOK = \"\"\n\n    def __init__(self, alphabet=None, **kwargs):\n        if alphabet is None:\n            alphabet = list(ascii_lowercase + \" \")\n        self.alphabet = alphabet\n        self.vocab = [self.EMPTY_TOK] + list(self.alphabet)\n        self.ind2char = dict(enumerate(self.vocab))\n        self.char2ind = {v: k for k, v in self.ind2char.items()}\n\n    def __len__(self):\n        return len(self.vocab)\n\n    def __getitem__(self, item: int):\n        assert type(item) is int\n        return self.ind2char[item]\n\n    def encode(self, text) -> torch.Tensor:\n        text = self.normalize_text(text)\n        try:\n            return torch.Tensor([self.char2ind[char] for char in text]).unsqueeze(0)\n        except KeyError:\n            unknown_chars = set([char for char in text if char not in self.char2ind])\n            raise Exception(\n                f\"Can't encode text '{text}'. Unknown chars: '{' '.join(unknown_chars)}'\"\n            )\n\n    def decode(self, inds) -> str:\n        return \"\".join([self.ind2char[int(ind)] for ind in inds]).strip()\n\n    @staticmethod\n    def normalize_text(text: str):\n        text = text.lower()\n        text = re.sub(r\"[^a-z ]\", \"\", text)\n        return text\n\n# Define the BaseDataset class\nclass BaseDataset(Dataset):\n    def __init__(self, index, text_encoder=None, target_sr=16000, limit=None, \n                 max_audio_length=None, max_text_length=None, shuffle_index=False, \n                 instance_transforms=None):\n        index = self._filter_records_from_dataset(\n            index, max_audio_length, max_text_length\n        )\n        index = self._shuffle_and_limit_index(index, limit, shuffle_index)\n        self._index: list[dict] = index\n        self.text_encoder = text_encoder\n        self.target_sr = target_sr\n        self.instance_transforms = instance_transforms\n\n    def __getitem__(self, ind):\n        data_dict = self._index[ind]\n        mix_path = data_dict['mix_path']\n        s1_path = data_dict['s1_path']\n        s2_path = data_dict['s2_path']\n        s1_mouth_path = data_dict['s1_mouth_path']\n        s2_mouth_path = data_dict['s2_mouth_path']\n        mix_audio_length = data_dict['mix_audio_length']\n        s1_audio_length = data_dict['s1_audio_length']\n        s2_audio_length = data_dict['s2_audio_length']\n        \n        mix_audio = self.load_audio(mix_path)\n        s1_audio = self.load_audio(s1_path)\n        s2_audio = self.load_audio(s2_path)\n\n        instance_data = {\n            'mix_path': mix_path,\n            's1_path': s1_path,\n            's2_path': s2_path,\n            's1_mouth_path': s1_mouth_path,\n            's2_mouth_path': s2_mouth_path,\n            'mix_audio_length': mix_audio_length,\n            's1_audio_length': s1_audio_length,\n            's2_audio_length': s2_audio_length,\n            'mix_audio': mix_audio,\n            's1_audio': s1_audio,\n            's2_audio': s2_audio,\n        }\n\n        return instance_data\n\n    def __len__(self):\n        return len(self._index)\n\n    def load_audio(self, path):\n        audio_tensor, sr = torchaudio.load(path)\n        audio_tensor = audio_tensor[0:1, :]\n        target_sr = self.target_sr\n        if sr != target_sr:\n            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)\n        return audio_tensor\n\n    def preprocess_data(self, instance_data):\n        if self.instance_transforms is not None:\n            for transform_name in self.instance_transforms.keys():\n                if transform_name == \"get_spectrogram\":\n                    continue\n                instance_data[transform_name] = self.instance_transforms[\n                    transform_name\n                ](instance_data[transform_name])\n        return instance_data\n\n    @staticmethod\n    def _filter_records_from_dataset(\n        index: list,\n        max_audio_length,\n        max_text_length,\n    ) -> list:\n        initial_size = len(index)\n        if max_audio_length is not None:\n            exceeds_audio_length = (\n                np.array([el[\"audio_len\"] for el in index]) >= max_audio_length\n            )\n            _total = exceeds_audio_length.sum()\n            logging.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_audio_length} seconds. Excluding them.\"\n            )\n        else:\n            exceeds_audio_length = False\n\n        initial_size = len(index)\n        if max_text_length is not None:\n            exceeds_text_length = (\n                np.array(\n                    [len(CTCTextEncoder.normalize_text(el[\"text\"])) for el in index]\n                )\n                >= max_text_length\n            )\n            _total = exceeds_text_length.sum()\n            logging.info(\n                f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n                f\"{max_text_length} characters. Excluding them.\"\n            )\n        else:\n            exceeds_text_length = False\n\n        records_to_filter = exceeds_text_length | exceeds_audio_length\n\n        if records_to_filter is not False and records_to_filter.any():\n            _total = records_to_filter.sum()\n            index = [el for el, exclude in zip(index, records_to_filter) if not exclude]\n            logging.info(\n                f\"Filtered {_total} ({_total / initial_size:.1%}) records  from dataset\"\n            )\n\n        return index\n\n    @staticmethod\n    def _assert_index_is_valid(index):\n        for entry in index:\n            assert \"path\" in entry, (\n                \"Each dataset item should include field 'path'\" \" - path to audio file.\"\n            )\n            assert \"text\" in entry, (\n                \"Each dataset item should include field 'text'\"\n                \" - object ground-truth transcription.\"\n            )\n            assert \"audio_len\" in entry, (\n                \"Each dataset item should include field 'audio_len'\"\n                \" - length of the audio.\"\n            )\n\n    @staticmethod\n    def _sort_index(index):\n        return sorted(index, key=lambda x: x[\"audio_len\"])\n\n    @staticmethod\n    def _shuffle_and_limit_index(index, limit, shuffle_index):\n        if shuffle_index:\n            random.seed(42)\n            random.shuffle(index)\n\n        if limit is not None:\n            index = index[:limit]\n        return index\n\n\n\n\n# Test function\ndef test__shuffle_and_limit_index():\n    mock_index = [\n        {\"mix_path\": \"path/to/audio1\", \"s1_path\": \"path/to/s1\", \"s2_path\": \"path/to/s2\", \n        \"s1_mouth_path\": \"path/to/s1_mouth\", \"s2_mouth_path\": \"path/to/s2_mouth\", \n        \"mix_audio_length\": 120, \"s1_audio_length\": 60, \"s2_audio_length\": 60},\n        \n        {\"mix_path\": \"path/to/audio2\", \"s1_path\": \"path/to/s1\", \"s2_path\": \"path/to/s2\", \n        \"s1_mouth_path\": \"path/to/s1_mouth\", \"s2_mouth_path\": \"path/to/s2_mouth\", \n        \"mix_audio_length\": 150, \"s1_audio_length\": 75, \"s2_audio_length\": 75}\n    ]\n\n    expected_index_1 = BaseDataset._shuffle_and_limit_index(mock_index.copy(), limit=1, shuffle_index=True)\n    actual_index_1 = BaseDataset._shuffle_and_limit_index_new_implementation(mock_index.copy(), limit=1, shuffle_index=True)\n    assert expected_index_1 == actual_index_1, \"Test Case 1 failed.\"\n\n    expected_index_2 = BaseDataset._shuffle_and_limit_index(mock_index.copy(), limit=None, shuffle_index=True)\n    actual_index_2 = BaseDataset._shuffle_and_limit_index_new_implementation(mock_index.copy(), limit=None, shuffle_index=True)\n    assert expected_index_2 == actual_index_2, \"Test Case 2 failed.\"\n\n    expected_index_3 = BaseDataset._shuffle_and_limit_index(mock_index.copy(), limit=2, shuffle_index=False)\n    actual_index_3 = BaseDataset._shuffle_and_limit_index_new_implementation(mock_index.copy(), limit=2, shuffle_index=False)\n    assert expected_index_3 == actual_index_3, \"Test Case 3 failed.\"\n\n# __main__ function\nif __name__ == '__main__':\n    test__shuffle_and_limit_index()\n    print(\"All tests passed!\")"
    },
    {
        "func_name": "BaseTrainer._progress",
        "idx": "47",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/trainer/base_trainer.py",
        "orig_func": "def _progress(self, batch_idx):\n    \"\"\"\n        Calculates the percentage of processed batch within the epoch.\n\n        Args:\n            batch_idx (int): the current batch index.\n        Returns:\n            progress (str): contains current step and percentage\n                within the epoch.\n        \"\"\"\n    base = '[{}/{} ({:.0f}%)]'\n    if hasattr(self.train_dataloader, 'n_samples'):\n        current = batch_idx * self.train_dataloader.batch_size\n        total = self.train_dataloader.n_samples\n    else:\n        current = batch_idx\n        total = self.epoch_len\n    return base.format(current, total, 100.0 * current / total)",
        "orig_context": "```python\n## src/metrics/tracker.py\nimport pandas as pd\n\nclass MetricTracker:\n    \"\"\"\n    Class to aggregate metrics from many batches.\n    \"\"\"\n\n    def __init__(self, *keys, writer=None):\n        \"\"\"\n        Args:\n            *keys (list[str]): list (as positional arguments) of metric\n                names (may include the names of losses)\n            writer (WandBWriter | CometMLWriter | None): experiment tracker.\n                Not used in this code version. Can be used to log metrics\n                from each batch.\n        \"\"\"\n        self.writer = writer\n        self._data = pd.DataFrame(index=keys, columns=[\"total\", \"counts\", \"average\"])\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Reset all metrics after epoch end.\n        \"\"\"\n        for col in self._data.columns:\n            self._data[col].values[:] = 0\n\n    def update(self, key, value, n=1):\n        \"\"\"\n        Update metrics DataFrame with new value.\n\n        Args:\n            key (str): metric name.\n            value (float): metric value on the batch.\n            n (int): how many times to count this value.\n        \"\"\"\n        # if self.writer is not None:\n        #     self.writer.add_scalar(key, value)\n        self._data.loc[key, \"total\"] += value * n\n        self._data.loc[key, \"counts\"] += n\n        self._data.loc[key, \"average\"] = self._data.total[key] / self._data.counts[key]\n\n    def avg(self, key):\n        \"\"\"\n        Return average value for a given metric.\n\n        Args:\n            key (str): metric name.\n        Returns:\n            average_value (float): average value for the metric.\n        \"\"\"\n        return self._data.average[key]\n\n    def result(self):\n        \"\"\"\n        Return average value of each metric.\n\n        Returns:\n            average_metrics (dict): dict, containing average metrics\n                for each metric name.\n        \"\"\"\n        return dict(self._data.average)\n\n    def keys(self):\n        \"\"\"\n        Return all metric names defined in the MetricTracker.\n\n        Returns:\n            metric_keys (Index): all metric names in the table.\n        \"\"\"\n        return self._data.total.keys()\n\n```\n\n\n```python\n## src/utils/io_utils.py\nfrom pathlib import Path\n\nROOT_PATH = Path(__file__).absolute().resolve().parent.parent.parent\n\n```\n\n\n```python\n## src/datasets/data_utils.py\nfrom itertools import repeat\n\ndef inf_loop(dataloader):\n    \"\"\"\n    Wrapper function for endless dataloader.\n    Used for iteration-based training scheme.\n\n    Args:\n        dataloader (DataLoader): classic finite dataloader.\n    \"\"\"\n    for loader in repeat(dataloader):\n        yield from loader\n\n```\n\n\n```python\n## src/trainer/base_trainer.py\nfrom abc import abstractmethod\n\nimport torch\n\nfrom numpy import inf\n\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom tqdm.auto import tqdm\n\nfrom src.datasets.data_utils import inf_loop\n\nfrom src.metrics.tracker import MetricTracker\n\nfrom src.utils.io_utils import ROOT_PATH\n\nclass BaseTrainer:\n    \"\"\"\n    Base class for all trainers.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        criterion,\n        metrics,\n        optimizer,\n        lr_scheduler,\n        config,\n        device,\n        dataloaders,\n        logger,\n        writer,\n        epoch_len=None,\n        skip_oom=True,\n        batch_transforms=None,\n    ):\n        \"\"\"\n        Args:\n            model (nn.Module): PyTorch model.\n            criterion (nn.Module): loss function for model training.\n            metrics (dict): dict with the definition of metrics for training\n                (metrics[train]) and inference (metrics[inference]). Each\n                metric is an instance of src.metrics.BaseMetric.\n            optimizer (Optimizer): optimizer for the model.\n            lr_scheduler (LRScheduler): learning rate scheduler for the\n                optimizer.\n            text_encoder (CTCTextEncoder): text encoder.\n            config (DictConfig): experiment config containing training config.\n            device (str): device for tensors and model.\n            dataloaders (dict[DataLoader]): dataloaders for different\n                sets of data.\n            logger (Logger): logger that logs output.\n            writer (WandBWriter | CometMLWriter): experiment tracker.\n            epoch_len (int | None): number of steps in each epoch for\n                iteration-based training. If None, use epoch-based\n                training (len(dataloader)).\n            skip_oom (bool): skip batches with the OutOfMemory error.\n            batch_transforms (dict[Callable] | None): transforms that\n                should be applied on the whole batch. Depend on the\n                tensor name.\n        \"\"\"\n        self.is_train = True\n\n        self.config = config\n        self.cfg_trainer = self.config.trainer\n\n        self.device = device\n        self.skip_oom = skip_oom\n\n        self.logger = logger\n        self.log_step = config.trainer.get(\"log_step\", 50)\n\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n\n        self.batch_transforms = batch_transforms\n\n        # define dataloaders\n        self.train_dataloader = dataloaders[\"train\"]\n        if epoch_len is None:\n            # epoch-based training\n            self.epoch_len = len(self.train_dataloader)\n        else:\n            # iteration-based training\n            self.train_dataloader = inf_loop(self.train_dataloader)\n            self.epoch_len = epoch_len\n\n        self.evaluation_dataloaders = {\n            k: v for k, v in dataloaders.items() if k != \"train\"\n        }\n\n        # define epochs\n        self._last_epoch = 0  # required for saving on interruption\n        self.start_epoch = 1\n        self.epochs = self.cfg_trainer.n_epochs\n\n        # configuration to monitor model performance and save best\n\n        self.save_period = (\n            self.cfg_trainer.save_period\n        )  # checkpoint each save_period epochs\n        self.monitor = self.cfg_trainer.get(\n            \"monitor\", \"off\"\n        )  # format: \"mnt_mode mnt_metric\"\n\n        if self.monitor == \"off\":\n            self.mnt_mode = \"off\"\n            self.mnt_best = 0\n        else:\n            self.mnt_mode, self.mnt_metric = self.monitor.split()\n            assert self.mnt_mode in [\"min\", \"max\"]\n\n            self.mnt_best = inf if self.mnt_mode == \"min\" else -inf\n            self.early_stop = self.cfg_trainer.get(\"early_stop\", inf)\n            if self.early_stop <= 0:\n                self.early_stop = inf\n\n        # setup visualization writer instance\n        self.writer = writer\n\n        # define metrics\n        self.metrics = metrics\n        self.train_metrics = MetricTracker(\n            *self.config.writer.loss_names,\n            \"grad_norm\",\n            *[m.name for m in self.metrics[\"train\"]],\n            writer=self.writer,\n        )\n        self.evaluation_metrics = MetricTracker(\n            *self.config.writer.loss_names,\n            *[m.name for m in self.metrics[\"inference\"]],\n            writer=self.writer,\n        )\n\n        # define checkpoint dir and init everything if required\n\n        self.checkpoint_dir = (\n            ROOT_PATH / config.trainer.save_dir / config.writer.run_name\n        )\n\n        if config.trainer.get(\"resume_from\") is not None:\n            resume_path = self.checkpoint_dir / config.trainer.resume_from\n            self._resume_checkpoint(resume_path)\n\n        if config.trainer.get(\"from_pretrained\") is not None:\n            self._from_pretrained(config.trainer.get(\"from_pretrained\"))\n\n    def train(self):\n        \"\"\"\n        Wrapper around training process to save model on keyboard interrupt.\n        \"\"\"\n        try:\n            self._train_process()\n        except KeyboardInterrupt as e:\n            self.logger.info(\"Saving model on keyboard interrupt\")\n            self._save_checkpoint(self._last_epoch, save_best=False)\n            raise e\n\n    def _train_process(self):\n        \"\"\"\n        Full training logic:\n\n        Training model for an epoch, evaluating it on non-train partitions,\n        and monitoring the performance improvement (for early stopping\n        and saving the best checkpoint).\n        \"\"\"\n        not_improved_count = 0\n        for epoch in range(self.start_epoch, self.epochs + 1):\n            self._last_epoch = epoch\n            result = self._train_epoch(epoch)\n\n            # save logged information into logs dict\n            logs = {\"epoch\": epoch}\n            logs.update(result)\n\n            # print logged information to the screen\n            for key, value in logs.items():\n                self.logger.info(f\"    {key:15s}: {value}\")\n\n            # evaluate model performance according to configured metric,\n            # save best checkpoint as model_best\n            best, stop_process, not_improved_count = self._monitor_performance(\n                logs, not_improved_count\n            )\n\n            if epoch % self.save_period == 0 or best:\n                self._save_checkpoint(epoch, save_best=best, only_best=True)\n\n            if stop_process:  # early_stop\n                break\n\n    def _train_epoch(self, epoch):\n        \"\"\"\n        Training logic for an epoch, including logging and evaluation on\n        non-train partitions.\n\n        Args:\n            epoch (int): current training epoch.\n        Returns:\n            logs (dict): logs that contain the average loss and metric in\n                this epoch.\n        \"\"\"\n        self.is_train = True\n        self.model.train()\n        self.train_metrics.reset()\n        self.writer.set_step((epoch - 1) * self.epoch_len)\n        self.writer.add_scalar(\"epoch\", epoch)\n        for batch_idx, batch in enumerate(\n            tqdm(self.train_dataloader, desc=\"train\", total=self.epoch_len)\n        ):\n            try:\n                batch = self.process_batch(\n                    batch,\n                    metrics=self.train_metrics,\n                )\n            except torch.cuda.OutOfMemoryError as e:\n                if self.skip_oom:\n                    self.logger.warning(\"OOM on batch. Skipping batch.\")\n                    torch.cuda.empty_cache()  # free some memory\n                    continue\n                else:\n                    raise e\n\n            self.train_metrics.update(\"grad_norm\", self._get_grad_norm())\n\n            # log current results\n            if batch_idx % self.log_step == 0:\n                self.writer.set_step((epoch - 1) * self.epoch_len + batch_idx)\n                self.logger.debug(\n                    \"Train Epoch: {} {} Loss: {:.6f}\".format(\n                        epoch, self._progress(batch_idx), batch[\"loss\"].item()\n                    )\n                )\n                self.writer.add_scalar(\n                    \"learning rate\", self.lr_scheduler.get_last_lr()[0]\n                )\n                self._log_scalars(self.train_metrics)\n                self._log_batch(batch_idx, batch)\n                # we don't want to reset train metrics at the start of every epoch\n                # because we are interested in recent train metrics\n                last_train_metrics = self.train_metrics.result()\n                self.train_metrics.reset()\n            if batch_idx + 1 >= self.epoch_len:\n                break\n\n        logs = last_train_metrics\n\n        # Run val/test\n        for part, dataloader in self.evaluation_dataloaders.items():\n            val_logs = self._evaluation_epoch(epoch, part, dataloader)\n            logs.update(**{f\"{part}_{name}\": value for name, value in val_logs.items()})\n\n        return logs\n\n    def _evaluation_epoch(self, epoch, part, dataloader):\n        \"\"\"\n        Evaluate model on the partition after training for an epoch.\n\n        Args:\n            epoch (int): current training epoch.\n            part (str): partition to evaluate on\n            dataloader (DataLoader): dataloader for the partition.\n        Returns:\n            logs (dict): logs that contain the information about evaluation.\n        \"\"\"\n        self.is_train = False\n        self.model.eval()\n        self.evaluation_metrics.reset()\n        with torch.no_grad():\n            for batch_idx, batch in tqdm(\n                enumerate(dataloader),\n                desc=part,\n                total=len(dataloader),\n            ):\n                batch = self.process_batch(\n                    batch,\n                    metrics=self.evaluation_metrics,\n                )\n            self.writer.set_step(epoch * self.epoch_len, part)\n            self._log_scalars(self.evaluation_metrics)\n            self._log_batch(\n                batch_idx, batch, part\n            )  # log only the last batch during inference\n\n        return self.evaluation_metrics.result()\n\n    def _monitor_performance(self, logs, not_improved_count):\n        \"\"\"\n        Check if there is an improvement in the metrics. Used for early\n        stopping and saving the best checkpoint.\n\n        Args:\n            logs (dict): logs after training and evaluating the model for\n                an epoch.\n            not_improved_count (int): the current number of epochs without\n                improvement.\n        Returns:\n            best (bool): if True, the monitored metric has improved.\n            stop_process (bool): if True, stop the process (early stopping).\n                The metric did not improve for too much epochs.\n            not_improved_count (int): updated number of epochs without\n                improvement.\n        \"\"\"\n        best = False\n        stop_process = False\n        if self.mnt_mode != \"off\":\n            try:\n                # check whether model performance improved or not,\n                # according to specified metric(mnt_metric)\n                if self.mnt_mode == \"min\":\n                    improved = logs[self.mnt_metric] <= self.mnt_best\n                elif self.mnt_mode == \"max\":\n                    improved = logs[self.mnt_metric] >= self.mnt_best\n                else:\n                    improved = False\n            except KeyError:\n                self.logger.warning(\n                    f\"Warning: Metric '{self.mnt_metric}' is not found. \"\n                    \"Model performance monitoring is disabled.\"\n                )\n                self.mnt_mode = \"off\"\n                improved = False\n\n            if improved:\n                self.mnt_best = logs[self.mnt_metric]\n                not_improved_count = 0\n                best = True\n            else:\n                not_improved_count += 1\n\n            if not_improved_count >= self.early_stop:\n                self.logger.info(\n                    \"Validation performance didn't improve for {} epochs. \"\n                    \"Training stops.\".format(self.early_stop)\n                )\n                stop_process = True\n        return best, stop_process, not_improved_count\n\n    def move_batch_to_device(self, batch):\n        \"\"\"\n        Move all necessary tensors to the device.\n\n        Args:\n            batch (dict): dict-based batch containing the data from\n                the dataloader.\n        Returns:\n            batch (dict): dict-based batch containing the data from\n                the dataloader with some of the tensors on the device.\n        \"\"\"\n        for tensor_for_device in self.cfg_trainer.device_tensors:\n            batch[tensor_for_device] = batch[tensor_for_device].to(self.device)\n        return batch\n\n    def transform_batch(self, batch):\n        \"\"\"\n        Transforms elements in batch. Like instance transform inside the\n        BaseDataset class, but for the whole batch. Improves pipeline speed,\n        especially if used with a GPU.\n\n        Each tensor in a batch undergoes its own transform defined by the key.\n\n        Args:\n            batch (dict): dict-based batch containing the data from\n                the dataloader.\n        Returns:\n            batch (dict): dict-based batch containing the data from\n                the dataloader (possibly transformed via batch transform).\n        \"\"\"\n        # do batch transforms on device\n        # transform_type = \"train\" if self.is_train else \"inference\"\n        # transforms = self.batch_transforms.get(transform_type)\n        # if transforms is not None:\n        #     for transform_name in transforms.keys():\n        #         batch[transform_name] = transforms[transform_name](\n        #             batch[transform_name]\n        #         )\n        return batch\n\n    def _clip_grad_norm(self):\n        \"\"\"\n        Clips the gradient norm by the value defined in\n        config.trainer.max_grad_norm\n        \"\"\"\n        if self.config[\"trainer\"].get(\"max_grad_norm\", None) is not None:\n            clip_grad_norm_(\n                self.model.parameters(), self.config[\"trainer\"][\"max_grad_norm\"]\n            )\n\n    @torch.no_grad()\n    def _get_grad_norm(self, norm_type=2):\n        \"\"\"\n        Calculates the gradient norm for logging.\n\n        Args:\n            norm_type (float | str | None): the order of the norm.\n        Returns:\n            total_norm (float): the calculated norm.\n        \"\"\"\n        parameters = self.model.parameters()\n        if isinstance(parameters, torch.Tensor):\n            parameters = [parameters]\n        parameters = [p for p in parameters if p.grad is not None]\n        total_norm = torch.norm(\n            torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n            norm_type,\n        )\n        return total_norm.item()\n\n    def _progress(self, batch_idx):\n        \"\"\"\n        Calculates the percentage of processed batch within the epoch.\n\n        Args:\n            batch_idx (int): the current batch index.\n        Returns:\n            progress (str): contains current step and percentage\n                within the epoch.\n        \"\"\"\n        base = \"[{}/{} ({:.0f}%)]\"\n        if hasattr(self.train_dataloader, \"n_samples\"):\n            current = batch_idx * self.train_dataloader.batch_size\n            total = self.train_dataloader.n_samples\n        else:\n            current = batch_idx\n            total = self.epoch_len\n        return base.format(current, total, 100.0 * current / total)\n\n    @abstractmethod\n    def _log_batch(self, batch_idx, batch, mode=\"train\"):\n        \"\"\"\n        Abstract method. Should be defined in the nested Trainer Class.\n\n        Log data from batch. Calls self.writer.add_* to log data\n        to the experiment tracker.\n\n        Args:\n            batch_idx (int): index of the current batch.\n            batch (dict): dict-based batch after going through\n                the 'process_batch' function.\n            mode (str): train or inference. Defines which logging\n                rules to apply.\n        \"\"\"\n        return NotImplementedError()\n\n    def _log_scalars(self, metric_tracker: MetricTracker):\n        \"\"\"\n        Wrapper around the writer 'add_scalar' to log all metrics.\n\n        Args:\n            metric_tracker (MetricTracker): calculated metrics.\n        \"\"\"\n        if self.writer is None:\n            return\n        for metric_name in metric_tracker.keys():\n            self.writer.add_scalar(f\"{metric_name}\", metric_tracker.avg(metric_name))\n\n    def _save_checkpoint(self, epoch, save_best=False, only_best=False):\n        \"\"\"\n        Save the checkpoints.\n\n        Args:\n            epoch (int): current epoch number.\n            save_best (bool): if True, rename the saved checkpoint to 'model_best.pth'.\n            only_best (bool): if True and the checkpoint is the best, save it only as\n                'model_best.pth'(do not duplicate the checkpoint as\n                checkpoint-epochEpochNumber.pth)\n        \"\"\"\n        arch = type(self.model).__name__\n        state = {\n            \"arch\": arch,\n            \"epoch\": epoch,\n            \"state_dict\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"lr_scheduler\": self.lr_scheduler.state_dict(),\n            \"monitor_best\": self.mnt_best,\n            \"config\": self.config,\n        }\n        filename = str(self.checkpoint_dir / f\"checkpoint-epoch{epoch}.pth\")\n        if not (only_best and save_best):\n            torch.save(state, filename)\n            if self.config.writer.log_checkpoints:\n                self.writer.add_checkpoint(filename, str(self.checkpoint_dir.parent))\n            self.logger.info(f\"Saving checkpoint: {filename} ...\")\n        if save_best:\n            best_path = str(self.checkpoint_dir / \"model_best.pth\")\n            torch.save(state, best_path)\n            if self.config.writer.log_checkpoints:\n                self.writer.add_checkpoint(best_path, str(self.checkpoint_dir.parent))\n            self.logger.info(\"Saving current best: model_best.pth ...\")\n\n    def _resume_checkpoint(self, resume_path):\n        \"\"\"\n        Resume from a saved checkpoint (in case of server crash, etc.).\n        The function loads state dicts for everything, including model,\n        optimizers, etc.\n\n        Notice that the checkpoint should be located in the current experiment\n        saved directory (where all checkpoints are saved in '_save_checkpoint').\n\n        Args:\n            resume_path (str): Path to the checkpoint to be resumed.\n        \"\"\"\n        resume_path = str(resume_path)\n        self.logger.info(f\"Loading checkpoint: {resume_path} ...\")\n        checkpoint = torch.load(resume_path, self.device)\n        self.start_epoch = checkpoint[\"epoch\"] + 1\n        self.mnt_best = checkpoint[\"monitor_best\"]\n\n        # load architecture params from checkpoint.\n        if checkpoint[\"config\"][\"model\"] != self.config[\"model\"]:\n            self.logger.warning(\n                \"Warning: Architecture configuration given in the config file is different from that \"\n                \"of the checkpoint. This may yield an exception when state_dict is loaded.\"\n            )\n        self.model.load_state_dict(checkpoint[\"state_dict\"])\n\n        # load optimizer state from checkpoint only when optimizer type is not changed.\n        if (\n            checkpoint[\"config\"][\"optimizer\"] != self.config[\"optimizer\"]\n            or checkpoint[\"config\"][\"lr_scheduler\"] != self.config[\"lr_scheduler\"]\n        ):\n            self.logger.warning(\n                \"Warning: Optimizer or lr_scheduler given in the config file is different \"\n                \"from that of the checkpoint. Optimizer and scheduler parameters \"\n                \"are not resumed.\"\n            )\n        else:\n            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n\n        self.logger.info(\n            f\"Checkpoint loaded. Resume training from epoch {self.start_epoch}\"\n        )\n\n    def _from_pretrained(self, pretrained_path):\n        \"\"\"\n        Init model with weights from pretrained pth file.\n\n        Notice that 'pretrained_path' can be any path on the disk. It is not\n        necessary to locate it in the experiment saved dir. The function\n        initializes only the model.\n\n        Args:\n            pretrained_path (str): path to the model state dict.\n        \"\"\"\n        pretrained_path = str(pretrained_path)\n        if hasattr(self, \"logger\"):  # to support both trainer and inferencer\n            self.logger.info(f\"Loading model weights from: {pretrained_path} ...\")\n        else:\n            print(f\"Loading model weights from: {pretrained_path} ...\")\n        checkpoint = torch.load(pretrained_path, self.device)\n\n        if checkpoint.get(\"state_dict\") is not None:\n            self.model.load_state_dict(checkpoint[\"state_dict\"])\n        else:\n            self.model.load_state_dict(checkpoint)\n\n```\n\n\n",
        "eval_script": "# Mock necessary components to run the BaseTrainer class.\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn\n\n# Mock dataset and dataloader\nclass MockDataset(Dataset):\n    def __init__(self):\n        self.data = list(range(100))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        return self.data[index]\n\nmock_dataloader = DataLoader(MockDataset(), batch_size=10)\n\n# Mock model, criterion, optimizer, and scheduler\nclass MockModel(nn.Module):\n    def __init__(self):\n        super(MockModel, self).__init__()\n        self.layer = nn.Linear(1, 1)\n    \n    def forward(self, x):\n        return self.layer(x)\n\nmodel = MockModel()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n\n# Mock configuration and logger\nclass MockLogger:\n    def info(self, msg):\n        print(\"[INFO]\", msg)\n    \n    def warning(self, msg):\n        print(\"[WARNING]\", msg)\n    \n    def debug(self, msg):\n        print(\"[DEBUG]\", msg)\n\nconfig = {\n    \"trainer\": {\n        \"n_epochs\": 10,\n        \"log_step\": 10,\n        \"save_period\": 1,\n        \"monitor\": \"off\",\n        \"save_dir\": \"checkpoints\",\n        \"resume_from\": None,\n        \"from_pretrained\": None\n    },\n    \"writer\": {\n        \"loss_names\": [\"loss\"],\n        \"run_name\": \"mock_run\",\n        \"log_checkpoints\": False\n    }\n}\n\nlogger = MockLogger()\n\nclass MockWriter:\n    def set_step(self, epoch):\n        pass\n    \n    def add_scalar(self, key, value):\n        pass\n    \n    def add_checkpoint(self, filename, path):\n        pass\n\nwriter = MockWriter()\n\n# Mock metrics\nmetrics = {\n    \"train\": [],\n    \"inference\": []\n}\n\n# Now import and define other necessary constructs from context\n\nimport pandas as pd\nfrom numpy import inf\nfrom itertools import repeat\nfrom pathlib import Path\n\nROOT_PATH = Path(\".\")\n\nclass MetricTracker:\n    def __init__(self, *keys, writer=None):\n        self.writer = writer\n        self._data = pd.DataFrame(index=keys, columns=[\"total\", \"counts\", \"average\"])\n        self.reset()\n\n    def reset(self):\n        for col in self._data.columns:\n            self._data[col].values[:] = 0\n\n    def update(self, key, value, n=1):\n        self._data.loc[key, \"total\"] += value * n\n        self._data.loc[key, \"counts\"] += n\n        self._data.loc[key, \"average\"] = self._data.total[key] / self._data.counts[key]\n\n    def avg(self, key):\n        return self._data.average[key]\n\n    def result(self):\n        return dict(self._data.average)\n\n    def keys(self):\n        return self._data.total.keys()\n\ndef inf_loop(dataloader):\n    for loader in repeat(dataloader):\n        yield from loader\n\n# Now we can define the BaseTrainer class\n\nclass BaseTrainer:\n    # (The whole BaseTrainer class code remains unchanged as provided)\n\n    \"\"\"\n    Base class for all trainers.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        criterion,\n        metrics,\n        optimizer,\n        lr_scheduler,\n        config,\n        device,\n        dataloaders,\n        logger,\n        writer,\n        epoch_len=None,\n        skip_oom=True,\n        batch_transforms=None,\n    ):\n        self.is_train = True\n\n        self.config = config\n        self.cfg_trainer = self.config[\"trainer\"]\n\n        self.device = device\n        self.skip_oom = skip_oom\n\n        self.logger = logger\n        self.log_step = config[\"trainer\"].get(\"log_step\", 50)\n\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n\n        self.batch_transforms = batch_transforms\n\n        # define dataloaders\n        self.train_dataloader = dataloaders[\"train\"]\n        if epoch_len is None:\n            # epoch-based training\n            self.epoch_len = len(self.train_dataloader)\n        else:\n            # iteration-based training\n            self.train_dataloader = inf_loop(self.train_dataloader)\n            self.epoch_len = epoch_len\n\n        self.evaluation_dataloaders = {\n            k: v for k, v in dataloaders.items() if k != \"train\"\n        }\n\n        # define epochs\n        self._last_epoch = 0  # required for saving on interruption\n        self.start_epoch = 1\n        self.epochs = self.cfg_trainer[\"n_epochs\"]\n\n        # configuration to monitor model performance and save best\n\n        self.save_period = (\n            self.cfg_trainer[\"save_period\"]\n        )  # checkpoint each save_period epochs\n        self.monitor = self.cfg_trainer.get(\n            \"monitor\", \"off\"\n        )  # format: \"mnt_mode mnt_metric\"\n\n        if self.monitor == \"off\":\n            self.mnt_mode = \"off\"\n            self.mnt_best = 0\n        else:\n            self.mnt_mode, self.mnt_metric = self.monitor.split()\n            assert self.mnt_mode in [\"min\", \"max\"]\n\n            self.mnt_best = inf if self.mnt_mode == \"min\" else -inf\n            self.early_stop = self.cfg_trainer.get(\"early_stop\", inf)\n            if self.early_stop <= 0:\n                self.early_stop = inf\n\n        # setup visualization writer instance\n        self.writer = writer\n\n        # define metrics\n        self.metrics = metrics\n        self.train_metrics = MetricTracker(\n            *self.config[\"writer\"][\"loss_names\"],\n            \"grad_norm\",\n            *[m.name for m in self.metrics[\"train\"]],\n            writer=self.writer,\n        )\n        self.evaluation_metrics = MetricTracker(\n            *self.config[\"writer\"][\"loss_names\"],\n            *[m.name for m in self.metrics[\"inference\"]],\n            writer=self.writer,\n        )\n\n        # define checkpoint dir and init everything if required\n\n        self.checkpoint_dir = (\n            ROOT_PATH / config[\"trainer\"][\"save_dir\"] / config[\"writer\"][\"run_name\"]\n        )\n\n        if config[\"trainer\"].get(\"resume_from\") is not None:\n            resume_path = self.checkpoint_dir / config[\"trainer\"][\"resume_from\"]\n            self._resume_checkpoint(resume_path)\n\n        if config[\"trainer\"].get(\"from_pretrained\") is not None:\n            self._from_pretrained(config[\"trainer\"].get(\"from_pretrained\"))\n\n    def train(self):\n        try:\n            self._train_process()\n        except KeyboardInterrupt as e:\n            self.logger.info(\"Saving model on keyboard interrupt\")\n            self._save_checkpoint(self._last_epoch, save_best=False)\n            raise e\n\n    def _train_process(self):\n        not_improved_count = 0\n        for epoch in range(self.start_epoch, self.epochs + 1):\n            self._last_epoch = epoch\n            result = self._train_epoch(epoch)\n\n            logs = {\"epoch\": epoch}\n            logs.update(result)\n\n            for key, value in logs.items():\n                self.logger.info(f\"    {key:15s}: {value}\")\n\n            best, stop_process, not_improved_count = self._monitor_performance(\n                logs, not_improved_count\n            )\n\n            if epoch % self.save_period == 0 or best:\n                self._save_checkpoint(epoch, save_best=best, only_best=True)\n\n            if stop_process:  # early_stop\n                break\n\n    def _train_epoch(self, epoch):\n        self.is_train = True\n        self.model.train()\n        self.train_metrics.reset()\n        self.writer.set_step((epoch - 1) * self.epoch_len)\n        self.writer.add_scalar(\"epoch\", epoch)\n        for batch_idx, batch in enumerate(\n            tqdm(self.train_dataloader, desc=\"train\", total=self.epoch_len)\n        ):\n            try:\n                batch = self.process_batch(\n                    batch,\n                    metrics=self.train_metrics,\n                )\n            except torch.cuda.OutOfMemoryError as e:\n                if self.skip_oom:\n                    self.logger.warning(\"OOM on batch. Skipping batch.\")\n                    torch.cuda.empty_cache()  # free some memory\n                    continue\n                else:\n                    raise e\n\n            self.train_metrics.update(\"grad_norm\", self._get_grad_norm())\n\n            if batch_idx % self.log_step == 0:\n                self.writer.set_step((epoch - 1) * self.epoch_len + batch_idx)\n                self.logger.debug(\n                    \"Train Epoch: {} {} Loss: {:.6f}\".format(\n                        epoch, self._progress(batch_idx), batch[\"loss\"].item()\n                    )\n                )\n                self.writer.add_scalar(\n                    \"learning rate\", self.lr_scheduler.get_last_lr()[0]\n                )\n                self._log_scalars(self.train_metrics)\n                self._log_batch(batch_idx, batch)\n                last_train_metrics = self.train_metrics.result()\n                self.train_metrics.reset()\n            if batch_idx + 1 >= self.epoch_len:\n                break\n\n        logs = last_train_metrics\n\n        for part, dataloader in self.evaluation_dataloaders.items():\n            val_logs = self._evaluation_epoch(epoch, part, dataloader)\n            logs.update(**{f\"{part}_{name}\": value for name, value in val_logs.items()})\n\n        return logs\n\n    def _evaluation_epoch(self, epoch, part, dataloader):\n        self.is_train = False\n        self.model.eval()\n        self.evaluation_metrics.reset()\n        with torch.no_grad():\n            for batch_idx, batch in tqdm(\n                enumerate(dataloader),\n                desc=part,\n                total=len(dataloader),\n            ):\n                batch = self.process_batch(\n                    batch,\n                    metrics=self.evaluation_metrics,\n                )\n            self.writer.set_step(epoch * self.epoch_len, part)\n            self._log_scalars(self.evaluation_metrics)\n            self._log_batch(\n                batch_idx, batch, part\n            )\n\n        return self.evaluation_metrics.result()\n\n    def _monitor_performance(self, logs, not_improved_count):\n        best = False\n        stop_process = False\n        if self.mnt_mode != \"off\":\n            try:\n                if self.mnt_mode == \"min\":\n                    improved = logs[self.mnt_metric] <= self.mnt_best\n                elif self.mnt_mode == \"max\":\n                    improved = logs[self.mnt_metric] >= self.mnt_best\n                else:\n                    improved = False\n            except KeyError:\n                self.logger.warning(\n                    f\"Warning: Metric '{self.mnt_metric}' is not found. \"\n                    \"Model performance monitoring is disabled.\"\n                )\n                self.mnt_mode = \"off\"\n                improved = False\n\n            if improved:\n                self.mnt_best = logs[self.mnt_metric]\n                not_improved_count = 0\n                best = True\n            else:\n                not_improved_count += 1\n\n            if not_improved_count >= self.early_stop:\n                self.logger.info(\n                    \"Validation performance didn't improve for {} epochs. \"\n                    \"Training stops.\".format(self.early_stop)\n                )\n                stop_process = True\n        return best, stop_process, not_improved_count\n\n    def move_batch_to_device(self, batch):\n        for tensor_for_device in self.cfg_trainer.get(\"device_tensors\", []):\n            batch[tensor_for_device] = batch[tensor_for_device].to(self.device)\n        return batch\n\n    def transform_batch(self, batch):\n        return batch\n\n    def _clip_grad_norm(self):\n        max_grad_norm = self.config[\"trainer\"].get(\"max_grad_norm\", None)\n        if max_grad_norm is not None:\n            clip_grad_norm_(\n                self.model.parameters(), max_grad_norm\n            )\n\n    @torch.no_grad()\n    def _get_grad_norm(self, norm_type=2):\n        parameters = self.model.parameters()\n        if isinstance(parameters, torch.Tensor):\n            parameters = [parameters]\n        parameters = [p for p in parameters if p.grad is not None]\n        total_norm = torch.norm(\n            torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n            norm_type,\n        )\n        return total_norm.item()\n\n    def _progress(self, batch_idx):\n        base = \"[{}/{} ({:.0f}%)]\"\n        if hasattr(self.train_dataloader, \"n_samples\"):\n            current = batch_idx * self.train_dataloader.batch_size\n            total = self.train_dataloader.n_samples\n        else:\n            current = batch_idx\n            total = self.epoch_len\n        return base.format(current, total, 100.0 * current / total)\n\n\n\n    def _log_batch(self, batch_idx, batch, mode=\"train\"):\n        raise NotImplementedError()\n\n    def _log_scalars(self, metric_tracker: MetricTracker):\n        if self.writer is None:\n            return\n        for metric_name in metric_tracker.keys():\n            self.writer.add_scalar(f\"{metric_name}\", metric_tracker.avg(metric_name))\n\n    def _save_checkpoint(self, epoch, save_best=False, only_best=False):\n        arch = type(self.model).__name__\n        state = {\n            \"arch\": arch,\n            \"epoch\": epoch,\n            \"state_dict\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"lr_scheduler\": self.lr_scheduler.state_dict(),\n            \"monitor_best\": self.mnt_best,\n            \"config\": self.config,\n        }\n        filename = str(self.checkpoint_dir / f\"checkpoint-epoch{epoch}.pth\")\n        if not (only_best and save_best):\n            torch.save(state, filename)\n            if self.config[\"writer\"].get(\"log_checkpoints\", False):\n                self.writer.add_checkpoint(filename, str(self.checkpoint_dir.parent))\n            self.logger.info(f\"Saving checkpoint: {filename} ...\")\n        if save_best:\n            best_path = str(self.checkpoint_dir / \"model_best.pth\")\n            torch.save(state, best_path)\n            if self.config[\"writer\"].get(\"log_checkpoints\", False):\n                self.writer.add_checkpoint(best_path, str(self.checkpoint_dir.parent))\n            self.logger.info(\"Saving current best: model_best.pth ...\")\n\n    def _resume_checkpoint(self, resume_path):\n        resume_path = str(resume_path)\n        self.logger.info(f\"Loading checkpoint: {resume_path} ...\")\n        checkpoint = torch.load(resume_path, self.device)\n        self.start_epoch = checkpoint[\"epoch\"] + 1\n        self.mnt_best = checkpoint[\"monitor_best\"]\n\n        if checkpoint[\"config\"][\"model\"] != self.config[\"model\"]:\n            self.logger.warning(\n                \"Warning: Architecture configuration given in the config file is different from that \"\n                \"of the checkpoint. This may yield an exception when state_dict is loaded.\"\n            )\n        self.model.load_state_dict(checkpoint[\"state_dict\"])\n\n        if (\n            checkpoint[\"config\"][\"optimizer\"] != self.config[\"optimizer\"]\n            or checkpoint[\"config\"][\"lr_scheduler\"] != self.config[\"lr_scheduler\"]\n        ):\n            self.logger.warning(\n                \"Warning: Optimizer or lr_scheduler given in the config file is different \"\n                \"from that of the checkpoint. Optimizer and scheduler parameters \"\n                \"are not resumed.\"\n            )\n        else:\n            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n\n        self.logger.info(\n            f\"Checkpoint loaded. Resume training from epoch {self.start_epoch}\"\n        )\n\n    def _from_pretrained(self, pretrained_path):\n        pretrained_path = str(pretrained_path)\n        if hasattr(self, \"logger\"):\n            self.logger.info(f\"Loading model weights from: {pretrained_path} ...\")\n        else:\n            print(f\"Loading model weights from: {pretrained_path} ...\")\n        checkpoint = torch.load(pretrained_path, self.device)\n\n        if checkpoint.get(\"state_dict\") is not None:\n            self.model.load_state_dict(checkpoint[\"state_dict\"])\n        else:\n            self.model.load_state_dict(checkpoint)\n\n# Function to test _progress implementations\ndef test__progress():\n    device = 'cpu'\n    dataloaders = {\n        \"train\": mock_dataloader,\n        \"val\": mock_dataloader,\n    }\n    trainer = BaseTrainer(\n        model=model,\n        criterion=criterion,\n        metrics=metrics,\n        optimizer=optimizer,\n        lr_scheduler=scheduler,\n        config=config,\n        device=device,\n        dataloaders=dataloaders,\n        logger=logger,\n        writer=writer\n    )\n\n    # Test cases with assert statements\n    assert trainer._progress(5) == trainer._progress_new_implementation(5), \"Test Case 1 failed\"\n    assert trainer._progress(0) == trainer._progress_new_implementation(0), \"Test Case 2 failed\"\n    assert trainer._progress(trainer.epoch_len) == trainer._progress_new_implementation(trainer.epoch_len), \"Test Case 3 failed\"\n\nif __name__ == \"__main__\":\n    test__progress()\n    print(\"All test cases passed!\")"
    },
    {
        "func_name": "BaseTrainer._from_pretrained",
        "idx": "49",
        "repo_name": "AntonNuzhdin___source_separation",
        "func_path": "src/trainer/base_trainer.py",
        "orig_func": "def _from_pretrained(self, pretrained_path):\n    \"\"\"\n        Init model with weights from pretrained pth file.\n\n        Notice that 'pretrained_path' can be any path on the disk. It is not\n        necessary to locate it in the experiment saved dir. The function\n        initializes only the model.\n\n        Args:\n            pretrained_path (str): path to the model state dict.\n        \"\"\"\n    pretrained_path = str(pretrained_path)\n    if hasattr(self, 'logger'):\n        self.logger.info(f'Loading model weights from: {pretrained_path} ...')\n    else:\n        print(f'Loading model weights from: {pretrained_path} ...')\n    checkpoint = torch.load(pretrained_path, self.device)\n    if checkpoint.get('state_dict') is not None:\n        self.model.load_state_dict(checkpoint['state_dict'])\n    else:\n        self.model.load_state_dict(checkpoint)",
        "orig_context": "```python\n## src/metrics/tracker.py\nimport pandas as pd\n\nclass MetricTracker:\n    \"\"\"\n    Class to aggregate metrics from many batches.\n    \"\"\"\n\n    def __init__(self, *keys, writer=None):\n        \"\"\"\n        Args:\n            *keys (list[str]): list (as positional arguments) of metric\n                names (may include the names of losses)\n            writer (WandBWriter | CometMLWriter | None): experiment tracker.\n                Not used in this code version. Can be used to log metrics\n                from each batch.\n        \"\"\"\n        self.writer = writer\n        self._data = pd.DataFrame(index=keys, columns=[\"total\", \"counts\", \"average\"])\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Reset all metrics after epoch end.\n        \"\"\"\n        for col in self._data.columns:\n            self._data[col].values[:] = 0\n\n    def update(self, key, value, n=1):\n        \"\"\"\n        Update metrics DataFrame with new value.\n\n        Args:\n            key (str): metric name.\n            value (float): metric value on the batch.\n            n (int): how many times to count this value.\n        \"\"\"\n        # if self.writer is not None:\n        #     self.writer.add_scalar(key, value)\n        self._data.loc[key, \"total\"] += value * n\n        self._data.loc[key, \"counts\"] += n\n        self._data.loc[key, \"average\"] = self._data.total[key] / self._data.counts[key]\n\n    def avg(self, key):\n        \"\"\"\n        Return average value for a given metric.\n\n        Args:\n            key (str): metric name.\n        Returns:\n            average_value (float): average value for the metric.\n        \"\"\"\n        return self._data.average[key]\n\n    def result(self):\n        \"\"\"\n        Return average value of each metric.\n\n        Returns:\n            average_metrics (dict): dict, containing average metrics\n                for each metric name.\n        \"\"\"\n        return dict(self._data.average)\n\n    def keys(self):\n        \"\"\"\n        Return all metric names defined in the MetricTracker.\n\n        Returns:\n            metric_keys (Index): all metric names in the table.\n        \"\"\"\n        return self._data.total.keys()\n\n```\n\n\n```python\n## src/utils/io_utils.py\nfrom pathlib import Path\n\nROOT_PATH = Path(__file__).absolute().resolve().parent.parent.parent\n\n```\n\n\n```python\n## src/datasets/data_utils.py\nfrom itertools import repeat\n\ndef inf_loop(dataloader):\n    \"\"\"\n    Wrapper function for endless dataloader.\n    Used for iteration-based training scheme.\n\n    Args:\n        dataloader (DataLoader): classic finite dataloader.\n    \"\"\"\n    for loader in repeat(dataloader):\n        yield from loader\n\n```\n\n\n```python\n## src/trainer/base_trainer.py\nfrom abc import abstractmethod\n\nimport torch\n\nfrom numpy import inf\n\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom tqdm.auto import tqdm\n\nfrom src.datasets.data_utils import inf_loop\n\nfrom src.metrics.tracker import MetricTracker\n\nfrom src.utils.io_utils import ROOT_PATH\n\nclass BaseTrainer:\n    \"\"\"\n    Base class for all trainers.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        criterion,\n        metrics,\n        optimizer,\n        lr_scheduler,\n        config,\n        device,\n        dataloaders,\n        logger,\n        writer,\n        epoch_len=None,\n        skip_oom=True,\n        batch_transforms=None,\n    ):\n        \"\"\"\n        Args:\n            model (nn.Module): PyTorch model.\n            criterion (nn.Module): loss function for model training.\n            metrics (dict): dict with the definition of metrics for training\n                (metrics[train]) and inference (metrics[inference]). Each\n                metric is an instance of src.metrics.BaseMetric.\n            optimizer (Optimizer): optimizer for the model.\n            lr_scheduler (LRScheduler): learning rate scheduler for the\n                optimizer.\n            text_encoder (CTCTextEncoder): text encoder.\n            config (DictConfig): experiment config containing training config.\n            device (str): device for tensors and model.\n            dataloaders (dict[DataLoader]): dataloaders for different\n                sets of data.\n            logger (Logger): logger that logs output.\n            writer (WandBWriter | CometMLWriter): experiment tracker.\n            epoch_len (int | None): number of steps in each epoch for\n                iteration-based training. If None, use epoch-based\n                training (len(dataloader)).\n            skip_oom (bool): skip batches with the OutOfMemory error.\n            batch_transforms (dict[Callable] | None): transforms that\n                should be applied on the whole batch. Depend on the\n                tensor name.\n        \"\"\"\n        self.is_train = True\n\n        self.config = config\n        self.cfg_trainer = self.config.trainer\n\n        self.device = device\n        self.skip_oom = skip_oom\n\n        self.logger = logger\n        self.log_step = config.trainer.get(\"log_step\", 50)\n\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n\n        self.batch_transforms = batch_transforms\n\n        # define dataloaders\n        self.train_dataloader = dataloaders[\"train\"]\n        if epoch_len is None:\n            # epoch-based training\n            self.epoch_len = len(self.train_dataloader)\n        else:\n            # iteration-based training\n            self.train_dataloader = inf_loop(self.train_dataloader)\n            self.epoch_len = epoch_len\n\n        self.evaluation_dataloaders = {\n            k: v for k, v in dataloaders.items() if k != \"train\"\n        }\n\n        # define epochs\n        self._last_epoch = 0  # required for saving on interruption\n        self.start_epoch = 1\n        self.epochs = self.cfg_trainer.n_epochs\n\n        # configuration to monitor model performance and save best\n\n        self.save_period = (\n            self.cfg_trainer.save_period\n        )  # checkpoint each save_period epochs\n        self.monitor = self.cfg_trainer.get(\n            \"monitor\", \"off\"\n        )  # format: \"mnt_mode mnt_metric\"\n\n        if self.monitor == \"off\":\n            self.mnt_mode = \"off\"\n            self.mnt_best = 0\n        else:\n            self.mnt_mode, self.mnt_metric = self.monitor.split()\n            assert self.mnt_mode in [\"min\", \"max\"]\n\n            self.mnt_best = inf if self.mnt_mode == \"min\" else -inf\n            self.early_stop = self.cfg_trainer.get(\"early_stop\", inf)\n            if self.early_stop <= 0:\n                self.early_stop = inf\n\n        # setup visualization writer instance\n        self.writer = writer\n\n        # define metrics\n        self.metrics = metrics\n        self.train_metrics = MetricTracker(\n            *self.config.writer.loss_names,\n            \"grad_norm\",\n            *[m.name for m in self.metrics[\"train\"]],\n            writer=self.writer,\n        )\n        self.evaluation_metrics = MetricTracker(\n            *self.config.writer.loss_names,\n            *[m.name for m in self.metrics[\"inference\"]],\n            writer=self.writer,\n        )\n\n        # define checkpoint dir and init everything if required\n\n        self.checkpoint_dir = (\n            ROOT_PATH / config.trainer.save_dir / config.writer.run_name\n        )\n\n        if config.trainer.get(\"resume_from\") is not None:\n            resume_path = self.checkpoint_dir / config.trainer.resume_from\n            self._resume_checkpoint(resume_path)\n\n        if config.trainer.get(\"from_pretrained\") is not None:\n            self._from_pretrained(config.trainer.get(\"from_pretrained\"))\n\n    def train(self):\n        \"\"\"\n        Wrapper around training process to save model on keyboard interrupt.\n        \"\"\"\n        try:\n            self._train_process()\n        except KeyboardInterrupt as e:\n            self.logger.info(\"Saving model on keyboard interrupt\")\n            self._save_checkpoint(self._last_epoch, save_best=False)\n            raise e\n\n    def _train_process(self):\n        \"\"\"\n        Full training logic:\n\n        Training model for an epoch, evaluating it on non-train partitions,\n        and monitoring the performance improvement (for early stopping\n        and saving the best checkpoint).\n        \"\"\"\n        not_improved_count = 0\n        for epoch in range(self.start_epoch, self.epochs + 1):\n            self._last_epoch = epoch\n            result = self._train_epoch(epoch)\n\n            # save logged information into logs dict\n            logs = {\"epoch\": epoch}\n            logs.update(result)\n\n            # print logged information to the screen\n            for key, value in logs.items():\n                self.logger.info(f\"    {key:15s}: {value}\")\n\n            # evaluate model performance according to configured metric,\n            # save best checkpoint as model_best\n            best, stop_process, not_improved_count = self._monitor_performance(\n                logs, not_improved_count\n            )\n\n            if epoch % self.save_period == 0 or best:\n                self._save_checkpoint(epoch, save_best=best, only_best=True)\n\n            if stop_process:  # early_stop\n                break\n\n    def _train_epoch(self, epoch):\n        \"\"\"\n        Training logic for an epoch, including logging and evaluation on\n        non-train partitions.\n\n        Args:\n            epoch (int): current training epoch.\n        Returns:\n            logs (dict): logs that contain the average loss and metric in\n                this epoch.\n        \"\"\"\n        self.is_train = True\n        self.model.train()\n        self.train_metrics.reset()\n        self.writer.set_step((epoch - 1) * self.epoch_len)\n        self.writer.add_scalar(\"epoch\", epoch)\n        for batch_idx, batch in enumerate(\n            tqdm(self.train_dataloader, desc=\"train\", total=self.epoch_len)\n        ):\n            try:\n                batch = self.process_batch(\n                    batch,\n                    metrics=self.train_metrics,\n                )\n            except torch.cuda.OutOfMemoryError as e:\n                if self.skip_oom:\n                    self.logger.warning(\"OOM on batch. Skipping batch.\")\n                    torch.cuda.empty_cache()  # free some memory\n                    continue\n                else:\n                    raise e\n\n            self.train_metrics.update(\"grad_norm\", self._get_grad_norm())\n\n            # log current results\n            if batch_idx % self.log_step == 0:\n                self.writer.set_step((epoch - 1) * self.epoch_len + batch_idx)\n                self.logger.debug(\n                    \"Train Epoch: {} {} Loss: {:.6f}\".format(\n                        epoch, self._progress(batch_idx), batch[\"loss\"].item()\n                    )\n                )\n                self.writer.add_scalar(\n                    \"learning rate\", self.lr_scheduler.get_last_lr()[0]\n                )\n                self._log_scalars(self.train_metrics)\n                self._log_batch(batch_idx, batch)\n                # we don't want to reset train metrics at the start of every epoch\n                # because we are interested in recent train metrics\n                last_train_metrics = self.train_metrics.result()\n                self.train_metrics.reset()\n            if batch_idx + 1 >= self.epoch_len:\n                break\n\n        logs = last_train_metrics\n\n        # Run val/test\n        for part, dataloader in self.evaluation_dataloaders.items():\n            val_logs = self._evaluation_epoch(epoch, part, dataloader)\n            logs.update(**{f\"{part}_{name}\": value for name, value in val_logs.items()})\n\n        return logs\n\n    def _evaluation_epoch(self, epoch, part, dataloader):\n        \"\"\"\n        Evaluate model on the partition after training for an epoch.\n\n        Args:\n            epoch (int): current training epoch.\n            part (str): partition to evaluate on\n            dataloader (DataLoader): dataloader for the partition.\n        Returns:\n            logs (dict): logs that contain the information about evaluation.\n        \"\"\"\n        self.is_train = False\n        self.model.eval()\n        self.evaluation_metrics.reset()\n        with torch.no_grad():\n            for batch_idx, batch in tqdm(\n                enumerate(dataloader),\n                desc=part,\n                total=len(dataloader),\n            ):\n                batch = self.process_batch(\n                    batch,\n                    metrics=self.evaluation_metrics,\n                )\n            self.writer.set_step(epoch * self.epoch_len, part)\n            self._log_scalars(self.evaluation_metrics)\n            self._log_batch(\n                batch_idx, batch, part\n            )  # log only the last batch during inference\n\n        return self.evaluation_metrics.result()\n\n    def _monitor_performance(self, logs, not_improved_count):\n        \"\"\"\n        Check if there is an improvement in the metrics. Used for early\n        stopping and saving the best checkpoint.\n\n        Args:\n            logs (dict): logs after training and evaluating the model for\n                an epoch.\n            not_improved_count (int): the current number of epochs without\n                improvement.\n        Returns:\n            best (bool): if True, the monitored metric has improved.\n            stop_process (bool): if True, stop the process (early stopping).\n                The metric did not improve for too much epochs.\n            not_improved_count (int): updated number of epochs without\n                improvement.\n        \"\"\"\n        best = False\n        stop_process = False\n        if self.mnt_mode != \"off\":\n            try:\n                # check whether model performance improved or not,\n                # according to specified metric(mnt_metric)\n                if self.mnt_mode == \"min\":\n                    improved = logs[self.mnt_metric] <= self.mnt_best\n                elif self.mnt_mode == \"max\":\n                    improved = logs[self.mnt_metric] >= self.mnt_best\n                else:\n                    improved = False\n            except KeyError:\n                self.logger.warning(\n                    f\"Warning: Metric '{self.mnt_metric}' is not found. \"\n                    \"Model performance monitoring is disabled.\"\n                )\n                self.mnt_mode = \"off\"\n                improved = False\n\n            if improved:\n                self.mnt_best = logs[self.mnt_metric]\n                not_improved_count = 0\n                best = True\n            else:\n                not_improved_count += 1\n\n            if not_improved_count >= self.early_stop:\n                self.logger.info(\n                    \"Validation performance didn't improve for {} epochs. \"\n                    \"Training stops.\".format(self.early_stop)\n                )\n                stop_process = True\n        return best, stop_process, not_improved_count\n\n    def move_batch_to_device(self, batch):\n        \"\"\"\n        Move all necessary tensors to the device.\n\n        Args:\n            batch (dict): dict-based batch containing the data from\n                the dataloader.\n        Returns:\n            batch (dict): dict-based batch containing the data from\n                the dataloader with some of the tensors on the device.\n        \"\"\"\n        for tensor_for_device in self.cfg_trainer.device_tensors:\n            batch[tensor_for_device] = batch[tensor_for_device].to(self.device)\n        return batch\n\n    def transform_batch(self, batch):\n        \"\"\"\n        Transforms elements in batch. Like instance transform inside the\n        BaseDataset class, but for the whole batch. Improves pipeline speed,\n        especially if used with a GPU.\n\n        Each tensor in a batch undergoes its own transform defined by the key.\n\n        Args:\n            batch (dict): dict-based batch containing the data from\n                the dataloader.\n        Returns:\n            batch (dict): dict-based batch containing the data from\n                the dataloader (possibly transformed via batch transform).\n        \"\"\"\n        # do batch transforms on device\n        # transform_type = \"train\" if self.is_train else \"inference\"\n        # transforms = self.batch_transforms.get(transform_type)\n        # if transforms is not None:\n        #     for transform_name in transforms.keys():\n        #         batch[transform_name] = transforms[transform_name](\n        #             batch[transform_name]\n        #         )\n        return batch\n\n    def _clip_grad_norm(self):\n        \"\"\"\n        Clips the gradient norm by the value defined in\n        config.trainer.max_grad_norm\n        \"\"\"\n        if self.config[\"trainer\"].get(\"max_grad_norm\", None) is not None:\n            clip_grad_norm_(\n                self.model.parameters(), self.config[\"trainer\"][\"max_grad_norm\"]\n            )\n\n    @torch.no_grad()\n    def _get_grad_norm(self, norm_type=2):\n        \"\"\"\n        Calculates the gradient norm for logging.\n\n        Args:\n            norm_type (float | str | None): the order of the norm.\n        Returns:\n            total_norm (float): the calculated norm.\n        \"\"\"\n        parameters = self.model.parameters()\n        if isinstance(parameters, torch.Tensor):\n            parameters = [parameters]\n        parameters = [p for p in parameters if p.grad is not None]\n        total_norm = torch.norm(\n            torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n            norm_type,\n        )\n        return total_norm.item()\n\n    def _progress(self, batch_idx):\n        \"\"\"\n        Calculates the percentage of processed batch within the epoch.\n\n        Args:\n            batch_idx (int): the current batch index.\n        Returns:\n            progress (str): contains current step and percentage\n                within the epoch.\n        \"\"\"\n        base = \"[{}/{} ({:.0f}%)]\"\n        if hasattr(self.train_dataloader, \"n_samples\"):\n            current = batch_idx * self.train_dataloader.batch_size\n            total = self.train_dataloader.n_samples\n        else:\n            current = batch_idx\n            total = self.epoch_len\n        return base.format(current, total, 100.0 * current / total)\n\n    @abstractmethod\n    def _log_batch(self, batch_idx, batch, mode=\"train\"):\n        \"\"\"\n        Abstract method. Should be defined in the nested Trainer Class.\n\n        Log data from batch. Calls self.writer.add_* to log data\n        to the experiment tracker.\n\n        Args:\n            batch_idx (int): index of the current batch.\n            batch (dict): dict-based batch after going through\n                the 'process_batch' function.\n            mode (str): train or inference. Defines which logging\n                rules to apply.\n        \"\"\"\n        return NotImplementedError()\n\n    def _log_scalars(self, metric_tracker: MetricTracker):\n        \"\"\"\n        Wrapper around the writer 'add_scalar' to log all metrics.\n\n        Args:\n            metric_tracker (MetricTracker): calculated metrics.\n        \"\"\"\n        if self.writer is None:\n            return\n        for metric_name in metric_tracker.keys():\n            self.writer.add_scalar(f\"{metric_name}\", metric_tracker.avg(metric_name))\n\n    def _save_checkpoint(self, epoch, save_best=False, only_best=False):\n        \"\"\"\n        Save the checkpoints.\n\n        Args:\n            epoch (int): current epoch number.\n            save_best (bool): if True, rename the saved checkpoint to 'model_best.pth'.\n            only_best (bool): if True and the checkpoint is the best, save it only as\n                'model_best.pth'(do not duplicate the checkpoint as\n                checkpoint-epochEpochNumber.pth)\n        \"\"\"\n        arch = type(self.model).__name__\n        state = {\n            \"arch\": arch,\n            \"epoch\": epoch,\n            \"state_dict\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"lr_scheduler\": self.lr_scheduler.state_dict(),\n            \"monitor_best\": self.mnt_best,\n            \"config\": self.config,\n        }\n        filename = str(self.checkpoint_dir / f\"checkpoint-epoch{epoch}.pth\")\n        if not (only_best and save_best):\n            torch.save(state, filename)\n            if self.config.writer.log_checkpoints:\n                self.writer.add_checkpoint(filename, str(self.checkpoint_dir.parent))\n            self.logger.info(f\"Saving checkpoint: {filename} ...\")\n        if save_best:\n            best_path = str(self.checkpoint_dir / \"model_best.pth\")\n            torch.save(state, best_path)\n            if self.config.writer.log_checkpoints:\n                self.writer.add_checkpoint(best_path, str(self.checkpoint_dir.parent))\n            self.logger.info(\"Saving current best: model_best.pth ...\")\n\n    def _resume_checkpoint(self, resume_path):\n        \"\"\"\n        Resume from a saved checkpoint (in case of server crash, etc.).\n        The function loads state dicts for everything, including model,\n        optimizers, etc.\n\n        Notice that the checkpoint should be located in the current experiment\n        saved directory (where all checkpoints are saved in '_save_checkpoint').\n\n        Args:\n            resume_path (str): Path to the checkpoint to be resumed.\n        \"\"\"\n        resume_path = str(resume_path)\n        self.logger.info(f\"Loading checkpoint: {resume_path} ...\")\n        checkpoint = torch.load(resume_path, self.device)\n        self.start_epoch = checkpoint[\"epoch\"] + 1\n        self.mnt_best = checkpoint[\"monitor_best\"]\n\n        # load architecture params from checkpoint.\n        if checkpoint[\"config\"][\"model\"] != self.config[\"model\"]:\n            self.logger.warning(\n                \"Warning: Architecture configuration given in the config file is different from that \"\n                \"of the checkpoint. This may yield an exception when state_dict is loaded.\"\n            )\n        self.model.load_state_dict(checkpoint[\"state_dict\"])\n\n        # load optimizer state from checkpoint only when optimizer type is not changed.\n        if (\n            checkpoint[\"config\"][\"optimizer\"] != self.config[\"optimizer\"]\n            or checkpoint[\"config\"][\"lr_scheduler\"] != self.config[\"lr_scheduler\"]\n        ):\n            self.logger.warning(\n                \"Warning: Optimizer or lr_scheduler given in the config file is different \"\n                \"from that of the checkpoint. Optimizer and scheduler parameters \"\n                \"are not resumed.\"\n            )\n        else:\n            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n\n        self.logger.info(\n            f\"Checkpoint loaded. Resume training from epoch {self.start_epoch}\"\n        )\n\n    def _from_pretrained(self, pretrained_path):\n        \"\"\"\n        Init model with weights from pretrained pth file.\n\n        Notice that 'pretrained_path' can be any path on the disk. It is not\n        necessary to locate it in the experiment saved dir. The function\n        initializes only the model.\n\n        Args:\n            pretrained_path (str): path to the model state dict.\n        \"\"\"\n        pretrained_path = str(pretrained_path)\n        if hasattr(self, \"logger\"):  # to support both trainer and inferencer\n            self.logger.info(f\"Loading model weights from: {pretrained_path} ...\")\n        else:\n            print(f\"Loading model weights from: {pretrained_path} ...\")\n        checkpoint = torch.load(pretrained_path, self.device)\n\n        if checkpoint.get(\"state_dict\") is not None:\n            self.model.load_state_dict(checkpoint[\"state_dict\"])\n        else:\n            self.model.load_state_dict(checkpoint)\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\n\n# Mock implementations and imports based on the provided context\nimport pandas as pd\nimport torch\nfrom torch.nn import Module, MSELoss\nfrom torch.optim import Optimizer, Adam, lr_scheduler\nfrom collections import defaultdict\nfrom numpy import inf\nfrom pathlib import Path\nfrom itertools import repeat\n\n# Utility classes and functions from context\ndef inf_loop(dataloader):\n    for loader in repeat(dataloader):\n        yield from loader\n        \nclass MetricTracker:\n    def __init__(self, *keys, writer=None):\n        self.writer = writer\n        self._data = pd.DataFrame(index=keys, columns=[\"total\", \"counts\", \"average\"])\n        self.reset()\n\n    def reset(self):\n        for col in self._data.columns:\n            self._data[col].values[:] = 0\n\n    def update(self, key, value, n=1):\n        self._data.loc[key, \"total\"] += value * n\n        self._data.loc[key, \"counts\"] += n\n        self._data.loc[key, \"average\"] = self._data.total[key] / self._data.counts[key]\n\n    def avg(self, key):\n        return self._data.average[key]\n\n    def result(self):\n        return dict(self._data.average)\n\n    def keys(self):\n        return self._data.total.keys()\n\nclass MockWriter:\n    def set_step(self, step, mode='train'):\n        pass\n\n    def add_scalar(self, name, value):\n        pass\n\n    def add_checkpoint(self, path, run_dir):\n        pass\n\nclass MockLogger:\n    def info(self, msg):\n        print(msg)\n\n    def warning(self, msg):\n        print(f\"Warning: {msg}\")\n\n    def debug(self, msg):\n        print(f\"Debug: {msg}\")\n\nROOT_PATH = Path(__file__).absolute().resolve().parent\n\n# The main BaseTrainer class provided\nclass BaseTrainer:\n\n    def __init__(\n        self,\n        model,\n        criterion,\n        metrics,\n        optimizer,\n        lr_scheduler,\n        config,\n        device,\n        dataloaders,\n        logger,\n        writer,\n        epoch_len=None,\n        skip_oom=True,\n        batch_transforms=None,\n    ):\n        self.is_train = True\n\n        self.config = config\n        self.cfg_trainer = self.config['trainer']\n\n        self.device = device\n        self.skip_oom = skip_oom\n\n        self.logger = logger\n        self.log_step = config['trainer'].get(\"log_step\", 50)\n\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n\n        self.batch_transforms = batch_transforms\n\n        self.train_dataloader = dataloaders[\"train\"]\n        if epoch_len is None:\n            self.epoch_len = len(self.train_dataloader)\n        else:\n            self.train_dataloader = inf_loop(self.train_dataloader)\n            self.epoch_len = epoch_len\n\n        self.evaluation_dataloaders = {k: v for k, v in dataloaders.items() if k != \"train\"}\n\n        self._last_epoch = 0\n        self.start_epoch = 1\n        self.epochs = self.cfg_trainer['n_epochs']\n\n        self.save_period = self.cfg_trainer['save_period']\n        self.monitor = self.cfg_trainer.get(\"monitor\", \"off\")\n\n        if self.monitor == \"off\":\n            self.mnt_mode = \"off\"\n            self.mnt_best = 0\n        else:\n            self.mnt_mode, self.mnt_metric = self.monitor.split()\n            assert self.mnt_mode in [\"min\", \"max\"]\n\n            self.mnt_best = inf if self.mnt_mode == \"min\" else -inf\n            self.early_stop = self.cfg_trainer.get(\"early_stop\", inf)\n            if self.early_stop <= 0:\n                self.early_stop = inf\n\n        self.writer = writer\n\n        self.metrics = metrics\n        self.train_metrics = MetricTracker(\n            *self.config['writer']['loss_names'],\n            \"grad_norm\",\n            *[m.name for m in self.metrics[\"train\"]],\n            writer=self.writer,\n        )\n        self.evaluation_metrics = MetricTracker(\n            *self.config['writer']['loss_names'],\n            *[m.name for m in self.metrics[\"inference\"]],\n            writer=self.writer,\n        )\n\n        self.checkpoint_dir = ROOT_PATH / config['trainer']['save_dir'] / config['writer']['run_name']\n\n        if config['trainer'].get(\"resume_from\") is not None:\n            resume_path = self.checkpoint_dir / config['trainer']['resume_from']\n            self._resume_checkpoint(resume_path)\n\n        if config['trainer'].get(\"from_pretrained\") is not None:\n            self._from_pretrained(config['trainer'].get(\"from_pretrained\"))\n\n    def train(self):\n        try:\n            self._train_process()\n        except KeyboardInterrupt as e:\n            self.logger.info(\"Saving model on keyboard interrupt\")\n            self._save_checkpoint(self._last_epoch, save_best=False)\n            raise e\n\n    def _train_process(self):\n        not_improved_count = 0\n        for epoch in range(self.start_epoch, self.epochs + 1):\n            self._last_epoch = epoch\n            result = self._train_epoch(epoch)\n            logs = {\"epoch\": epoch}\n            logs.update(result)\n            for key, value in logs.items():\n                self.logger.info(f\"    {key:15s}: {value}\")\n            best, stop_process, not_improved_count = self._monitor_performance(logs, not_improved_count)\n            if epoch % self.save_period == 0 or best:\n                self._save_checkpoint(epoch, save_best=best, only_best=True)\n            if stop_process:\n                break\n\n    def _train_epoch(self, epoch):\n        self.is_train = True\n        self.model.train()\n        self.train_metrics.reset()\n        self.writer.set_step((epoch - 1) * self.epoch_len)\n        self.writer.add_scalar(\"epoch\", epoch)\n        last_train_metrics = None\n        for batch_idx, batch in enumerate(\n            tqdm(self.train_dataloader, desc=\"train\", total=self.epoch_len)\n        ):\n            try:\n                batch = self.process_batch(batch, metrics=self.train_metrics)\n            except torch.cuda.OutOfMemoryError as e:\n                if self.skip_oom:\n                    self.logger.warning(\"OOM on batch. Skipping batch.\")\n                    torch.cuda.empty_cache()\n                    continue\n                else:\n                    raise e\n            self.train_metrics.update(\"grad_norm\", self._get_grad_norm())\n            if batch_idx % self.log_step == 0:\n                self.writer.set_step((epoch - 1) * self.epoch_len + batch_idx)\n                self.logger.debug(\n                    \"Train Epoch: {} {} Loss: {:.6f}\".format(\n                        epoch, self._progress(batch_idx), batch[\"loss\"].item()\n                    )\n                )\n                self.writer.add_scalar(\n                    \"learning rate\", self.lr_scheduler.get_last_lr()[0]\n                )\n                self._log_scalars(self.train_metrics)\n                self._log_batch(batch_idx, batch)\n                last_train_metrics = self.train_metrics.result()\n                self.train_metrics.reset()\n            if batch_idx + 1 >= self.epoch_len:\n                break\n        logs = last_train_metrics\n        for part, dataloader in self.evaluation_dataloaders.items():\n            val_logs = self._evaluation_epoch(epoch, part, dataloader)\n            logs.update(**{f\"{part}_{name}\": value for name, value in val_logs.items()})\n        return logs\n\n    def _evaluation_epoch(self, epoch, part, dataloader):\n        self.is_train = False\n        self.model.eval()\n        self.evaluation_metrics.reset()\n        with torch.no_grad():\n            for batch_idx, batch in tqdm(\n                enumerate(dataloader),\n                desc=part,\n                total=len(dataloader),\n            ):\n                batch = self.process_batch(batch, metrics=self.evaluation_metrics)\n            self.writer.set_step(epoch * self.epoch_len, part)\n            self._log_scalars(self.evaluation_metrics)\n            self._log_batch(batch_idx, batch, part)\n        return self.evaluation_metrics.result()\n\n    def _monitor_performance(self, logs, not_improved_count):\n        best = False\n        stop_process = False\n        if self.mnt_mode != \"off\":\n            try:\n                if self.mnt_mode == \"min\":\n                    improved = logs[self.mnt_metric] <= self.mnt_best\n                elif self.mnt_mode == \"max\":\n                    improved = logs[self.mnt_metric] >= self.mnt_best\n                else:\n                    improved = False\n            except KeyError:\n                self.logger.warning(\n                    f\"Warning: Metric '{self.mnt_metric}' is not found. \"\n                    \"Model performance monitoring is disabled.\"\n                )\n                self.mnt_mode = \"off\"\n                improved = False\n\n            if improved:\n                self.mnt_best = logs[self.mnt_metric]\n                not_improved_count = 0\n                best = True\n            else:\n                not_improved_count += 1\n\n            if not_improved_count >= self.early_stop:\n                self.logger.info(\n                    \"Validation performance didn't improve for {} epochs. \"\n                    \"Training stops.\".format(self.early_stop)\n                )\n                stop_process = True\n        return best, stop_process, not_improved_count\n\n    def move_batch_to_device(self, batch):\n        for tensor_for_device in self.cfg_trainer['device_tensors']:\n            batch[tensor_for_device] = batch[tensor_for_device].to(self.device)\n        return batch\n\n    def transform_batch(self, batch):\n        return batch\n\n    def _clip_grad_norm(self):\n        if self.config[\"trainer\"].get(\"max_grad_norm\", None) is not None:\n            clip_grad_norm_(\n                self.model.parameters(), self.config[\"trainer\"][\"max_grad_norm\"]\n            )\n\n    @torch.no_grad()\n    def _get_grad_norm(self, norm_type=2):\n        parameters = self.model.parameters()\n        if isinstance(parameters, torch.Tensor):\n            parameters = [parameters]\n        parameters = [p for p in parameters if p.grad is not None]\n        total_norm = torch.norm(\n            torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n            norm_type,\n        )\n        return total_norm.item()\n\n    def _progress(self, batch_idx):\n        base = \"[{}/{} ({:.0f}%)]\"\n        if hasattr(self.train_dataloader, \"n_samples\"):\n            current = batch_idx * self.train_dataloader.batch_size\n            total = self.train_dataloader.n_samples\n        else:\n            current = batch_idx\n            total = self.epoch_len\n        return base.format(current, total, 100.0 * current / total)\n\n    def _log_batch(self, batch_idx, batch, mode=\"train\"):\n        pass\n\n    def _log_scalars(self, metric_tracker: MetricTracker):\n        if self.writer is None:\n            return\n        for metric_name in metric_tracker.keys():\n            self.writer.add_scalar(f\"{metric_name}\", metric_tracker.avg(metric_name))\n\n    def _save_checkpoint(self, epoch, save_best=False, only_best=False):\n        arch = type(self.model).__name__\n        state = {\n            \"arch\": arch,\n            \"epoch\": epoch,\n            \"state_dict\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n            \"lr_scheduler\": self.lr_scheduler.state_dict(),\n            \"monitor_best\": self.mnt_best,\n            \"config\": self.config,\n        }\n        filename = str(self.checkpoint_dir / f\"checkpoint-epoch{epoch}.pth\")\n        if not (only_best and save_best):\n            torch.save(state, filename)\n            if self.config['writer']['log_checkpoints']:\n                self.writer.add_checkpoint(filename, str(self.checkpoint_dir.parent))\n            self.logger.info(f\"Saving checkpoint: {filename} ...\")\n        if save_best:\n            best_path = str(self.checkpoint_dir / \"model_best.pth\")\n            torch.save(state, best_path)\n            if self.config['writer']['log_checkpoints']:\n                self.writer.add_checkpoint(best_path, str(self.checkpoint_dir.parent))\n            self.logger.info(\"Saving current best: model_best.pth ...\")\n\n    def _resume_checkpoint(self, resume_path):\n        resume_path = str(resume_path)\n        self.logger.info(f\"Loading checkpoint: {resume_path} ...\")\n        checkpoint = torch.load(resume_path, self.device)\n        self.start_epoch = checkpoint[\"epoch\"] + 1\n        self.mnt_best = checkpoint[\"monitor_best\"]\n        if checkpoint[\"config\"][\"model\"] != self.config[\"model\"]:\n            self.logger.warning(\n                \"Warning: Architecture configuration given in the config file is different from that \"\n                \"of the checkpoint. This may yield an exception when state_dict is loaded.\"\n            )\n        self.model.load_state_dict(checkpoint[\"state_dict\"])\n        if (\n            checkpoint[\"config\"][\"optimizer\"] != self.config[\"optimizer\"]\n            or checkpoint[\"config\"][\"lr_scheduler\"] != self.config[\"lr_scheduler\"]\n        ):\n            self.logger.warning(\n                \"Warning: Optimizer or lr_scheduler given in the config file is different \"\n                \"from that of the checkpoint. Optimizer and scheduler parameters \"\n                \"are not resumed.\"\n            )\n        else:\n            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n        self.logger.info(\n            f\"Checkpoint loaded. Resume training from epoch {self.start_epoch}\"\n        )\n\n    def _from_pretrained(self, pretrained_path):\n        pretrained_path = str(pretrained_path)\n        if hasattr(self, \"logger\"):\n            self.logger.info(f\"Loading model weights from: {pretrained_path} ...\")\n        else:\n            print(f\"Loading model weights from: {pretrained_path} ...\")\n        checkpoint = torch.load(pretrained_path, self.device)\n        if checkpoint.get(\"state_dict\") is not None:\n            self.model.load_state_dict(checkpoint[\"state_dict\"])\n        else:\n            self.model.load_state_dict(checkpoint)\n\n\n# Mock configuration, model, and other components for testing\nclass MockModel(Module):\n    def __init__(self):\n        super(MockModel, self).__init__()\n        self.dummy_param = torch.nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        return x\n\n# Nested method to handle config access\ndef get_nested_defaultdict():\n    return defaultdict(get_nested_defaultdict)\n\n# Uses a nested defaultdict for config\nconfig = get_nested_defaultdict()\nconfig[\"trainer\"][\"n_epochs\"] = 10\nconfig[\"trainer\"][\"save_period\"] = 1\nconfig[\"trainer\"][\"monitor\"] = 'off'\nconfig[\"writer\"][\"loss_names\"] = []\nconfig[\"trainer\"][\"save_dir\"] = 'checkpoints'\nconfig[\"writer\"][\"run_name\"] = 'run_1'\nconfig[\"trainer\"][\"device_tensors\"] = ['input']\n\n# Instantiate the trainer with mock components\nmodel = MockModel()\ncriterion = MSELoss()\nmetrics = {\"train\": [], \"inference\": []}\noptimizer = Adam(model.parameters())\nlr_scheduler = lr_scheduler.StepLR(optimizer, step_size=1)\nlogger = MockLogger()\nwriter = MockWriter()\ndevice = \"cpu\"\ndataloaders = {\"train\": [], \"validation\": []}\n\ntrainer = BaseTrainer(\n    model=model,\n    criterion=criterion,\n    metrics=metrics,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    config=config,\n    device=device,\n    dataloaders=dataloaders,\n    logger=logger,\n    writer=writer\n)\n\ndef test__from_pretrained():\n    pretrained_path = '/home/user/tmp/pretrained_model.pth'\n    torch.save({\"state_dict\": trainer.model.state_dict()}, pretrained_path)\n\n    # Log storage\n    log_messages = []\n\n    # Mock logger to capture logs\n    class MockLoggerCapture(MockLogger):\n        def info(self, msg):\n            log_messages.append(msg)\n\n    trainer.logger = MockLoggerCapture()\n    \n    # Testing original implementation\n    trainer._from_pretrained(pretrained_path)\n    original_state_dict = trainer.model.state_dict()\n    original_logs = log_messages.copy()\n\n    # Resetting model parameters\n    trainer.model = MockModel()\n\n    # Testing new implementation\n    log_messages.clear()\n    trainer._from_pretrained_new_implementation(pretrained_path)\n    new_state_dict = trainer.model.state_dict()\n    new_logs = log_messages.copy()\n\n    # Assertions\n    assert original_state_dict == new_state_dict, \"State dicts do not match\"\n    assert original_logs == new_logs, \"Log outputs do not match\"\n    assert str(trainer.logger) != \"\", \"Logger should not be empty\"\n\n# Entry point\nif __name__ == \"__main__\":\n    test__from_pretrained()\n    print(\"All tests passed!\")"
    },
    {
        "func_name": "he_uniform_init",
        "idx": "52",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "init_strat.py",
        "orig_func": "def he_uniform_init(params: Tensor, data: Tensor, is_prior: bool, nl: str='relu') -> Tensor:\n    \"\"\"He uniform initialization: [-x, x], where x=\u221a(6/Fi).\"\"\"\n    (fan_in, _) = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    gain = torch.nn.init.calculate_gain(nonlinearity=nl)\n    std = gain / math.sqrt(float(fan_in))\n    std *= math.sqrt(3.0)\n    print(f'Using he uniform initialization: fan_in {fan_in}.')\n    return _std_uniform_init(params, data, is_prior, std)",
        "orig_context": "```python\n## init_strat.py\nimport torch\n\nimport math\n\nfrom torch import Tensor\n\ndef _std_uniform_init(params: Tensor, data: Tensor, is_prior: bool, std: float) -> Tensor:\n    \"\"\"Build uniform initialization based on a given std.\"\"\"\n    assert std is not None\n    if is_prior:\n        l, h = data.min().item(), data.max().item()\n        l, h = l*std, h*std\n    else:\n        l, h = -std, std\n    return params.uniform_(l, h)\n\ndef he_uniform_init(params: Tensor, data: Tensor, is_prior: bool, nl: str = 'relu') -> Tensor:\n    \"\"\"He uniform initialization: [-x, x], where x=\u221a(6/Fi).\"\"\"\n    fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    gain = torch.nn.init.calculate_gain(nonlinearity=nl)\n    std = gain / math.sqrt(float(fan_in))\n    std *= math.sqrt(3.0)  # Calculate uniform bounds from standard deviation\n    print(f\"Using he uniform initialization: fan_in {fan_in}.\")\n    return _std_uniform_init(params, data, is_prior, std)\n\n```\n\n\n",
        "eval_script": "## init_strat.py\nimport torch\n\nimport math\n\nfrom torch import Tensor\n\ndef _std_uniform_init(params: Tensor, data: Tensor, is_prior: bool, std: float) -> Tensor:\n    \"\"\"Build uniform initialization based on a given std.\"\"\"\n    assert std is not None\n    if is_prior:\n        l, h = data.min().item(), data.max().item()\n        l, h = l*std, h*std\n    else:\n        l, h = -std, std\n    return params.uniform_(l, h)\n\ndef he_uniform_init(params: Tensor, data: Tensor, is_prior: bool, nl: str = 'relu') -> Tensor:\n    \"\"\"He uniform initialization: [-x, x], where x=\u221a(6/Fi).\"\"\"\n    fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    gain = torch.nn.init.calculate_gain(nonlinearity=nl)\n    std = gain / math.sqrt(float(fan_in))\n    std *= math.sqrt(3.0)  # Calculate uniform bounds from standard deviation\n    print(f\"Using he uniform initialization: fan_in {fan_in}.\")\n    return _std_uniform_init(params, data, is_prior, std)\n\n\ndef test_he_uniform_init():\n    nl = 'relu'\n    \n    # Test case 1: Checking with is_prior = True\n    torch.manual_seed(42)  # Set seed for reproducibility\n    params = torch.empty(3, 5)\n    data = torch.rand(3, 5)\n    result1 = he_uniform_init(params.clone(), data, True, nl)\n    torch.manual_seed(42)  # Reset seed\n    params = torch.empty(3, 5)\n    data = torch.rand(3, 5)\n    result2 = he_uniform_init_new_implementation(params.clone(), data, True, nl)\n    assert torch.equal(result1, result2), \"Test 1: Results do not match!\"\n    \n    # Test case 2: Checking with is_prior = False\n    torch.manual_seed(42)  # Set seed for reproducibility\n    params = torch.empty(3, 5)\n    data = torch.rand(3, 5)\n    result1 = he_uniform_init(params.clone(), data, False, nl)\n    torch.manual_seed(42)  # Reset seed\n    params = torch.empty(3, 5)\n    data = torch.rand(3, 5)\n    result2 = he_uniform_init_new_implementation(params.clone(), data, False, nl)\n    assert torch.equal(result1, result2), \"Test 2: Results do not match!\"\n\n    # Test case 3: Different non-linearity\n    nl = 'tanh'\n    torch.manual_seed(42)  # Set seed for reproducibility\n    params = torch.empty(3, 5)\n    data = torch.rand(3, 5)\n    result1 = he_uniform_init(params.clone(), data, False, nl)\n    torch.manual_seed(42)  # Reset seed\n    params = torch.empty(3, 5)\n    data = torch.rand(3, 5)\n    result2 = he_uniform_init_new_implementation(params.clone(), data, False, nl)\n    assert torch.equal(result1, result2), \"Test 3: Results do not match!\"\n    \nif __name__ == \"__main__\":\n    test_he_uniform_init()"
    },
    {
        "func_name": "glorot_uniform_init",
        "idx": "54",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "init_strat.py",
        "orig_func": "def glorot_uniform_init(params: Tensor, data: Tensor, is_prior: bool, gain: float=1.0) -> Tensor:\n    \"\"\"Glorot uniform initialization: [-x, x], where x=\u221a(6/(Fi+Fo)).\"\"\"\n    (fan_in, fan_out) = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    std *= math.sqrt(3.0)\n    print(f'Using glorot uniform initialization: fan_in {fan_in}, fan_out {fan_out}.')\n    return _std_uniform_init(params, data, is_prior, std)",
        "orig_context": "```python\n## init_strat.py\nimport torch\n\nimport math\n\nfrom torch import Tensor\n\ndef _std_uniform_init(params: Tensor, data: Tensor, is_prior: bool, std: float) -> Tensor:\n    \"\"\"Build uniform initialization based on a given std.\"\"\"\n    assert std is not None\n    if is_prior:\n        l, h = data.min().item(), data.max().item()\n        l, h = l*std, h*std\n    else:\n        l, h = -std, std\n    return params.uniform_(l, h)\n\ndef glorot_uniform_init(params: Tensor, data: Tensor, is_prior: bool, gain: float = 1.) -> Tensor:\n    \"\"\"Glorot uniform initialization: [-x, x], where x=\u221a(6/(Fi+Fo)).\"\"\"\n    fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    std *= math.sqrt(3.0)  # Calculate uniform bounds from standard deviation\n    print(f\"Using glorot uniform initialization: fan_in {fan_in}, fan_out {fan_out}.\")\n    return _std_uniform_init(params, data, is_prior, std)\n\n```\n\n\n",
        "eval_script": "import torch\nimport math\nfrom torch import Tensor\n\ndef _std_uniform_init(params: Tensor, data: Tensor, is_prior: bool, std: float) -> Tensor:\n    \"\"\"Build uniform initialization based on a given std.\"\"\"\n    assert std is not None\n    if is_prior:\n        l, h = data.min().item(), data.max().item()\n        l, h = l*std, h*std\n    else:\n        l, h = -std, std\n    return params.uniform_(l, h)\n\ndef glorot_uniform_init(params: Tensor, data: Tensor, is_prior: bool, gain: float = 1.) -> Tensor:\n    \"\"\"Glorot uniform initialization: [-x, x], where x=\u221a(6/(Fi+Fo)).\"\"\"\n    fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    std *= math.sqrt(3.0)  # Calculate uniform bounds from standard deviation\n    print(f\"Using glorot uniform initialization: fan_in {fan_in}, fan_out {fan_out}.\")\n    return _std_uniform_init(params, data, is_prior, std)\n\n\ndef test_glorot_uniform_init():\n    # Test case 1: Simple case with a 2x3 Tensor\n    data1 = torch.randn(10, 5)\n    params1 = torch.empty(2, 3)\n    torch.manual_seed(0)\n    out1_old = glorot_uniform_init(params1.clone(), data1, is_prior=False)\n    torch.manual_seed(0)\n    out1_new = glorot_uniform_init_new_implementation(params1.clone(), data1, is_prior=False)\n    assert torch.allclose(out1_old, out1_new, atol=1e-6), \"Test case 1 failed!\"\n\n    # Test case 2: Larger Tensor size\n    data2 = torch.randn(50, 20)\n    params2 = torch.empty(10, 10)\n    torch.manual_seed(0)\n    out2_old = glorot_uniform_init(params2.clone(), data2, is_prior=True)\n    torch.manual_seed(0)\n    out2_new = glorot_uniform_init_new_implementation(params2.clone(), data2, is_prior=True)\n    assert torch.allclose(out2_old, out2_new, atol=1e-6), \"Test case 2 failed!\"\n\n    # Test case 3: Test with different gain\n    gain = 0.5\n    data3 = torch.randn(25, 25)\n    params3 = torch.empty(5, 5)\n    torch.manual_seed(0)\n    out3_old = glorot_uniform_init(params3.clone(), data3, is_prior=False, gain=gain)\n    torch.manual_seed(0)\n    out3_new = glorot_uniform_init_new_implementation(params3.clone(), data3, is_prior=False, gain=gain)\n    assert torch.allclose(out3_old, out3_new, atol=1e-6), \"Test case 3 failed!\"\n\n    print(\"All tests passed!\")\n\nif __name__ == \"__main__\":\n    test_glorot_uniform_init()"
    },
    {
        "func_name": "glorot_normal_init",
        "idx": "55",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "init_strat.py",
        "orig_func": "def glorot_normal_init(params: Tensor, data: Tensor, is_prior: bool, gain: float=1.0) -> Tensor:\n    \"\"\"Glorot normal initialization: N(0, std), where std = x=\u221a(2/(Fi+Fo)).\"\"\"\n    (fan_in, fan_out) = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    print(f'Using glorot normal initialization: fan_in {fan_in}, fan_out {fan_out}.')\n    return _std_normal_init(params, data, is_prior, std)",
        "orig_context": "```python\n## init_strat.py\nimport torch\n\nimport math\n\nfrom torch import Tensor\n\ndef _std_normal_init(params: Tensor, data: Tensor, is_prior: bool, std: float) -> Tensor:\n    \"\"\"Build normal initialization based on a given std.\"\"\"\n    assert std is not None\n    if is_prior:  # consider data information as prior distribution.\n        mu, sigma = torch.mean(data).item(), torch.std(data).item()\n    else:  # assume standard normal distribution.\n        mu, sigma = 0.0, 1.0\n    sigma *= std  # update the sigma\n    return params.normal_(mean=mu, std=sigma)\n\ndef glorot_normal_init(params: Tensor, data: Tensor, is_prior: bool, gain: float = 1.) -> Tensor:\n    \"\"\"Glorot normal initialization: N(0, std), where std = x=\u221a(2/(Fi+Fo)).\"\"\"\n    fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    print(f\"Using glorot normal initialization: fan_in {fan_in}, fan_out {fan_out}.\")\n    return _std_normal_init(params, data, is_prior, std)\n\n```\n\n\n",
        "eval_script": "## init_strat.py\nimport torch\n\nimport math\n\nfrom torch import Tensor\n\ndef _std_normal_init(params: Tensor, data: Tensor, is_prior: bool, std: float) -> Tensor:\n    \"\"\"Build normal initialization based on a given std.\"\"\"\n    assert std is not None\n    if is_prior:  # consider data information as prior distribution.\n        mu, sigma = torch.mean(data).item(), torch.std(data).item()\n    else:  # assume standard normal distribution.\n        mu, sigma = 0.0, 1.0\n    sigma *= std  # update the sigma\n    return params.normal_(mean=mu, std=sigma)\n\ndef glorot_normal_init(params: Tensor, data: Tensor, is_prior: bool, gain: float = 1.) -> Tensor:\n    \"\"\"Glorot normal initialization: N(0, std), where std = x=\u221a(2/(Fi+Fo)).\"\"\"\n    fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(params)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    print(f\"Using glorot normal initialization: fan_in {fan_in}, fan_out {fan_out}.\")\n    return _std_normal_init(params, data, is_prior, std)\n\n\ndef test_glorot_normal_init():\n    params_base = torch.empty(3, 5)  # Example tensor\n    data = torch.randn(3, 5)\n\n    # Test case 1: Default gain and is_prior=True\n    torch.manual_seed(0)  # for reproducibility\n    result1 = glorot_normal_init(params_base.clone(), data, True)\n    torch.manual_seed(0)  # re-seed for reproducibility\n    result1_new = glorot_normal_init_new_implementation(params_base.clone(), data, True)\n    assert torch.allclose(result1, result1_new), \"Test case 1 failed\"\n\n    # Test case 2: Custom gain and is_prior=False\n    torch.manual_seed(0)  # re-seed for reproducibility\n    result2 = glorot_normal_init(params_base.clone(), data, False, gain=0.5)\n    torch.manual_seed(0)  # re-seed for reproducibility\n    result2_new = glorot_normal_init_new_implementation(params_base.clone(), data, False, gain=0.5)\n    assert torch.allclose(result2, result2_new), \"Test case 2 failed\"\n\n    # Test case 3: Zero gain and is_prior=True\n    torch.manual_seed(0)  # re-seed for reproducibility\n    result3 = glorot_normal_init(params_base.clone(), data, True, gain=0.0)\n    torch.manual_seed(0)  # re-seed for reproducibility\n    result3_new = glorot_normal_init_new_implementation(params_base.clone(), data, True, gain=0.0)\n    assert torch.allclose(result3, result3_new), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_glorot_normal_init()"
    },
    {
        "func_name": "init_beta_ebayes",
        "idx": "56",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "init_strat.py",
        "orig_func": "def init_beta_ebayes(data, shape):\n    data = data.cpu().detach().numpy().flatten()\n    data_r = data.reshape(data.size)\n    sz = np.prod(shape)\n    (dmin, dmax) = (np.min(data_r), np.max(data_r))\n    for i in range(len(data_r)):\n        data_r[i] = (data_r[i] - dmin) / (dmax - dmin)\n    zmask = data_r <= 0\n    omask = data_r >= 1\n    data_r[zmask] = data_r[zmask] + 1e-08\n    data_r[omask] = data_r[omask] - 1e-08\n    print(data_r, 'dr')\n\n    def filter_array(data, min, max):\n        return [x for x in data_r if 0 < (x - dmin) / (dmax - dmin) < 1]\n    filtered_data = filter_array(data_r, dmin, dmax)\n    (a, b, _, _) = beta.fit(filtered_data, floc=0, fscale=1)\n    print(f'Found alpha:{a}, beta:{b}')\n    return qnp.random.beta(a=a, b=b, size=sz).reshape(shape)",
        "orig_context": "```python\n## init_strat.py\nimport numpy as np\n\nimport pennylane.numpy as qnp\n\nfrom scipy.stats import truncnorm, beta\n\ndef init_beta_ebayes(data, shape):\n    data = data.cpu().detach().numpy().flatten()  # Flatten for normalization\n    data_r = data.reshape(data.size)\n    sz = np.prod(shape)\n\n    dmin, dmax = np.min(data_r), np.max(data_r)\n    for i in range(len(data_r)):\n        data_r[i] =  (data_r[i] - dmin) / (dmax - dmin)\n    zmask = data_r <= 0\n    omask = data_r >= 1\n    data_r[zmask] = data_r[zmask] + 1e-8\n    data_r[omask] = data_r[omask] - 1e-8\n    print(data_r, \"dr\")\n\n    def filter_array(data, min, max):\n        # Filter the array based on the condition\n        return [x for x in data_r if 0 < (x - dmin) / (dmax - dmin) < 1]\n\n    filtered_data = filter_array(data_r, dmin, dmax)\n\n    a, b, _, _ = beta.fit(filtered_data, floc=0, fscale=1)\n    print(f\"Found alpha:{a}, beta:{b}\")\n    return qnp.random.beta(a=a, b=b, size=sz).reshape(shape)\n\n```\n\n\n",
        "eval_script": "## init_strat.py\nimport numpy as np\nimport pennylane.numpy as qnp\nfrom scipy.stats import beta\n\ndef init_beta_ebayes(data, shape):\n    data = data.cpu().detach().numpy().flatten()  # Flatten for normalization\n    data_r = data.reshape(data.size)\n    sz = np.prod(shape)\n\n    dmin, dmax = np.min(data_r), np.max(data_r)\n    for i in range(len(data_r)):\n        data_r[i] =  (data_r[i] - dmin) / (dmax - dmin)\n    zmask = data_r <= 0\n    omask = data_r >= 1\n    data_r[zmask] = data_r[zmask] + 1e-8\n    data_r[omask] = data_r[omask] - 1e-8\n    print(data_r, \"dr\")\n\n    def filter_array(data, min, max):\n        # Filter the array based on the correct already normalized range\n        return [x for x in data if 0 < x < 1]\n\n    filtered_data = filter_array(data_r, dmin, dmax)\n\n    a, b, _, _ = beta.fit(filtered_data, floc=0, fscale=1)\n    print(f\"Found alpha:{a}, beta:{b}\")\n    \n    # Set seed for reproducibility within the tests\n    qnp.random.seed(42)\n    return qnp.random.beta(a=a, b=b, size=sz).reshape(shape)\n\n\ndef test_init_beta_ebayes():\n    input_data = np.array([0.5, 0.6, 0.7], dtype='float32')\n    shape = (1, 3)\n    \n    # Convert numpy array to a mock object with required methods (detach and cpu)\n    class MockTensor:\n        def __init__(self, array):\n            self.array = array\n        \n        def cpu(self):\n            return self\n        \n        def detach(self):\n            return self\n        \n        def numpy(self):\n            return self.array\n    \n    mock_data = MockTensor(input_data)\n    \n    # Expected output: similar arrays from both implementations for the same inputs\n    output_1 = init_beta_ebayes(mock_data, shape)\n    output_2 = init_beta_ebayes_new_implementation(mock_data, shape)\n    \n    # Assert that both implementations are closely equal\n    assert np.allclose(output_1, output_2), \"Outputs from the two functions do not match!\"\n\n    # Test when data has different range\n    input_data2 = np.array([0.2, 0.8, 0.5], dtype='float32')\n    mock_data2 = MockTensor(input_data2)\n    output_3 = init_beta_ebayes(mock_data2, shape)\n    output_4 = init_beta_ebayes_new_implementation(mock_data2, shape)\n    \n    assert np.allclose(output_3, output_4), \"Outputs from the two functions do not match!\"\n\n    # Test with different shape\n    shape2 = (3, 1)\n    output_5 = init_beta_ebayes(mock_data, shape2)\n    output_6 = init_beta_ebayes_new_implementation(mock_data, shape2)\n    \n    assert np.allclose(output_5, output_6), \"Outputs from the two functions do not match!\"\n\nif __name__ == \"__main__\":\n    test_init_beta_ebayes()"
    },
    {
        "func_name": "init_uniform_norm",
        "idx": "57",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "init_strat.py",
        "orig_func": "def init_uniform_norm(data, shape):\n    data = data.cpu().detach().numpy().flatten()\n    dr = data.reshape(data.size)\n    sz = np.prod(shape)\n    (dmin, dmax) = (np.min(data), np.max(data))\n    for i in range(len(data)):\n        dr[i] = (dr[i] - dmin) / (dmax - dmin)\n    zmask = dr <= 0\n    omask = dr >= 1\n    dr[zmask] = dr[zmask] + 1e-08\n    dr[omask] = dr[omask] - 1e-08\n    (l, h) = ss.uniform.fit(dr)\n    print(f'Determined range: [{l},{h}]')\n    return qnp.random.uniform(l, h, size=sz).reshape(shape)",
        "orig_context": "```python\n## init_strat.py\nimport numpy as np\n\nimport pennylane.numpy as qnp\n\nimport scipy.stats as ss\n\ndef init_uniform_norm(data, shape): # Implemented\n    data = data.cpu().detach().numpy().flatten()  # Flatten for normalization\n    dr = data.reshape(data.size)\n    sz = np.prod(shape)\n    dmin, dmax = np.min(data), np.max(data)\n\n    for i in range(len(data)):\n        dr[i] = (dr[i] - dmin)/(dmax - dmin)\n    zmask = dr <= 0\n    omask = dr >= 1\n    dr[zmask] = dr[zmask] + 1e-8\n    dr[omask] = dr[omask] - 1e-8\n    l, h = ss.uniform.fit(dr)\n    print(f\"Determined range: [{l},{h}]\")\n    return qnp.random.uniform(l, h, size=sz).reshape(shape)\n\n```\n\n\n",
        "eval_script": "## init_strat.py\nimport numpy as np\nimport pennylane.numpy as qnp\nimport scipy.stats as ss\n\ndef init_uniform_norm(data, shape): # Implemented\n    data = data.cpu().detach().numpy().flatten()  # Flatten for normalization\n    dr = data.reshape(data.size)\n    sz = np.prod(shape)\n    dmin, dmax = np.min(data), np.max(data)\n\n    for i in range(len(data)):\n        dr[i] = (dr[i] - dmin)/(dmax - dmin)\n    zmask = dr <= 0\n    omask = dr >= 1\n    dr[zmask] = dr[zmask] + 1e-8\n    dr[omask] = dr[omask] - 1e-8\n    l, h = ss.uniform.fit(dr)\n    print(f\"Determined range: [{l},{h}]\")\n    return qnp.random.uniform(l, h, size=sz).reshape(shape)\n\n\n# Assume this is the correct definition for the new implementation.\n\n\ndef test_init_uniform_norm():\n    # Mock data with a similar structure to what the original function handles.\n    # Assuming we're using PyTorch Tensors which have `.cpu()` and `.detach()` methods.\n    import torch\n\n    shape = (5,)\n\n    # Mock data\n    data1 = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n    data2 = torch.tensor([10.0, 20.0, 30.0, 40.0, 50.0])\n    data3 = torch.tensor([-1.0, 0.0, 1.0, 2.0, 3.0])\n\n    # Set random seed for reproducibility for each function call\n    qnp.random.seed(0)\n    output1_old = init_uniform_norm(data1, shape)\n    qnp.random.seed(0)\n    output1_new = init_uniform_norm_new_implementation(data1, shape)\n\n    qnp.random.seed(0)\n    output2_old = init_uniform_norm(data2, shape)\n    qnp.random.seed(0)\n    output2_new = init_uniform_norm_new_implementation(data2, shape)\n\n    qnp.random.seed(0)\n    output3_old = init_uniform_norm(data3, shape)\n    qnp.random.seed(0)\n    output3_new = init_uniform_norm_new_implementation(data3, shape)\n\n    # Assert for mean equality\n    assert np.allclose(np.mean(output1_old), np.mean(output1_new)), \"Test 1 failed: Means differ\"\n    assert np.allclose(np.mean(output2_old), np.mean(output2_new)), \"Test 2 failed: Means differ\"\n    assert np.allclose(np.mean(output3_old), np.mean(output3_new)), \"Test 3 failed: Means differ\"\n\n    # Assert for variance equality\n    assert np.allclose(np.var(output1_old), np.var(output1_new)), \"Test 1 failed: Variances differ\"\n    assert np.allclose(np.var(output2_old), np.var(output2_new)), \"Test 2 failed: Variances differ\"\n    assert np.allclose(np.var(output3_old), np.var(output3_new)), \"Test 3 failed: Variances differ\"\n\n    # Assert for range (min-max) in each array should be similar\n    assert np.allclose(np.min(output1_old), np.min(output1_new)), \"Test 1 failed: Min values differ\"\n    assert np.allclose(np.max(output1_old), np.max(output1_new)), \"Test 1 failed: Max values differ\"\n\ndef __main__():\n    test_init_uniform_norm()\n\nif __name__ == \"__main__\":\n    __main__()"
    },
    {
        "func_name": "gaussian_initialization",
        "idx": "58",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "init_strat.py",
        "orig_func": "def gaussian_initialization(num_qubits, layers):\n    sigma = 1 / (2 * layers)\n    weights = np.random.normal(0, sigma, (layers, num_qubits, 2))\n    return weights",
        "orig_context": "```python\n## init_strat.py\nimport numpy as np\n\ndef gaussian_initialization(num_qubits, layers):\n    # Using Gaussian distribution N(0, \u03c3^2) for initialization\n    sigma = 1 / (2 * layers)\n    weights = np.random.normal(0, sigma, (layers, num_qubits, 2))\n    return weights\n\n```\n\n\n",
        "eval_script": "## init_strat.py\nimport numpy as np\n\ndef gaussian_initialization(num_qubits, layers):\n    # Using Gaussian distribution N(0, \u03c3^2) for initialization\n    sigma = 1 / (2 * layers)\n    weights = np.random.normal(0, sigma, (layers, num_qubits, 2))\n    return weights\n\n\n\ndef test_gaussian_initialization():\n    num_qubits = 5\n    layers = 3\n    runs = 1000\n\n    old_samples = np.array([gaussian_initialization(num_qubits, layers) for _ in range(runs)])\n    new_samples = np.array([gaussian_initialization_new_implementation(num_qubits, layers) for _ in range(runs)])\n\n    # Calculate means and standard deviations for both implementations\n    old_mean = old_samples.mean()\n    new_mean = new_samples.mean()\n    old_std = old_samples.std()\n    new_std = new_samples.std()\n\n    # Allow a small delta due to randomness\n    delta = 0.1\n\n    # Assert that means are roughly equal\n    assert abs(old_mean - new_mean) < delta, \"Means are significantly different\"\n\n    # Assert that standard deviations are roughly equal\n    assert abs(old_std - new_std) < delta, \"Standard deviations are significantly different\"\n\n    # Assert that the shapes of the outputs are the same\n    sample_shape = old_samples[0].shape\n    assert all(x.shape == sample_shape for x in old_samples), \"Old implementation produced inconsistent shapes\"\n    assert all(x.shape == sample_shape for x in new_samples), \"New implementation produced inconsistent shapes\"\n    \n    print(\"All tests passed.\")\n\nif __name__ == \"__main__\":\n    test_gaussian_initialization()"
    },
    {
        "func_name": "calculate_bp_gradient_norm",
        "idx": "59",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "utils.py",
        "orig_func": "def calculate_bp_gradient_norm(model):\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    return total_norm ** 0.5",
        "orig_context": "```python\n## utils.py\ndef calculate_bp_gradient_norm(model):\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    return total_norm ** 0.5\n\n```\n\n\n",
        "eval_script": "## utils.py\ndef calculate_bp_gradient_norm(model):\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    return total_norm ** 0.5\n\n\n\ndef test_calculate_bp_gradient_norm():\n    import torch\n    import torch.nn as nn\n    \n    class DummyModel(nn.Module):\n        def __init__(self, grad_values):\n            super(DummyModel, self).__init__()\n            self.param1 = nn.Parameter(torch.tensor(1.0))\n            self.param2 = nn.Parameter(torch.tensor(2.0))\n            self.param1.grad = torch.tensor(grad_values[0])\n            self.param2.grad = torch.tensor(grad_values[1])\n        \n        def parameters(self):\n            return [self.param1, self.param2]\n    \n    # Test case 1\n    model1 = DummyModel([3.0, 4.0])\n    assert calculate_bp_gradient_norm(model1) == calculate_bp_gradient_norm_new_implementation(model1)\n    \n    # Test case 2 - different gradient values\n    model2 = DummyModel([6.0, 8.0])\n    assert calculate_bp_gradient_norm(model2) == calculate_bp_gradient_norm_new_implementation(model2)\n    \n    # Test case 3 - one zero gradient\n    model3 = DummyModel([0.0, 5.0])\n    assert calculate_bp_gradient_norm(model3) == calculate_bp_gradient_norm_new_implementation(model3)\n\n    print(\"All test cases passed.\")\n\nif __name__ == \"__main__\":\n    test_calculate_bp_gradient_norm()"
    },
    {
        "func_name": "IrisModel.forward",
        "idx": "62",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "model.py",
        "orig_func": "def forward(self, x):\n    y = self\n    x = self.relu(self.fc1(x))\n    x = self.fc2(x)\n    return x",
        "orig_context": "```python\n## model.py\nimport torch.nn as nn\n\nhidden_dim = 10\n\noutput_dim = 3\n\nclass IrisModel(nn.Module):\n    def __init__(self, input_dim, init_fn=None):\n        super(IrisModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n        # Apply custom initialization if provided\n        if init_fn:\n            self.apply(init_fn)\n\n    def forward(self, x):\n        y = self;\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n```\n\n\n",
        "eval_script": "# The new PYTHON CODE containing your test function test_forward and the __main__ function.\n\nimport torch\nimport torch.nn as nn\n\nhidden_dim = 10\noutput_dim = 3\n\nclass IrisModel(nn.Module):\n    def __init__(self, input_dim, init_fn=None):\n        super(IrisModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n        # Apply custom initialization if provided\n        if init_fn:\n            self.apply(init_fn)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n\n\ndef test_forward():\n    input_dim = 4  # Assuming input dimension of the iris dataset\n    model = IrisModel(input_dim)\n    test_input = torch.randn(5, input_dim)  # Creating a random test input\n\n    # Get outputs from both implementations\n    output1 = model.forward(test_input)\n    output2 = model.forward_new_implementation(test_input)\n\n    # Conducting the tests\n    assert output1.shape == output2.shape, \"Output shapes are different\"\n    assert torch.allclose(output1, output2, atol=1e-6), \"Outputs are not close enough\"\n    assert (output1 == output2).all(), \"Outputs are not exactly equal\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "QuantumNeuralNetwork.forward",
        "idx": "63",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "model.py",
        "orig_func": "def forward(self, x):\n    batch_size = x.shape[0]\n    q_out_list = []\n    for i in range(batch_size):\n        q_out = self.circuit(x[i], self.weight_shapes['weights'])\n        q_out_list.append(q_out)\n    q_out_tensor = torch.tensor(q_out_list, dtype=torch.float32)\n    output = self.fc(q_out_tensor)\n    return output",
        "orig_context": "```python\n## model.py\nimport torch\n\nimport torch.nn as nn\n\nimport pennylane as qml\n\nfrom pennylane import numpy as np\n\nimport yaml\n\nargs = yaml.safe_load(file)\n\nclass QuantumNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(QuantumNeuralNetwork, self).__init__()\n        self.n_qubits = 4  # Example\n        self.dev = qml.device('default.qubit', wires=self.n_qubits)\n\n        @qml.qnode(self.dev, interface=\"torch\")\n        def circuit(inputs, weights):\n            for i in range(self.n_qubits):\n                qml.RX(inputs[i], wires=i)\n            qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n\n        weights = np.random.rand(4, 4, 3)\n        self.circuit = circuit\n        self.weight_shapes = {\"weights\": weights}  # Example\n        if(args['dataset'] == \"Iris\"):\n            self.fc = nn.Linear(self.n_qubits, 3)  # 3 output classes for Iris\n        elif(args['dataset'] == \"MNIST\" or args['dataset'] == \"MedMNIST\"):\n            self.fc = nn.Linear(self.n_qubits, 10)  # 3 output classes for mnist\n\n    def forward(self, x):\n        batch_size = x.shape[0]  # Get the batch size (120 in your case)\n        q_out_list = []\n\n        # Apply the quantum circuit for each sample in the batch\n        for i in range(batch_size):\n            q_out = self.circuit(x[i], self.weight_shapes[\"weights\"])\n            q_out_list.append(q_out)\n\n        # Convert the list of quantum outputs to a tensor\n        q_out_tensor = torch.tensor(q_out_list, dtype=torch.float32)  # Shape: [120, 4]\n        # print(q_out_tensor.shape, \"from forward\")\n        # Pass through the fully connected layer (applied to each sample independently)\n        output = self.fc(q_out_tensor)  # Shape: [120, 3]\n\n        return output\n\n```\n\n\n",
        "eval_script": "# Mock content to simulate YAML configuration.\nimport torch\nimport torch.nn as nn\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport io\nimport yaml\n\n# Mock YAML content\nyaml_content = \"\"\"\ndataset: \"Iris\"\n\"\"\"\n\n# Load the mock YAML\nargs = yaml.safe_load(io.StringIO(yaml_content))\n\nclass QuantumNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(QuantumNeuralNetwork, self).__init__()\n        self.n_qubits = 4  # Example\n        self.dev = qml.device('default.qubit', wires=self.n_qubits)\n\n        @qml.qnode(self.dev, interface=\"torch\")\n        def circuit(inputs, weights):\n            for i in range(self.n_qubits):\n                qml.RX(inputs[i], wires=i)\n            qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n\n        weights = np.random.rand(4, 4, 3)\n        self.circuit = circuit\n        self.weight_shapes = {\"weights\": weights}  # Example\n        \n        if(args['dataset'] == \"Iris\"):\n            self.fc = nn.Linear(self.n_qubits, 3)  # 3 output classes for Iris\n        elif(args['dataset'] == \"MNIST\" or args['dataset'] == \"MedMNIST\"):\n            self.fc = nn.Linear(self.n_qubits, 10)  # 10 output classes for mnist and medmnist\n\n    def forward(self, x):\n        batch_size = x.shape[0]  # Get the batch size (120 in your case)\n        q_out_list = []\n\n        # Apply the quantum circuit for each sample in the batch\n        for i in range(batch_size):\n            q_out = self.circuit(x[i], self.weight_shapes[\"weights\"])\n            q_out_list.append(q_out)\n\n        # Convert the list of quantum outputs to a tensor\n        q_out_tensor = torch.tensor(q_out_list, dtype=torch.float32)  # Shape: [120, 4]\n        # print(q_out_tensor.shape, \"from forward\")\n        # Pass through the fully connected layer (applied to each sample independently)\n        output = self.fc(q_out_tensor)  # Shape: [120, 3]\n\n        return output\n\n\n\ndef test_forward():\n    qnn = QuantumNeuralNetwork()\n    # Creating mock data for testing\n    mock_x = torch.rand(10, 4)  # Assuming input has shape [10, 4] for this test\n    \n    # Calculate outputs from both implementations\n    output_old = qnn.forward(mock_x)\n    output_new = qnn.forward_new_implementation(mock_x)\n    \n    # Assert that both outputs are equal\n    assert torch.allclose(output_old, output_new), \"Test 1: Outputs are not equal.\"\n    assert output_old.shape == output_new.shape, \"Test 2: Output shapes do not match.\"\n    assert torch.all(output_old == output_new), \"Test 3: Outputs are not exactly equal.\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "QuantumModel2.compute_loss",
        "idx": "64",
        "repo_name": "mrrajon04___Benchmark_BP_Free",
        "func_path": "model.py",
        "orig_func": "def compute_loss(self, predictions, targets):\n    criterion = nn.CrossEntropyLoss()\n    return criterion(predictions, targets)",
        "orig_context": "```python\n## model.py\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nimport yaml\n\nargs = yaml.safe_load(file)\n\nclass QuantumModel2(nn.Module):\n    def __init__(self, initialization_strategy):\n        super(QuantumModel2, self).__init__()\n        self.params = initialization_strategy.initialize_params()\n        self.optimizer = optim.Adam([self.params], lr=args['learning_rate'])\n        if(args['dataset'] == 'MNIST'):\n            self.fc = nn.Linear(784, 10)  # Change 4 to 784 to match the MNIST input size\n\n    #\n    def forward(self, X):\n        # Apply quantum operations here\n        if args['dataset'] == 'MNIST':\n            # Reshape X to [batch_size, 784] if it's in [batch_size, 1, 28, 28]\n            if X.dim() > 2:\n                X = X.view(X.size(0), -1)  # Flatten the input\n            # Perform forward pass using the fully connected layer for MNIST\n            output = self.fc(X)  # Shape [batch_size, 10]\n            return output  # Output shape should be [batch_size, 10]\n\n        elif(args['dataset'] == 'Iris' or args['dataset'] == 'MedMNIST'):\n            return (X.mm(self.params))\n    def compute_loss(self, predictions, targets):\n        criterion = nn.CrossEntropyLoss()\n        return criterion(predictions, targets)\n\n```\n\n\n",
        "eval_script": "# Mock configuration and initialization strategy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\n\nclass MockInitializationStrategy:\n    def initialize_params(self):\n        # Return a tensor with the appropriate shape\n        return torch.randn(784, 10)  # Example shape for MNIST\n    \n# Mock args directly instead of using yaml.safe_load(file)\nargs = {\n    'learning_rate': 0.001,\n    'dataset': 'MNIST'\n}\n\nclass QuantumModel2(nn.Module):\n    def __init__(self, initialization_strategy):\n        super(QuantumModel2, self).__init__()\n        self.params = initialization_strategy.initialize_params()\n        self.optimizer = optim.Adam([self.params], lr=args['learning_rate'])\n        if(args['dataset'] == 'MNIST'):\n            self.fc = nn.Linear(784, 10)\n\n    def forward(self, X):\n        if args['dataset'] == 'MNIST':\n            if X.dim() > 2:\n                X = X.view(X.size(0), -1)\n            output = self.fc(X)\n            return output\n        elif args['dataset'] == 'Iris' or args['dataset'] == 'MedMNIST':\n            return (X.mm(self.params))\n    \n    def compute_loss(self, predictions, targets):\n        criterion = nn.CrossEntropyLoss()\n        return criterion(predictions, targets)\n    \n\n\ndef test_compute_loss():\n    initialization_strategy = MockInitializationStrategy()\n    model = QuantumModel2(initialization_strategy)\n\n    # Test Case 1: Basic correct labels\n    dummy_input = torch.randn(2, 1, 28, 28)\n    target_1 = torch.tensor([0, 1], dtype=torch.long)\n    predictions_1 = model(dummy_input)\n    expected_1 = model.compute_loss(predictions_1, target_1)\n    result_1 = model.compute_loss_new_implementation(predictions_1, target_1)\n    assert torch.isclose(expected_1, result_1), f\"Test Case 1 Failed: {expected_1} != {result_1}\"\n\n    # Test Case 2: All zero labels\n    target_2 = torch.tensor([0, 0], dtype=torch.long)\n    predictions_2 = model(dummy_input)\n    expected_2 = model.compute_loss(predictions_2, target_2)\n    result_2 = model.compute_loss_new_implementation(predictions_2, target_2)\n    assert torch.isclose(expected_2, result_2), f\"Test Case 2 Failed: {expected_2} != {result_2}\"\n\n    # Test Case 3: All same labels\n    target_3 = torch.tensor([1, 1], dtype=torch.long)\n    predictions_3 = model(dummy_input)\n    expected_3 = model.compute_loss(predictions_3, target_3)\n    result_3 = model.compute_loss_new_implementation(predictions_3, target_3)\n    assert torch.isclose(expected_3, result_3), f\"Test Case 3 Failed: {expected_3} != {result_3}\"\n\nif __name__ == \"__main__\":\n    test_compute_loss()"
    },
    {
        "func_name": "get_audio_duration",
        "idx": "66",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/train/finetune_gradio.py",
        "orig_func": "def get_audio_duration(audio_path):\n    \"\"\"Calculate the duration mono of an audio file.\"\"\"\n    (audio, sample_rate) = torchaudio.load(audio_path)\n    return audio.shape[1] / sample_rate",
        "orig_context": "```python\n## src/f5_tts/train/finetune_gradio.py\nimport torchaudio\n\ndef get_audio_duration(audio_path):\n    \"\"\"Calculate the duration mono of an audio file.\"\"\"\n    audio, sample_rate = torchaudio.load(audio_path)\n    return audio.shape[1] / sample_rate\n\n```\n\n\n",
        "eval_script": "import os\nimport numpy as np\nimport wave\nfrom scipy.io import wavfile\n\ndef generate_mock_wav_file(file_path, duration_s=1, sample_rate=16000):\n    \"\"\"Generate a mock WAV file with silence.\"\"\"\n    n_samples = int(duration_s * sample_rate)  # convert to integer\n    audio_data = np.zeros((n_samples,), dtype=np.int16)\n\n    with wave.open(file_path, 'w') as wf:\n        wf.setnchannels(1)        # mono\n        wf.setsampwidth(2)        # 2 bytes for int16\n        wf.setframerate(sample_rate)\n        wf.writeframes(audio_data.tobytes())\n\ndef get_audio_duration(audio_path):\n    \"\"\"Calculate the duration mono of an audio file.\"\"\"\n    sample_rate, audio = wavfile.read(audio_path)  \n    return len(audio) / sample_rate\n\n\ndef test_get_audio_duration():\n    # Create directory if it doesn't exist\n    os.makedirs(\"/home/user/tmp\", exist_ok=True)\n\n    # Test case 1: 1 second of audio at 16000 Hz\n    path1 = \"/home/user/tmp/mock_audio_1s.wav\"\n    generate_mock_wav_file(path1, duration_s=1, sample_rate=16000)\n    assert get_audio_duration(path1) == get_audio_duration_new_implementation(path1)\n\n    # Test case 2: 2 seconds of audio at 22050 Hz\n    path2 = \"/home/user/tmp/mock_audio_2s.wav\"\n    generate_mock_wav_file(path2, duration_s=2, sample_rate=22050)\n    assert get_audio_duration(path2) == get_audio_duration_new_implementation(path2)\n\n    # Test case 3: 0.5 seconds of audio at 44100 Hz\n    path3 = \"/home/user/tmp/mock_audio_05s.wav\"\n    generate_mock_wav_file(path3, duration_s=0.5, sample_rate=44100)\n    assert get_audio_duration(path3) == get_audio_duration_new_implementation(path3)\n\nif __name__ == \"__main__\":\n    test_get_audio_duration()"
    },
    {
        "func_name": "chunk_text",
        "idx": "67",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/infer/utils_infer.py",
        "orig_func": "def chunk_text(text, max_chars=135):\n    \"\"\"\n    Splits the input text into chunks, each with a maximum number of characters.\n\n    Args:\n        text (str): The text to be split.\n        max_chars (int): The maximum number of characters per chunk.\n\n    Returns:\n        List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = ''\n    sentences = re.split('(?<=[;:,.!?])\\\\s+|(?<=[\uff1b\uff1a\uff0c\u3002\uff01\uff1f])', text)\n    for sentence in sentences:\n        if len(current_chunk.encode('utf-8')) + len(sentence.encode('utf-8')) <= max_chars:\n            current_chunk += sentence + ' ' if sentence and len(sentence[-1].encode('utf-8')) == 1 else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence + ' ' if sentence and len(sentence[-1].encode('utf-8')) == 1 else sentence\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n    return chunks",
        "orig_context": "```python\n## src/f5_tts/infer/utils_infer.py\nimport re\n\ndef chunk_text(text, max_chars=135):\n    \"\"\"\n    Splits the input text into chunks, each with a maximum number of characters.\n\n    Args:\n        text (str): The text to be split.\n        max_chars (int): The maximum number of characters per chunk.\n\n    Returns:\n        List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = \"\"\n    # Split the text into sentences based on punctuation followed by whitespace\n    sentences = re.split(r\"(?<=[;:,.!?])\\s+|(?<=[\uff1b\uff1a\uff0c\u3002\uff01\uff1f])\", text)\n\n    for sentence in sentences:\n        if len(current_chunk.encode(\"utf-8\")) + len(sentence.encode(\"utf-8\")) <= max_chars:\n            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/infer/utils_infer.py\nimport re\n\ndef chunk_text(text, max_chars=135):\n    \"\"\"\n    Splits the input text into chunks, each with a maximum number of characters.\n\n    Args:\n        text (str): The text to be split.\n        max_chars (int): The maximum number of characters per chunk.\n\n    Returns:\n        List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = \"\"\n    # Split the text into sentences based on punctuation followed by whitespace\n    sentences = re.split(r\"(?<=[;:,.!?])\\s+|(?<=[\uff1b\uff1a\uff0c\u3002\uff01\uff1f])\", text)\n\n    for sentence in sentences:\n        if len(current_chunk.encode(\"utf-8\")) + len(sentence.encode(\"utf-8\")) <= max_chars:\n            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\n# Assume the new implementation is defined here\n\n\ndef test_chunk_text():\n    # Test case 1: Regular sentence\n    text1 = \"This is a test. Let's see how it works! Is it effective?\"\n    assert chunk_text(text1) == chunk_text_new_implementation(text1)\n\n    # Test case 2: Long sentence exceeding max_chars\n    text2 = \"This is a very long test sentence that definitely exceeds the default maximum character limit of the chunk_text function, and so it should be split into multiple parts.\"\n    assert chunk_text(text2) == chunk_text_new_implementation(text2)\n\n    # Test case 3: Multiple punctuation and mixed language\n    text3 = \"Hello! \u4f60\u597d\uff01 This is a test. \u8fd9\u662f\u4e00\u4e2a\u6d4b\u8bd5\u3002\"\n    assert chunk_text(text3) == chunk_text_new_implementation(text3)\n\nif __name__ == \"__main__\":\n    test_chunk_text()"
    },
    {
        "func_name": "parse_speechtypes_text",
        "idx": "68",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/infer/infer_gradio.py",
        "orig_func": "def parse_speechtypes_text(gen_text):\n    pattern = '\\\\{(.*?)\\\\}'\n    tokens = re.split(pattern, gen_text)\n    segments = []\n    current_style = 'Regular'\n    for i in range(len(tokens)):\n        if i % 2 == 0:\n            text = tokens[i].strip()\n            if text:\n                segments.append({'style': current_style, 'text': text})\n        else:\n            style = tokens[i].strip()\n            current_style = style\n    return segments",
        "orig_context": "```python\n## src/f5_tts/infer/infer_gradio.py\nimport re\n\ndef parse_speechtypes_text(gen_text):\n    # Pattern to find {speechtype}\n    pattern = r\"\\{(.*?)\\}\"\n\n    # Split the text by the pattern\n    tokens = re.split(pattern, gen_text)\n\n    segments = []\n\n    current_style = \"Regular\"\n\n    for i in range(len(tokens)):\n        if i % 2 == 0:\n            # This is text\n            text = tokens[i].strip()\n            if text:\n                segments.append({\"style\": current_style, \"text\": text})\n        else:\n            # This is style\n            style = tokens[i].strip()\n            current_style = style\n\n    return segments\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/infer/infer_gradio.py\nimport re\n\ndef parse_speechtypes_text(gen_text):\n    # Pattern to find {speechtype}\n    pattern = r\"\\{(.*?)\\}\"\n\n    # Split the text by the pattern\n    tokens = re.split(pattern, gen_text)\n\n    segments = []\n\n    current_style = \"Regular\"\n\n    for i in range(len(tokens)):\n        if i % 2 == 0:\n            # This is text\n            text = tokens[i].strip()\n            if text:\n                segments.append({\"style\": current_style, \"text\": text})\n        else:\n            # This is style\n            style = tokens[i].strip()\n            current_style = style\n\n    return segments\n\n\n\ndef test_parse_speechtypes_text():\n    # Test case 1\n    input_text1 = \"Hello {Happy} world! How are {Sad} you?\"\n    expected_output1 = [\n        {\"style\": \"Regular\", \"text\": \"Hello\"},\n        {\"style\": \"Happy\", \"text\": \"world! How are\"},\n        {\"style\": \"Sad\", \"text\": \"you?\"}\n    ]\n    \n    assert parse_speechtypes_text(input_text1) == expected_output1\n    assert parse_speechtypes_text_new_implementation(input_text1) == expected_output1\n\n    # Test case 2\n    input_text2 = \"This is a {Test} simple test.\"\n    expected_output2 = [\n        {\"style\": \"Regular\", \"text\": \"This is a\"},\n        {\"style\": \"Test\", \"text\": \"simple test.\"}\n    ]\n\n    assert parse_speechtypes_text(input_text2) == expected_output2\n    assert parse_speechtypes_text_new_implementation(input_text2) == expected_output2\n\n    # Test case 3\n    input_text3 = \"Nothing to parse here.\"\n    expected_output3 = [\n        {\"style\": \"Regular\", \"text\": \"Nothing to parse here.\"}\n    ]\n\n    assert parse_speechtypes_text(input_text3) == expected_output3\n    assert parse_speechtypes_text_new_implementation(input_text3) == expected_output3\n\nif __name__ == \"__main__\":\n    test_parse_speechtypes_text()"
    },
    {
        "func_name": "get_audio_select",
        "idx": "69",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/train/finetune_gradio.py",
        "orig_func": "def get_audio_select(file_sample):\n    select_audio_ref = file_sample\n    select_audio_gen = file_sample\n    if file_sample is not None:\n        select_audio_ref += '_ref.wav'\n        select_audio_gen += '_gen.wav'\n    return (select_audio_ref, select_audio_gen)",
        "orig_context": "```python\n## src/f5_tts/train/finetune_gradio.py\ndef get_audio_select(file_sample):\n    select_audio_ref = file_sample\n    select_audio_gen = file_sample\n\n    if file_sample is not None:\n        select_audio_ref += \"_ref.wav\"\n        select_audio_gen += \"_gen.wav\"\n\n    return select_audio_ref, select_audio_gen\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/train/finetune_gradio.py\ndef get_audio_select(file_sample):\n    select_audio_ref = file_sample\n    select_audio_gen = file_sample\n\n    if file_sample is not None:\n        select_audio_ref += \"_ref.wav\"\n        select_audio_gen += \"_gen.wav\"\n\n    return select_audio_ref, select_audio_gen\n\n\n\ndef test_get_audio_select():\n    # Test case 1: Check with a sample string input\n    input_sample_1 = \"audio_sample_1\"\n    assert get_audio_select(input_sample_1) == get_audio_select_new_implementation(input_sample_1), \"Test case 1 failed\"\n\n    # Test case 2: Check with an empty string input\n    input_sample_2 = \"\"\n    assert get_audio_select(input_sample_2) == get_audio_select_new_implementation(input_sample_2), \"Test case 2 failed\"\n\n    # Test case 3: Check with None input\n    input_sample_3 = None\n    assert get_audio_select(input_sample_3) == get_audio_select_new_implementation(input_sample_3), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_get_audio_select()"
    },
    {
        "func_name": "read_audio_text_pairs",
        "idx": "70",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/train/datasets/prepare_csv_wavs.py",
        "orig_func": "def read_audio_text_pairs(csv_file_path):\n    audio_text_pairs = []\n    parent = Path(csv_file_path).parent\n    with open(csv_file_path, mode='r', newline='', encoding='utf-8-sig') as csvfile:\n        reader = csv.reader(csvfile, delimiter='|')\n        next(reader)\n        for row in reader:\n            if len(row) >= 2:\n                audio_file = row[0].strip()\n                text = row[1].strip()\n                audio_file_path = parent / audio_file\n                audio_text_pairs.append((audio_file_path.as_posix(), text))\n    return audio_text_pairs",
        "orig_context": "```python\n## src/f5_tts/train/datasets/prepare_csv_wavs.py\nimport csv\n\nfrom pathlib import Path\n\ndef read_audio_text_pairs(csv_file_path):\n    audio_text_pairs = []\n\n    parent = Path(csv_file_path).parent\n    with open(csv_file_path, mode=\"r\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:\n        reader = csv.reader(csvfile, delimiter=\"|\")\n        next(reader)  # Skip the header row\n        for row in reader:\n            if len(row) >= 2:\n                audio_file = row[0].strip()  # First column: audio file path\n                text = row[1].strip()  # Second column: text\n                audio_file_path = parent / audio_file\n                audio_text_pairs.append((audio_file_path.as_posix(), text))\n\n    return audio_text_pairs\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/train/datasets/prepare_csv_wavs.py\nimport csv\nfrom pathlib import Path\n\ndef read_audio_text_pairs(csv_file_path):\n    audio_text_pairs = []\n\n    parent = Path(csv_file_path).parent\n    with open(csv_file_path, mode=\"r\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:\n        reader = csv.reader(csvfile, delimiter=\"|\")\n        next(reader)  # Skip the header row\n        for row in reader:\n            if len(row) >= 2:\n                audio_file = row[0].strip()  # First column: audio file path\n                text = row[1].strip()  # Second column: text\n                audio_file_path = parent / audio_file\n                audio_text_pairs.append((audio_file_path.as_posix(), text))\n\n    return audio_text_pairs\n\n\n\ndef test_read_audio_text_pairs():\n    # Create temporary CSV file\n    csv_file_path = \"/home/user/tmp/test_audio_text_pairs.csv\"\n    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:\n        writer = csv.writer(csvfile, delimiter=\"|\")\n        writer.writerow(['audio_file', 'text'])\n        writer.writerow(['audio1.wav', 'Hello world'])\n        writer.writerow(['audio2.wav', 'Test'])\n        writer.writerow(['audio3.wav', 'Another'])\n\n    # Test both implementations\n    expected_pairs = [\n        ('/home/user/tmp/audio1.wav', 'Hello world'),\n        ('/home/user/tmp/audio2.wav', 'Test'),\n        ('/home/user/tmp/audio3.wav', 'Another')\n    ]\n    \n    old_result = read_audio_text_pairs(csv_file_path)\n    new_result = read_audio_text_pairs_new_implementation(csv_file_path)\n\n    # Assert results are the same\n    assert old_result == expected_pairs, f\"Old implementation failed: {old_result}\"\n    assert new_result == expected_pairs, f\"New implementation failed: {new_result}\"\n    assert old_result == new_result, \"Old and new implementations produce different results\"\n\nif __name__ == \"__main__\":\n    test_read_audio_text_pairs()"
    },
    {
        "func_name": "list_str_to_tensor",
        "idx": "71",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/utils.py",
        "orig_func": "def list_str_to_tensor(text: list[str], padding_value=-1) -> int['b nt']:\n    list_tensors = [torch.tensor([*bytes(t, 'UTF-8')]) for t in text]\n    text = pad_sequence(list_tensors, padding_value=padding_value, batch_first=True)\n    return text",
        "orig_context": "```python\n## src/f5_tts/model/utils.py\nimport torch\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef list_str_to_tensor(text: list[str], padding_value=-1) -> int[\"b nt\"]:  # noqa: F722\n    list_tensors = [torch.tensor([*bytes(t, \"UTF-8\")]) for t in text]  # ByT5 style\n    text = pad_sequence(list_tensors, padding_value=padding_value, batch_first=True)\n    return text\n\n```\n\n\n",
        "eval_script": "import torch\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef list_str_to_tensor(text: list[str], padding_value=-1) -> torch.Tensor:\n    list_tensors = [torch.tensor([*bytes(t, \"UTF-8\")]) for t in text]\n    text = pad_sequence(list_tensors, padding_value=padding_value, batch_first=True)\n    return text\n\n# Assume this is the new function to be tested\n\n\ndef test_list_str_to_tensor():\n    # Test case 1: Basic example test\n    example_text = [\"hello\", \"world\"]\n    expected = list_str_to_tensor(example_text)\n    new_output = list_str_to_tensor_new_implementation(example_text)\n    assert torch.equal(expected, new_output), \"Test case 1 failed.\"\n\n    # Test case 2: Including empty string\n    test_text = [\"hi\", \"\"]\n    expected = list_str_to_tensor(test_text)\n    new_output = list_str_to_tensor_new_implementation(test_text)\n    assert torch.equal(expected, new_output), \"Test case 2 failed.\"\n\n    # Test case 3: Different length strings\n    test_text = [\"a\", \"abc\", \"ab\"]\n    expected = list_str_to_tensor(test_text)\n    new_output = list_str_to_tensor_new_implementation(test_text)\n    assert torch.equal(expected, new_output), \"Test case 3 failed.\"\n\nif __name__ == \"__main__\":\n    test_list_str_to_tensor()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "infer_process",
        "idx": "72",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/infer/utils_infer.py",
        "orig_func": "def infer_process(ref_audio, ref_text, gen_text, model_obj, vocoder, mel_spec_type=mel_spec_type, show_info=print, progress=tqdm, target_rms=target_rms, cross_fade_duration=cross_fade_duration, nfe_step=nfe_step, cfg_strength=cfg_strength, sway_sampling_coef=sway_sampling_coef, speed=speed, fix_duration=fix_duration, device=device):\n    (audio, sr) = torchaudio.load(ref_audio)\n    max_chars = int(len(ref_text.encode('utf-8')) / (audio.shape[-1] / sr) * (25 - audio.shape[-1] / sr))\n    gen_text_batches = chunk_text(gen_text, max_chars=max_chars)\n    for (i, gen_text) in enumerate(gen_text_batches):\n        print(f'gen_text {i}', gen_text)\n    show_info(f'Generating audio in {len(gen_text_batches)} batches...')\n    return infer_batch_process((audio, sr), ref_text, gen_text_batches, model_obj, vocoder, mel_spec_type=mel_spec_type, progress=progress, target_rms=target_rms, cross_fade_duration=cross_fade_duration, nfe_step=nfe_step, cfg_strength=cfg_strength, sway_sampling_coef=sway_sampling_coef, speed=speed, fix_duration=fix_duration, device=device)",
        "orig_context": "```python\n## src/f5_tts/infer/utils_infer.py\nimport re\n\nimport numpy as np\n\nimport torch\n\nimport torchaudio\n\nimport tqdm\n\nfrom f5_tts.model.utils import (\n    get_tokenizer,\n    convert_char_to_pinyin,\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\ntarget_sample_rate = 24000\n\nhop_length = 256\n\nmel_spec_type = \"vocos\"\n\ntarget_rms = 0.1\n\ncross_fade_duration = 0.15\n\nnfe_step = 32\n\ncfg_strength = 2.0\n\nsway_sampling_coef = -1.0\n\nspeed = 1.0\n\nfix_duration = None\n\ndef chunk_text(text, max_chars=135):\n    \"\"\"\n    Splits the input text into chunks, each with a maximum number of characters.\n\n    Args:\n        text (str): The text to be split.\n        max_chars (int): The maximum number of characters per chunk.\n\n    Returns:\n        List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = \"\"\n    # Split the text into sentences based on punctuation followed by whitespace\n    sentences = re.split(r\"(?<=[;:,.!?])\\s+|(?<=[\uff1b\uff1a\uff0c\u3002\uff01\uff1f])\", text)\n\n    for sentence in sentences:\n        if len(current_chunk.encode(\"utf-8\")) + len(sentence.encode(\"utf-8\")) <= max_chars:\n            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\ndef infer_batch_process(\n    ref_audio,\n    ref_text,\n    gen_text_batches,\n    model_obj,\n    vocoder,\n    mel_spec_type=\"vocos\",\n    progress=tqdm,\n    target_rms=0.1,\n    cross_fade_duration=0.15,\n    nfe_step=32,\n    cfg_strength=2.0,\n    sway_sampling_coef=-1,\n    speed=1,\n    fix_duration=None,\n    device=None,\n):\n    audio, sr = ref_audio\n    if audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n\n    rms = torch.sqrt(torch.mean(torch.square(audio)))\n    if rms < target_rms:\n        audio = audio * target_rms / rms\n    if sr != target_sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n        audio = resampler(audio)\n    audio = audio.to(device)\n\n    generated_waves = []\n    spectrograms = []\n\n    if len(ref_text[-1].encode(\"utf-8\")) == 1:\n        ref_text = ref_text + \" \"\n    for i, gen_text in enumerate(progress.tqdm(gen_text_batches)):\n        # Prepare the text\n        text_list = [ref_text + gen_text]\n        final_text_list = convert_char_to_pinyin(text_list)\n\n        ref_audio_len = audio.shape[-1] // hop_length\n        if fix_duration is not None:\n            duration = int(fix_duration * target_sample_rate / hop_length)\n        else:\n            # Calculate duration\n            ref_text_len = len(ref_text.encode(\"utf-8\"))\n            gen_text_len = len(gen_text.encode(\"utf-8\"))\n            duration = ref_audio_len + int(ref_audio_len / ref_text_len * gen_text_len / speed)\n\n        # inference\n        with torch.inference_mode():\n            generated, _ = model_obj.sample(\n                cond=audio,\n                text=final_text_list,\n                duration=duration,\n                steps=nfe_step,\n                cfg_strength=cfg_strength,\n                sway_sampling_coef=sway_sampling_coef,\n            )\n\n            generated = generated.to(torch.float32)\n            generated = generated[:, ref_audio_len:, :]\n            generated_mel_spec = generated.permute(0, 2, 1)\n            if mel_spec_type == \"vocos\":\n                generated_wave = vocoder.decode(generated_mel_spec)\n            elif mel_spec_type == \"bigvgan\":\n                generated_wave = vocoder(generated_mel_spec)\n            if rms < target_rms:\n                generated_wave = generated_wave * rms / target_rms\n\n            # wav -> numpy\n            generated_wave = generated_wave.squeeze().cpu().numpy()\n\n            generated_waves.append(generated_wave)\n            spectrograms.append(generated_mel_spec[0].cpu().numpy())\n\n    # Combine all generated waves with cross-fading\n    if cross_fade_duration <= 0:\n        # Simply concatenate\n        final_wave = np.concatenate(generated_waves)\n    else:\n        final_wave = generated_waves[0]\n        for i in range(1, len(generated_waves)):\n            prev_wave = final_wave\n            next_wave = generated_waves[i]\n\n            # Calculate cross-fade samples, ensuring it does not exceed wave lengths\n            cross_fade_samples = int(cross_fade_duration * target_sample_rate)\n            cross_fade_samples = min(cross_fade_samples, len(prev_wave), len(next_wave))\n\n            if cross_fade_samples <= 0:\n                # No overlap possible, concatenate\n                final_wave = np.concatenate([prev_wave, next_wave])\n                continue\n\n            # Overlapping parts\n            prev_overlap = prev_wave[-cross_fade_samples:]\n            next_overlap = next_wave[:cross_fade_samples]\n\n            # Fade out and fade in\n            fade_out = np.linspace(1, 0, cross_fade_samples)\n            fade_in = np.linspace(0, 1, cross_fade_samples)\n\n            # Cross-faded overlap\n            cross_faded_overlap = prev_overlap * fade_out + next_overlap * fade_in\n\n            # Combine\n            new_wave = np.concatenate(\n                [prev_wave[:-cross_fade_samples], cross_faded_overlap, next_wave[cross_fade_samples:]]\n            )\n\n            final_wave = new_wave\n\n    # Create a combined spectrogram\n    combined_spectrogram = np.concatenate(spectrograms, axis=1)\n\n    return final_wave, target_sample_rate, combined_spectrogram\n\ndef infer_process(\n    ref_audio,\n    ref_text,\n    gen_text,\n    model_obj,\n    vocoder,\n    mel_spec_type=mel_spec_type,\n    show_info=print,\n    progress=tqdm,\n    target_rms=target_rms,\n    cross_fade_duration=cross_fade_duration,\n    nfe_step=nfe_step,\n    cfg_strength=cfg_strength,\n    sway_sampling_coef=sway_sampling_coef,\n    speed=speed,\n    fix_duration=fix_duration,\n    device=device,\n):\n    # Split the input text into batches\n    audio, sr = torchaudio.load(ref_audio)\n    max_chars = int(len(ref_text.encode(\"utf-8\")) / (audio.shape[-1] / sr) * (25 - audio.shape[-1] / sr))\n    gen_text_batches = chunk_text(gen_text, max_chars=max_chars)\n    for i, gen_text in enumerate(gen_text_batches):\n        print(f\"gen_text {i}\", gen_text)\n\n    show_info(f\"Generating audio in {len(gen_text_batches)} batches...\")\n    return infer_batch_process(\n        (audio, sr),\n        ref_text,\n        gen_text_batches,\n        model_obj,\n        vocoder,\n        mel_spec_type=mel_spec_type,\n        progress=progress,\n        target_rms=target_rms,\n        cross_fade_duration=cross_fade_duration,\n        nfe_step=nfe_step,\n        cfg_strength=cfg_strength,\n        sway_sampling_coef=sway_sampling_coef,\n        speed=speed,\n        fix_duration=fix_duration,\n        device=device,\n    )\n\n```\n\n\n",
        "eval_script": "import re\nimport numpy as np\nimport torch\nimport torchaudio\nfrom unittest.mock import Mock\nimport tqdm\n\n# Mock functions to replace the actual imports that are not available\ndef get_tokenizer():\n    return Mock()\n\ndef convert_char_to_pinyin(text_list):\n    # Assuming this function converts Chinese characters to Pinyin; mocking it simply returns the same text for testing\n    return text_list\n\n# Mocking the f5_tts.model.utils module\nclass f5_tts:\n    class model:\n        class utils:\n            get_tokenizer = get_tokenizer\n            convert_char_to_pinyin = convert_char_to_pinyin\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\ntarget_sample_rate = 24000\n\n# Removed problematic import statement\n\nhop_length = 256\n\nmel_spec_type = \"vocos\"\n\ntarget_rms = 0.1\n\ncross_fade_duration = 0.15\n\nnfe_step = 32\n\ncfg_strength = 2.0\n\nsway_sampling_coef = -1.0\n\nspeed = 1.0\n\nfix_duration = None\n\ndef chunk_text(text, max_chars=135):\n    \"\"\"\n    Splits the input text into chunks, each with a maximum number of characters.\n\n    Args:\n        text (str): The text to be split.\n        max_chars (int): The maximum number of characters per chunk.\n\n    Returns:\n        List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = \"\"\n    # Split the text into sentences based on punctuation followed by whitespace\n    sentences = re.split(r\"(?<=[;:,.!?])\\s+|(?<=[\uff1b\uff1a\uff0c\u3002\uff01\uff1f])\", text)\n\n    for sentence in sentences:\n        if len(current_chunk.encode(\"utf-8\")) + len(sentence.encode(\"utf-8\")) <= max_chars:\n            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\ndef infer_batch_process(\n    ref_audio,\n    ref_text,\n    gen_text_batches,\n    model_obj,\n    vocoder,\n    mel_spec_type=\"vocos\",\n    progress=tqdm,\n    target_rms=0.1,\n    cross_fade_duration=0.15,\n    nfe_step=32,\n    cfg_strength=2.0,\n    sway_sampling_coef=-1,\n    speed=1,\n    fix_duration=None,\n    device=None,\n):\n    audio, sr = ref_audio\n    if audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n\n    rms = torch.sqrt(torch.mean(torch.square(audio)))\n    if rms < target_rms:\n        audio = audio * target_rms / rms\n    if sr != target_sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n        audio = resampler(audio)\n    audio = audio.to(device)\n\n    generated_waves = []\n    spectrograms = []\n\n    if len(ref_text[-1].encode(\"utf-8\")) == 1:\n        ref_text = ref_text + \" \"\n    for i, gen_text in enumerate(progress.tqdm(gen_text_batches)):\n        # Prepare the text\n        text_list = [ref_text + gen_text]\n        final_text_list = convert_char_to_pinyin(text_list)\n\n        ref_audio_len = audio.shape[-1] // hop_length\n        if fix_duration is not None:\n            duration = int(fix_duration * target_sample_rate / hop_length)\n        else:\n            # Calculate duration\n            ref_text_len = len(ref_text.encode(\"utf-8\"))\n            gen_text_len = len(gen_text.encode(\"utf-8\"))\n            duration = ref_audio_len + int(ref_audio_len / ref_text_len * gen_text_len / speed)\n\n        # inference\n        with torch.inference_mode():\n            generated, _ = model_obj.sample(\n                cond=audio,\n                text=final_text_list,\n                duration=duration,\n                steps=nfe_step,\n                cfg_strength=cfg_strength,\n                sway_sampling_coef=sway_sampling_coef,\n            )\n\n            generated = generated.to(torch.float32)\n            generated = generated[:, ref_audio_len:, :]\n            generated_mel_spec = generated.permute(0, 2, 1)\n            if mel_spec_type == \"vocos\":\n                generated_wave = vocoder.decode(generated_mel_spec)\n            elif mel_spec_type == \"bigvgan\":\n                generated_wave = vocoder(generated_mel_spec)\n            if rms < target_rms:\n                generated_wave = generated_wave * rms / target_rms\n\n            # wav -> numpy\n            generated_wave = generated_wave.squeeze().cpu().numpy()\n\n            generated_waves.append(generated_wave)\n            spectrograms.append(generated_mel_spec[0].cpu().numpy())\n\n    # Combine all generated waves with cross-fading\n    if cross_fade_duration <= 0:\n        # Simply concatenate\n        final_wave = np.concatenate(generated_waves)\n    else:\n        final_wave = generated_waves[0]\n        for i in range(1, len(generated_waves)):\n            prev_wave = final_wave\n            next_wave = generated_waves[i]\n\n            # Calculate cross-fade samples, ensuring it does not exceed wave lengths\n            cross_fade_samples = int(cross_fade_duration * target_sample_rate)\n            cross_fade_samples = min(cross_fade_samples, len(prev_wave), len(next_wave))\n\n            if cross_fade_samples <= 0:\n                # No overlap possible, concatenate\n                final_wave = np.concatenate([prev_wave, next_wave])\n                continue\n\n            # Overlapping parts\n            prev_overlap = prev_wave[-cross_fade_samples:]\n            next_overlap = next_wave[:cross_fade_samples]\n\n            # Fade out and fade in\n            fade_out = np.linspace(1, 0, cross_fade_samples)\n            fade_in = np.linspace(0, 1, cross_fade_samples)\n\n            # Cross-faded overlap\n            cross_faded_overlap = prev_overlap * fade_out + next_overlap * fade_in\n\n            # Combine\n            new_wave = np.concatenate(\n                [prev_wave[:-cross_fade_samples], cross_faded_overlap, next_wave[cross_fade_samples:]]\n            )\n\n            final_wave = new_wave\n\n    # Create a combined spectrogram\n    combined_spectrogram = np.concatenate(spectrograms, axis=1)\n\n    return final_wave, target_sample_rate, combined_spectrogram\n\ndef infer_process(\n    ref_audio,\n    ref_text,\n    gen_text,\n    model_obj,\n    vocoder,\n    mel_spec_type=mel_spec_type,\n    show_info=print,\n    progress=tqdm,\n    target_rms=target_rms,\n    cross_fade_duration=cross_fade_duration,\n    nfe_step=nfe_step,\n    cfg_strength=cfg_strength,\n    sway_sampling_coef=sway_sampling_coef,\n    speed=speed,\n    fix_duration=fix_duration,\n    device=device,\n):\n    # ref_audio is expected to be (audio_tensor, sample_rate)\n    audio, sr = ref_audio\n    max_chars = int(len(ref_text.encode(\"utf-8\")) / (audio.shape[-1] / sr) * (25 - audio.shape[-1] / sr))\n    gen_text_batches = chunk_text(gen_text, max_chars=max_chars)\n    for i, gen_text in enumerate(gen_text_batches):\n        print(f\"gen_text {i}\", gen_text)\n\n    show_info(f\"Generating audio in {len(gen_text_batches)} batches...\")\n    return infer_batch_process(\n        (audio, sr),\n        ref_text,\n        gen_text_batches,\n        model_obj,\n        vocoder,\n        mel_spec_type=mel_spec_type,\n        progress=progress,\n        target_rms=target_rms,\n        cross_fade_duration=cross_fade_duration,\n        nfe_step=nfe_step,\n        cfg_strength=cfg_strength,\n        sway_sampling_coef=sway_sampling_coef,\n        speed=speed,\n        fix_duration=fix_duration,\n        device=device,\n    )\n\n\ndef test_infer_process():\n    # Example mock for ref_audio, since the real one cannot be used\n    ref_audio = (torch.randn(1, 48000), 24000)  # Mock (audio, sample rate)\n    ref_text = \"example reference text\"\n    gen_text = \"example generated text\"\n    mock_model = Mock()\n    mock_model.sample = Mock(return_value=(torch.randn(1, 80, 600), None))\n    \n    mock_vocoder = Mock()\n    mock_vocoder.decode = Mock(return_value=torch.randn(1, 1, 44100))\n    \n    # Test 1\n    result1 = infer_process(ref_audio, ref_text, gen_text, mock_model, mock_vocoder)\n    result2 = infer_process_new_implementation(ref_audio, ref_text, gen_text, mock_model, mock_vocoder)\n    assert np.array_equal(result1[0], result2[0]), \"Test 1 failed: Waveforms are different\"\n    assert result1[1] == result2[1], \"Test 1 failed: Sampling rates are different\"\n    assert np.array_equal(result1[2], result2[2]), \"Test 1 failed: Spectrograms are different\"\n    \n    # Test 2 with different input\n    gen_text2 = \"another example generated text\"\n    result1 = infer_process(ref_audio, ref_text, gen_text2, mock_model, mock_vocoder)\n    result2 = infer_process_new_implementation(ref_audio, ref_text, gen_text2, mock_model, mock_vocoder)\n    assert np.array_equal(result1[0], result2[0]), \"Test 2 failed: Waveforms are different\"\n    assert result1[1] == result2[1], \"Test 2 failed: Sampling rates are different\"\n    assert np.array_equal(result1[2], result2[2]), \"Test 2 failed: Spectrograms are different\"\n\n    # Test 3 with another variation\n    ref_text3 = \"different reference text\"\n    result1 = infer_process(ref_audio, ref_text3, gen_text2, mock_model, mock_vocoder)\n    result2 = infer_process_new_implementation(ref_audio, ref_text3, gen_text2, mock_model, mock_vocoder)\n    assert np.array_equal(result1[0], result2[0]), \"Test 3 failed: Waveforms are different\"\n    assert result1[1] == result2[1], \"Test 3 failed: Sampling rates are different\"\n    assert np.array_equal(result1[2], result2[2]), \"Test 3 failed: Spectrograms are different\"\n    \n    print(\"All tests passed successfully.\")\n    \nif __name__ == \"__main__\":\n    test_infer_process()"
    },
    {
        "func_name": "remove_silence_edges",
        "idx": "73",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/infer/utils_infer.py",
        "orig_func": "def remove_silence_edges(audio, silence_threshold=-42):\n    non_silent_start_idx = silence.detect_leading_silence(audio, silence_threshold=silence_threshold)\n    audio = audio[non_silent_start_idx:]\n    non_silent_end_duration = audio.duration_seconds\n    for ms in reversed(audio):\n        if ms.dBFS > silence_threshold:\n            break\n        non_silent_end_duration -= 0.001\n    trimmed_audio = audio[:int(non_silent_end_duration * 1000)]\n    return trimmed_audio",
        "orig_context": "```python\n## src/f5_tts/infer/utils_infer.py\nfrom pydub import AudioSegment, silence\n\ndef remove_silence_edges(audio, silence_threshold=-42):\n    # Remove silence from the start\n    non_silent_start_idx = silence.detect_leading_silence(audio, silence_threshold=silence_threshold)\n    audio = audio[non_silent_start_idx:]\n\n    # Remove silence from the end\n    non_silent_end_duration = audio.duration_seconds\n    for ms in reversed(audio):\n        if ms.dBFS > silence_threshold:\n            break\n        non_silent_end_duration -= 0.001\n    trimmed_audio = audio[: int(non_silent_end_duration * 1000)]\n\n    return trimmed_audio\n\n```\n\n\n",
        "eval_script": "from pydub import AudioSegment, silence\nfrom pydub.generators import Sine\n\ndef remove_silence_edges(audio, silence_threshold=-42):\n    # Remove silence from the start\n    non_silent_start_idx = detect_leading_silence(audio, silence_threshold=silence_threshold)\n    audio = audio[non_silent_start_idx:]\n\n    # Remove silence from the end\n    non_silent_end_duration = audio.duration_seconds\n    for ms in reversed(audio):\n        if ms.dBFS > silence_threshold:\n            break\n        non_silent_end_duration -= 0.001\n    trimmed_audio = audio[: int(non_silent_end_duration * 1000)]\n\n    return trimmed_audio\n\n\ndef detect_leading_silence(sound, silence_threshold=-42.0, chunk_size=10):\n    trim_ms = 0\n    assert chunk_size > 0  # chunk size must be greater than 0\n    while trim_ms < len(sound) and sound[trim_ms:trim_ms + chunk_size].dBFS < silence_threshold:\n        trim_ms += chunk_size\n    return trim_ms\n\n# Assume we have the new function implemented\n\n\ndef test_remove_silence_edges():\n    # Example test cases with dummy audio segments\n    audio1 = AudioSegment.silent(duration=1000) + AudioSegment.from_mono_audiosegments(\n        AudioSegment.silent(duration=500), AudioSegment.silent(duration=500)\n    )  # Silent audio\n    audio2 = AudioSegment.silent(duration=1000) + Sine(440).to_audio_segment(duration=1000)  # Leading silence\n    audio3 = Sine(440).to_audio_segment(duration=1000) + AudioSegment.silent(duration=1000)  # Trailing silence\n\n    # Test 1: Fully silent audio\n    assert remove_silence_edges(audio1) == remove_silence_edges_new_implementation(audio1)\n\n    # Test 2: Audio with leading silence\n    assert remove_silence_edges(audio2) == remove_silence_edges_new_implementation(audio2)\n\n    # Test 3: Audio with trailing silence\n    assert remove_silence_edges(audio3) == remove_silence_edges_new_implementation(audio3)\n\nif __name__ == \"__main__\":\n    test_remove_silence_edges()"
    },
    {
        "func_name": "lens_to_mask",
        "idx": "74",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/utils.py",
        "orig_func": "def lens_to_mask(t: int['b'], length: int | None=None) -> bool['b n']:\n    if not exists(length):\n        length = t.amax()\n    seq = torch.arange(length, device=t.device)\n    return seq[None, :] < t[:, None]",
        "orig_context": "```python\n## src/f5_tts/model/utils.py\nimport torch\n\ndef exists(v):\n    return v is not None\n\ndef lens_to_mask(t: int[\"b\"], length: int | None = None) -> bool[\"b n\"]:  # noqa: F722 F821\n    if not exists(length):\n        length = t.amax()\n\n    seq = torch.arange(length, device=t.device)\n    return seq[None, :] < t[:, None]\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/model/utils.py\nimport torch\n\ndef exists(v):\n    return v is not None\n\ndef lens_to_mask(t: torch.Tensor, length: int | None = None) -> torch.Tensor:\n    if not exists(length):\n        length = t.amax()\n\n    seq = torch.arange(length, device=t.device)\n    return seq[None, :] < t[:, None]\n\n\n\ndef test_lens_to_mask():\n    # Test case 1: Basic functionality\n    input_tensor = torch.tensor([3, 1, 2])\n    expected_output = lens_to_mask(input_tensor)\n    assert torch.equal(lens_to_mask_new_implementation(input_tensor), expected_output), \"Test case 1 failed\"\n\n    # Test case 2: With specified length\n    input_tensor = torch.tensor([2, 3])\n    length = 4\n    expected_output = lens_to_mask(input_tensor, length)\n    assert torch.equal(lens_to_mask_new_implementation(input_tensor, length), expected_output), \"Test case 2 failed\"\n\n    # Test case 3: Single-element tensor\n    input_tensor = torch.tensor([0])\n    expected_output = lens_to_mask(input_tensor)\n    assert torch.equal(lens_to_mask_new_implementation(input_tensor), expected_output), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_lens_to_mask()"
    },
    {
        "func_name": "SE_Connect.forward",
        "idx": "75",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/eval/ecapa_tdnn.py",
        "orig_func": "def forward(self, x):\n    out = x.mean(dim=2)\n    out = F.relu(self.linear1(out))\n    out = torch.sigmoid(self.linear2(out))\n    out = x * out.unsqueeze(2)\n    return out",
        "orig_context": "```python\n## src/f5_tts/eval/ecapa_tdnn.py\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nclass SE_Connect(nn.Module):\n    def __init__(self, channels, se_bottleneck_dim=128):\n        super().__init__()\n        self.linear1 = nn.Linear(channels, se_bottleneck_dim)\n        self.linear2 = nn.Linear(se_bottleneck_dim, channels)\n\n    def forward(self, x):\n        out = x.mean(dim=2)\n        out = F.relu(self.linear1(out))\n        out = torch.sigmoid(self.linear2(out))\n        out = x * out.unsqueeze(2)\n\n        return out\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/eval/ecapa_tdnn.py\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nclass SE_Connect(nn.Module):\n    def __init__(self, channels, se_bottleneck_dim=128):\n        super().__init__()\n        self.linear1 = nn.Linear(channels, se_bottleneck_dim)\n        self.linear2 = nn.Linear(se_bottleneck_dim, channels)\n\n    def forward(self, x):\n        out = x.mean(dim=2)\n        out = F.relu(self.linear1(out))\n        out = torch.sigmoid(self.linear2(out))\n        out = x * out.unsqueeze(2)\n        return out\n        \n\n\ndef test_forward():\n    channels = 10\n    se_bottleneck_dim = 5\n    model = SE_Connect(channels, se_bottleneck_dim)\n    \n    # Test case 1\n    x1 = torch.randn(1, channels, 4)\n    assert torch.allclose(model.forward(x1), model.forward_new_implementation(x1)), \"Test case 1 failed\"\n    \n    # Test case 2\n    x2 = torch.randn(2, channels, 6)\n    assert torch.allclose(model.forward(x2), model.forward_new_implementation(x2)), \"Test case 2 failed\"\n\n    # Test case 3\n    x3 = torch.randn(3, channels, 5)\n    assert torch.allclose(model.forward(x3), model.forward_new_implementation(x3)), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "ConvPositionEmbedding.forward",
        "idx": "76",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/modules.py",
        "orig_func": "def forward(self, x: float['b n d'], mask: bool['b n'] | None=None):\n    if mask is not None:\n        mask = mask[..., None]\n        x = x.masked_fill(~mask, 0.0)\n    x = x.permute(0, 2, 1)\n    x = self.conv1d(x)\n    out = x.permute(0, 2, 1)\n    if mask is not None:\n        out = out.masked_fill(~mask, 0.0)\n    return out",
        "orig_context": "```python\n## src/f5_tts/model/modules.py\nfrom torch import nn\n\nclass ConvPositionEmbedding(nn.Module):\n    def __init__(self, dim, kernel_size=31, groups=16):\n        super().__init__()\n        assert kernel_size % 2 != 0\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(dim, dim, kernel_size, groups=groups, padding=kernel_size // 2),\n            nn.Mish(),\n            nn.Conv1d(dim, dim, kernel_size, groups=groups, padding=kernel_size // 2),\n            nn.Mish(),\n        )\n\n    def forward(self, x: float[\"b n d\"], mask: bool[\"b n\"] | None = None):  # noqa: F722\n        if mask is not None:\n            mask = mask[..., None]\n            x = x.masked_fill(~mask, 0.0)\n\n        x = x.permute(0, 2, 1)\n        x = self.conv1d(x)\n        out = x.permute(0, 2, 1)\n\n        if mask is not None:\n            out = out.masked_fill(~mask, 0.0)\n\n        return out\n\n```\n\n\n",
        "eval_script": "import torch\nfrom torch import nn\n\nclass ConvPositionEmbedding(nn.Module):\n    def __init__(self, dim, kernel_size=31, groups=16):\n        super().__init__()\n        assert kernel_size % 2 != 0\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(dim, dim, kernel_size, groups=groups, padding=kernel_size // 2),\n            nn.Mish(),\n            nn.Conv1d(dim, dim, kernel_size, groups=groups, padding=kernel_size // 2),\n            nn.Mish(),\n        )\n\n    def forward(self, x: torch.FloatTensor, mask: torch.BoolTensor | None = None):  # noqa: F722\n        if mask is not None:\n            mask = mask[..., None]\n            x = x.masked_fill(~mask, 0.0)\n\n        x = x.permute(0, 2, 1)\n        x = self.conv1d(x)\n        out = x.permute(0, 2, 1)\n\n        if mask is not None:\n            out = out.masked_fill(~mask, 0.0)\n\n        return out\n\n\n\ndef test_forward():\n    # Create a ConvPositionEmbedding instance\n    dim = 32\n    model = ConvPositionEmbedding(dim)\n\n    # Generate test inputs\n    x = torch.randn(8, 10, dim)  # A random input tensor\n    mask = (torch.randn(8, 10) > 0).bool()  # Random boolean mask\n\n    # Get outputs from both implementations\n    original_output = model.forward(x, mask)\n    new_output = model.forward_new_implementation(x, mask)\n\n    # Test assertions\n    assert torch.allclose(original_output, new_output), \"Outputs do not match!\"\n    \n    # Additional tests with no mask\n    original_output_nomask = model.forward(x)\n    new_output_nomask = model.forward_new_implementation(x)\n    assert torch.allclose(original_output_nomask, new_output_nomask), \"Outputs without mask do not match!\"\n    \n    # Testing with zero input\n    zero_x = torch.zeros_like(x)\n    original_output_zero = model.forward(zero_x)\n    new_output_zero = model.forward_new_implementation(zero_x)\n    assert torch.allclose(original_output_zero, new_output_zero), \"Outputs with zero input do not match!\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "TextEmbedding.forward",
        "idx": "78",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/backbones/mmdit.py",
        "orig_func": "def forward(self, text: int['b nt'], drop_text=False) -> int['b nt d']:\n    text = text + 1\n    if drop_text:\n        text = torch.zeros_like(text)\n    text = self.text_embed(text)\n    batch_start = torch.zeros((text.shape[0],), dtype=torch.long)\n    batch_text_len = text.shape[1]\n    pos_idx = get_pos_embed_indices(batch_start, batch_text_len, max_pos=self.precompute_max_pos)\n    text_pos_embed = self.freqs_cis[pos_idx]\n    text = text + text_pos_embed\n    return text",
        "orig_context": "```python\n## src/f5_tts/model/backbones/mmdit.py\nimport torch\n\nfrom torch import nn\n\nfrom f5_tts.model.modules import (\n    TimestepEmbedding,\n    ConvPositionEmbedding,\n    MMDiTBlock,\n    AdaLayerNormZero_Final,\n    precompute_freqs_cis,\n    get_pos_embed_indices,\n)\n\nclass TextEmbedding(nn.Module):\n    def __init__(self, out_dim, text_num_embeds):\n        super().__init__()\n        self.text_embed = nn.Embedding(text_num_embeds + 1, out_dim)  # will use 0 as filler token\n\n        self.precompute_max_pos = 1024\n        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(out_dim, self.precompute_max_pos), persistent=False)\n\n    def forward(self, text: int[\"b nt\"], drop_text=False) -> int[\"b nt d\"]:  # noqa: F722\n        text = text + 1\n        if drop_text:\n            text = torch.zeros_like(text)\n        text = self.text_embed(text)\n\n        # sinus pos emb\n        batch_start = torch.zeros((text.shape[0],), dtype=torch.long)\n        batch_text_len = text.shape[1]\n        pos_idx = get_pos_embed_indices(batch_start, batch_text_len, max_pos=self.precompute_max_pos)\n        text_pos_embed = self.freqs_cis[pos_idx]\n\n        text = text + text_pos_embed\n\n        return text\n\n```\n\n\n",
        "eval_script": "import torch\nfrom torch import nn\n\n# Mocked functions and classes\ndef precompute_freqs_cis(out_dim, precompute_max_pos):\n    # Return a tensor mimicking the frequency computations\n    return torch.zeros((precompute_max_pos, out_dim))\n\ndef get_pos_embed_indices(batch_start, batch_text_len, max_pos):\n    # Mock embedding index generation\n    return torch.arange(batch_text_len).expand((len(batch_start), batch_text_len))\n\n# Main TextEmbedding class\nclass TextEmbedding(nn.Module):\n    def __init__(self, out_dim, text_num_embeds):\n        super().__init__()\n        self.text_embed = nn.Embedding(text_num_embeds + 1, out_dim)  # will use 0 as filler token\n\n        self.precompute_max_pos = 1024\n        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(out_dim, self.precompute_max_pos), persistent=False)\n\n    def forward(self, text, drop_text=False):\n        text = text + 1\n        if drop_text:\n            text = torch.zeros_like(text)\n        text = self.text_embed(text)\n\n        # sinus pos emb\n        batch_start = torch.zeros((text.shape[0],), dtype=torch.long)\n        batch_text_len = text.shape[1]\n        pos_idx = get_pos_embed_indices(batch_start, batch_text_len, max_pos=self.precompute_max_pos)\n        text_pos_embed = self.freqs_cis[pos_idx]\n\n        text = text + text_pos_embed\n\n        return text\n\n\n\ndef test_forward():\n    # Initialize text embedding\n    out_dim = 10\n    text_num_embeds = 20\n    embedding = TextEmbedding(out_dim, text_num_embeds)\n    \n    # Test case 1: Standard input\n    text = torch.randint(0, text_num_embeds, (2, 5))  # random text input tensor\n    output1 = embedding.forward(text)\n    output2 = embedding.forward_new_implementation(text)\n    assert torch.allclose(output1, output2), \"Mismatch in standard input\"\n\n    # Test case 2: Input with drop_text=True\n    output3 = embedding.forward(text, drop_text=True)\n    output4 = embedding.forward_new_implementation(text, drop_text=True)\n    assert torch.allclose(output3, output4), \"Mismatch with drop_text=True\"\n\n    # Test case 3: Different size input\n    text = torch.randint(0, text_num_embeds, (3, 7))  # different size input\n    output5 = embedding.forward(text)\n    output6 = embedding.forward_new_implementation(text)\n    assert torch.allclose(output5, output6), \"Mismatch with different size input\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "InputEmbedding.forward",
        "idx": "79",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/backbones/unett.py",
        "orig_func": "def forward(self, x: float['b n d'], cond: float['b n d'], text_embed: float['b n d'], drop_audio_cond=False):\n    if drop_audio_cond:\n        cond = torch.zeros_like(cond)\n    x = self.proj(torch.cat((x, cond, text_embed), dim=-1))\n    x = self.conv_pos_embed(x) + x\n    return x",
        "orig_context": "```python\n## src/f5_tts/model/backbones/unett.py\nimport torch\n\nfrom torch import nn\n\nfrom f5_tts.model.modules import (\n    TimestepEmbedding,\n    ConvNeXtV2Block,\n    ConvPositionEmbedding,\n    Attention,\n    AttnProcessor,\n    FeedForward,\n    precompute_freqs_cis,\n    get_pos_embed_indices,\n)\n\nclass InputEmbedding(nn.Module):\n    def __init__(self, mel_dim, text_dim, out_dim):\n        super().__init__()\n        self.proj = nn.Linear(mel_dim * 2 + text_dim, out_dim)\n        self.conv_pos_embed = ConvPositionEmbedding(dim=out_dim)\n\n    def forward(self, x: float[\"b n d\"], cond: float[\"b n d\"], text_embed: float[\"b n d\"], drop_audio_cond=False):  # noqa: F722\n        if drop_audio_cond:  # cfg for cond audio\n            cond = torch.zeros_like(cond)\n\n        x = self.proj(torch.cat((x, cond, text_embed), dim=-1))\n        x = self.conv_pos_embed(x) + x\n        return x\n\n```\n\n\n",
        "eval_script": "import torch\nfrom torch import nn\n\n# Mock class for ConvPositionEmbedding\nclass ConvPositionEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return x\n\n# Redefine InputEmbedding class with our mock ConvPositionEmbedding\nclass InputEmbedding(nn.Module):\n    def __init__(self, mel_dim, text_dim, out_dim):\n        super().__init__()\n        self.proj = nn.Linear(mel_dim * 2 + text_dim, out_dim)\n        self.conv_pos_embed = ConvPositionEmbedding(dim=out_dim)\n\n    def forward(self, x: torch.Tensor, cond: torch.Tensor, text_embed: torch.Tensor, drop_audio_cond=False):\n        if drop_audio_cond:\n            cond = torch.zeros_like(cond)\n\n        x = self.proj(torch.cat((x, cond, text_embed), dim=-1))\n        x = self.conv_pos_embed(x) + x\n        return x\n\n\n\n# Test function to verify that both forward methods are equivalent\ndef test_forward():\n    mel_dim = 4\n    text_dim = 3\n    out_dim = 5\n\n    input_emb = InputEmbedding(mel_dim, text_dim, out_dim)\n\n    x = torch.randn(2, 3, mel_dim)\n    cond = torch.randn(2, 3, mel_dim)\n    text_embed = torch.randn(2, 3, text_dim)\n\n    # Check outputs are the same for default configuration\n    output_old = input_emb.forward(x, cond, text_embed)\n    output_new = input_emb.forward_new_implementation(x, cond, text_embed)\n    assert torch.equal(output_old, output_new), \"Outputs do not match for default config\"\n\n    # Check outputs are the same when drop_audio_cond is True\n    output_old = input_emb.forward(x, cond, text_embed, drop_audio_cond=True)\n    output_new = input_emb.forward_new_implementation(x, cond, text_embed, drop_audio_cond=True)\n    assert torch.equal(output_old, output_new), \"Outputs do not match when drop_audio_cond is True\"\n\n    # Check with other random inputs for robustness\n    x = torch.randn(2, 5, mel_dim)  # Different sequence length\n    cond = torch.randn(2, 5, mel_dim)\n    text_embed = torch.randn(2, 5, text_dim)\n    output_old = input_emb.forward(x, cond, text_embed)\n    output_new = input_emb.forward_new_implementation(x, cond, text_embed)\n    assert torch.equal(output_old, output_new), \"Outputs do not match for different sequence length\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "AudioEmbedding.forward",
        "idx": "80",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/backbones/mmdit.py",
        "orig_func": "def forward(self, x: float['b n d'], cond: float['b n d'], drop_audio_cond=False):\n    if drop_audio_cond:\n        cond = torch.zeros_like(cond)\n    x = torch.cat((x, cond), dim=-1)\n    x = self.linear(x)\n    x = self.conv_pos_embed(x) + x\n    return x",
        "orig_context": "```python\n## src/f5_tts/model/backbones/mmdit.py\nimport torch\n\nfrom torch import nn\n\nfrom f5_tts.model.modules import (\n    TimestepEmbedding,\n    ConvPositionEmbedding,\n    MMDiTBlock,\n    AdaLayerNormZero_Final,\n    precompute_freqs_cis,\n    get_pos_embed_indices,\n)\n\nclass AudioEmbedding(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(2 * in_dim, out_dim)\n        self.conv_pos_embed = ConvPositionEmbedding(out_dim)\n\n    def forward(self, x: float[\"b n d\"], cond: float[\"b n d\"], drop_audio_cond=False):  # noqa: F722\n        if drop_audio_cond:\n            cond = torch.zeros_like(cond)\n        x = torch.cat((x, cond), dim=-1)\n        x = self.linear(x)\n        x = self.conv_pos_embed(x) + x\n        return x\n\n```\n\n\n",
        "eval_script": "# Mocking necessary imports and classes due to missing modules\nimport torch\nfrom torch import nn\nimport numpy as np\n\n# Mock implementation of ConvPositionEmbedding\nclass ConvPositionEmbedding(nn.Module):\n    def __init__(self, out_dim):\n        super().__init__()\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        # Simple mock implementation that returns input as is\n        return x\n\n# Other mocks that are not directly used by the forward function\ndef TimestepEmbedding():\n    pass\n\ndef MMDiTBlock():\n    pass\n\ndef AdaLayerNormZero_Final():\n    pass\n\ndef precompute_freqs_cis():\n    pass\n\ndef get_pos_embed_indices():\n    pass\n\n# Original class with minor modification to path\nclass AudioEmbedding(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(2 * in_dim, out_dim)\n        self.conv_pos_embed = ConvPositionEmbedding(out_dim)\n\n    def forward(self, x: torch.Tensor, cond: torch.Tensor, drop_audio_cond=False):\n        if drop_audio_cond:\n            cond = torch.zeros_like(cond)\n        x = torch.cat((x, cond), dim=-1)\n        x = self.linear(x)\n        x = self.conv_pos_embed(x) + x\n        return x\n\n\n\ndef test_forward():\n    in_dim, out_dim = 4, 6\n    model = AudioEmbedding(in_dim, out_dim)\n    x = torch.randn(1, in_dim)\n    cond = torch.randn(1, in_dim)\n\n    # Test case without dropping audio condition\n    output_original = model.forward(x, cond, drop_audio_cond=False)\n    output_new = model.forward_new_implementation(x, cond, drop_audio_cond=False)\n    assert torch.allclose(output_original, output_new), \"Test failed for drop_audio_cond=False\"\n\n    # Test case with dropping audio condition\n    output_original = model.forward(x, cond, drop_audio_cond=True)\n    output_new = model.forward_new_implementation(x, cond, drop_audio_cond=True)\n    assert torch.allclose(output_original, output_new), \"Test failed for drop_audio_cond=True\"\n\n    # Test with zero condition tensor\n    zero_cond = torch.zeros_like(cond)\n    output_original = model.forward(x, zero_cond, drop_audio_cond=False)\n    output_new = model.forward_new_implementation(x, zero_cond, drop_audio_cond=False)\n    assert torch.allclose(output_original, output_new), \"Test failed for zero condition\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "SinusPositionEmbedding.forward",
        "idx": "81",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/modules.py",
        "orig_func": "def forward(self, x, scale=1000):\n    device = x.device\n    half_dim = self.dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)\n    emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n    return emb",
        "orig_context": "```python\n## src/f5_tts/model/modules.py\nimport math\n\nimport torch\n\nfrom torch import nn\n\nclass SinusPositionEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x, scale=1000):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)\n        emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/model/modules.py\nimport math\n\nimport torch\n\nfrom torch import nn\n\nclass SinusPositionEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x, scale=1000):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)\n        emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n    \n\n\ndef test_forward():\n    model = SinusPositionEmbedding(dim=8)\n    \n    # Test with a simple tensor\n    x = torch.tensor([0.1, 0.2, 0.3])\n    assert torch.allclose(\n        model.forward(x), model.forward_new_implementation(x)\n    ), \"Test case 1 failed\"\n    \n    # Test with larger data\n    x_large = torch.linspace(0, 1, 1000)\n    assert torch.allclose(\n        model.forward(x_large), model.forward_new_implementation(x_large)\n    ), \"Test case 2 failed\"\n    \n    # Test with zero tensor\n    x_zero = torch.zeros(5)\n    assert torch.allclose(\n        model.forward(x_zero), model.forward_new_implementation(x_zero)\n    ), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_forward()\n    print(\"All tests passed successfully.\")"
    },
    {
        "func_name": "GRN.forward",
        "idx": "82",
        "repo_name": "SWivid___F5-TTS",
        "func_path": "src/f5_tts/model/modules.py",
        "orig_func": "def forward(self, x):\n    Gx = torch.norm(x, p=2, dim=1, keepdim=True)\n    Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-06)\n    return self.gamma * (x * Nx) + self.beta + x",
        "orig_context": "```python\n## src/f5_tts/model/modules.py\nimport torch\n\nfrom torch import nn\n\nclass GRN(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n\n    def forward(self, x):\n        Gx = torch.norm(x, p=2, dim=1, keepdim=True)\n        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n        return self.gamma * (x * Nx) + self.beta + x\n\n```\n\n\n",
        "eval_script": "## src/f5_tts/model/modules.py\nimport torch\n\nfrom torch import nn\n\nclass GRN(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n\n    def forward(self, x):\n        Gx = torch.norm(x, p=2, dim=1, keepdim=True)\n        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n        return self.gamma * (x * Nx) + self.beta + x\n\n    # Assume this is the placeholder for the new implementation\n\n\ndef test_forward():\n    dim = 5\n    grn = GRN(dim)\n    test_input = torch.randn(3, 3, dim)  # Random test input tensor\n\n    # First assertion: Check for a specific random input\n    original_output = grn.forward(test_input)\n    new_output = grn.forward_new_implementation(test_input)\n    assert torch.allclose(original_output, new_output), \"Outputs differ for random input.\"\n\n    # Second assertion: Check for a zero input\n    zero_input = torch.zeros(2, 3, dim)\n    original_output = grn.forward(zero_input)\n    new_output = grn.forward_new_implementation(zero_input)\n    assert torch.allclose(original_output, new_output), \"Outputs differ for zero input.\"\n\n    # Third assertion: Check for a ones input\n    ones_input = torch.ones(4, 2, dim)\n    original_output = grn.forward(ones_input)\n    new_output = grn.forward_new_implementation(ones_input)\n    assert torch.allclose(original_output, new_output), \"Outputs differ for ones input.\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "completion",
        "idx": "83",
        "repo_name": "ajrlewis___aikit",
        "func_path": "src/aikit/chat/chat.py",
        "orig_func": "def completion(client, model: str=None, context_messages: list[dict]=[], max_tokens: int=6000, temperature: float=0.2, top_p: float=1, frequency_penalty: float=0, presence_penalty: float=0) -> dict:\n    \"\"\"Returns the next message using a chat completion.\n\n    Args:\n        client: The inference client.\n        context_messages: A list of messages.\n        max_token: The maximum number of tokens allowed in the response 75 words approximately equals 100 tokens.\n        temperature: Controls randomness of the generations between [0, 2]. Lower values ensure less random completions.\n        top_p: Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.\n        frequency_penalty: How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim.\n        presence_penalty: How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics.\n    \"\"\"\n    logger.debug(f'model = {model!r} context_messages = {context_messages!r}')\n    logger.debug(f'temperature = {temperature!r} max_tokens = {max_tokens!r}')\n    output = client.chat.completions.create(messages=context_messages, model=model, temperature=temperature, max_tokens=max_tokens)\n    choices = output.choices\n    choice = choices[0]\n    message = choice.message\n    logger.debug(f'message = {message!r}')\n    content = message.content.strip()\n    logger.debug(f'content = {content!r}')\n    message = messages.create_assistant_message(content=content)\n    return message",
        "orig_context": "```python\n## src/aikit/chat/__init__.py\nfrom . import messages\n\n```\n\n\n```python\n## src/aikit/chat/chat.py\nfrom loguru import logger\n\nfrom . import messages\n\ndef completion(\n    client,\n    model: str = None,\n    context_messages: list[dict] = [],\n    # max_tokens: int = 4096,\n    max_tokens: int = 6000,\n    temperature: float = 0.2,\n    top_p: float = 1,\n    frequency_penalty: float = 0,\n    presence_penalty: float = 0,\n) -> dict:\n    \"\"\"Returns the next message using a chat completion.\n\n    Args:\n        client: The inference client.\n        context_messages: A list of messages.\n        max_token: The maximum number of tokens allowed in the response 75 words approximately equals 100 tokens.\n        temperature: Controls randomness of the generations between [0, 2]. Lower values ensure less random completions.\n        top_p: Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.\n        frequency_penalty: How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim.\n        presence_penalty: How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics.\n    \"\"\"\n    logger.debug(f\"{model = } {context_messages = }\")\n    logger.debug(f\"{temperature = } {max_tokens = }\")\n    output = client.chat.completions.create(\n        messages=context_messages,\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    choices = output.choices\n    choice = choices[0]\n    message = choice.message\n    logger.debug(f\"{message = }\")\n    content = message.content.strip()\n    logger.debug(f\"{content = }\")\n    message = messages.create_assistant_message(content=content)\n    return message\n\n```\n\n\n",
        "eval_script": "# src/aikit/chat/chat.py\n\nfrom loguru import logger\n\nclass MockClient:\n    class Chat:\n        class Completions:\n            @staticmethod\n            def create(messages, model, temperature, max_tokens):\n                # Mock response structure\n                return {\n                    'choices': [\n                        {\n                            'message': {\n                                'content': 'This is a mock response.'\n                            }\n                        }\n                    ]\n                }\n    \n    chat = Chat()\n\nclass messages:\n    @staticmethod\n    def create_assistant_message(content):\n        return {'role': 'assistant', 'content': content}\n\ndef completion(\n    client,\n    model: str = None,\n    context_messages: list[dict] = [],\n    # max_tokens: int = 4096,\n    max_tokens: int = 6000,\n    temperature: float = 0.2,\n    top_p: float = 1,\n    frequency_penalty: float = 0,\n    presence_penalty: float = 0,\n) -> dict:\n    \"\"\"Returns the next message using a chat completion.\n\n    Args:\n        client: The inference client.\n        context_messages: A list of messages.\n        max_token: The maximum number of tokens allowed in the response 75 words approximately equals 100 tokens.\n        temperature: Controls randomness of the generations between [0, 2]. Lower values ensure less random completions.\n        top_p: Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.\n        frequency_penalty: How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim.\n        presence_penalty: How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics.\n    \"\"\"\n    logger.debug(f\"{model = } {context_messages = }\")\n    logger.debug(f\"{temperature = } {max_tokens = }\")\n    output = client.chat.Completions.create(\n        messages=context_messages,\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    choices = output['choices']\n    choice = choices[0]\n    message = choice['message']\n    logger.debug(f\"{message = }\")\n    content = message['content'].strip()\n    logger.debug(f\"{content = }\")\n    message = messages.create_assistant_message(content=content)\n    return message\n\n\ndef test_completion():\n    client = MockClient()\n    model = \"test-model\"\n    context_messages = [{'role': 'user', 'content': 'Hello'}]\n\n    result_old = completion(client, model, context_messages)\n    result_new = completion_new_implementation(client, model, context_messages)\n    assert result_old == result_new, f\"Expected {result_old}, got {result_new}\"\n\n    context_messages = [{'role': 'user', 'content': 'How are you?'}]\n    result_old = completion(client, model, context_messages)\n    result_new = completion_new_implementation(client, model, context_messages)\n    assert result_old == result_new, f\"Expected {result_old}, got {result_new}\"\n\n    context_messages = [{'role': 'user', 'content': 'Goodbye'}]\n    result_old = completion(client, model, context_messages)\n    result_new = completion_new_implementation(client, model, context_messages)\n    assert result_old == result_new, f\"Expected {result_old}, got {result_new}\"\n\nif __name__ == \"__main__\":\n    test_completion()"
    },
    {
        "func_name": "generate_image_name",
        "idx": "86",
        "repo_name": "ajrlewis___aikit",
        "func_path": "src/aikit/image/naming.py",
        "orig_func": "def generate_image_name(prompt):\n    prompt = prompt.replace(' ', '_')\n    prompt = ''.join((e for e in prompt if e.isalnum() or e in ['_', '-', '.']))\n    unique_id = str(uuid.uuid4())\n    image_name = f'{prompt}_{unique_id}.jpg'\n    return image_name",
        "orig_context": "```python\n## src/aikit/image/naming.py\nimport uuid\n\ndef generate_image_name(prompt):\n    # Replace spaces with underscores\n    prompt = prompt.replace(\" \", \"_\")\n\n    # Remove special characters\n    prompt = \"\".join(e for e in prompt if e.isalnum() or e in [\"_\", \"-\", \".\"])\n\n    # Add a unique identifier\n\n    unique_id = str(uuid.uuid4())\n\n    # Combine prompt and unique identifier\n    image_name = f\"{prompt}_{unique_id}.jpg\"\n\n    return image_name\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\n# src/aikit/image/naming.py\nimport uuid\n\ndef generate_image_name(prompt):\n    # Replace spaces with underscores\n    prompt = prompt.replace(\" \", \"_\")\n\n    # Remove special characters\n    prompt = \"\".join(e for e in prompt if e.isalnum() or e in [\"_\", \"-\", \".\"])\n\n    # Add a unique identifier\n    unique_id = str(uuid.uuid4())\n\n    # Combine prompt and unique identifier\n    image_name = f\"{prompt}_{unique_id}.jpg\"\n\n    return image_name\n\n\ndef test_generate_image_name():\n    prompt1 = \"simple test\"\n    prompt2 = \"complex-test_with.special@characters!\"\n    prompt3 = \"another_test_example\"\n\n    generated_old1 = generate_image_name(prompt1)\n    generated_new1 = generate_image_name_new_implementation(prompt1)\n    assert generated_old1.rsplit(\"_\", 1)[0] == generated_new1.rsplit(\"_\", 1)[0]\n\n    generated_old2 = generate_image_name(prompt2)\n    generated_new2 = generate_image_name_new_implementation(prompt2)\n    assert generated_old2.rsplit(\"_\", 1)[0] == generated_new2.rsplit(\"_\", 1)[0]\n\n    generated_old3 = generate_image_name(prompt3)\n    generated_new3 = generate_image_name_new_implementation(prompt3)\n    assert generated_old3.rsplit(\"_\", 1)[0] == generated_new3.rsplit(\"_\", 1)[0]\n\nif __name__ == \"__main__\":\n    test_generate_image_name()"
    },
    {
        "func_name": "get_client",
        "idx": "87",
        "repo_name": "ajrlewis___aikit",
        "func_path": "src/aikit/client/hugging_face_client.py",
        "orig_func": "def get_client(model: str) -> InferenceClient:\n    token = get_token()\n    check_token_access(model, token=token)\n    client = InferenceClient(model, token=token)\n    return client",
        "orig_context": "```python\n## src/aikit/client/hugging_face_client.py\nimport os\n\nfrom huggingface_hub import auth_check, InferenceClient\n\nfrom huggingface_hub.utils import GatedRepoError, RepositoryNotFoundError\n\ndef get_token():\n    token = os.getenv(\"HF_TOKEN\")\n    if not token:\n        raise KeyError(\"HF_TOKEN environmental variable not set\")\n    return token\n\ndef check_token_access(model: str, token: str):\n    try:\n        auth_check(model, token=token)\n    except GatedRepoError:\n        raise GatedRepoError(f\"you don't have permission to access {model = }\")\n    except RepositoryNotFoundError:\n        raise GatedRepoError(\n            f\"the repository was not found or you do not have access {model = }\"\n        )\n\ndef get_client(model: str) -> InferenceClient:\n    token = get_token()  # Get the token\n    check_token_access(model, token=token)  # Check the token can access the repo\n    client = InferenceClient(model, token=token)  # Create and return the client\n    return client\n\n```\n\n\n",
        "eval_script": "# Revised Code.\n\n## src/aikit/client/hugging_face_client.py\nimport os\n\n# Mock version of auth_check that always passes\ndef auth_check(model, token=None):\n    pass\n\n# Mock version of InferenceClient that simulates the client without real functionality\nclass InferenceClient:\n    def __init__(self, model, token=None):\n        self.model = model\n        self.token = token\n\nclass GatedRepoError(Exception):\n    pass\n\nclass RepositoryNotFoundError(Exception):\n    pass\n\ndef get_token():\n    # Return a mock token instead of reading from the environment\n    return \"mock_token\"\n\ndef check_token_access(model: str, token: str):\n    try:\n        auth_check(model, token=token)\n    except GatedRepoError:\n        raise GatedRepoError(f\"you don't have permission to access {model = }\")\n    except RepositoryNotFoundError:\n        raise GatedRepoError(\n            f\"the repository was not found or you do not have access {model = }\"\n        )\n\ndef get_client(model: str) -> InferenceClient:\n    token = get_token()  # Get the token\n    check_token_access(model, token=token)  # Check the token can access the repo\n    client = InferenceClient(model, token=token)  # Create and return the client\n    return client\n\n\n\ndef test_get_client():\n    model_name_1 = \"dummy-model-1\"\n    model_name_2 = \"dummy-model-2\"\n    \n    # Create client instances using the original and new implementation\n    original_client_1 = get_client(model_name_1)\n    new_implementation_client_1 = get_client_new_implementation(model_name_1)\n    \n    original_client_2 = get_client(model_name_2)\n    new_implementation_client_2 = get_client_new_implementation(model_name_2)\n    \n    # Assertions to ensure functionality is the same\n    assert original_client_1.model == new_implementation_client_1.model\n    assert original_client_1.token == new_implementation_client_1.token\n\n    assert original_client_2.model == new_implementation_client_2.model\n    assert original_client_2.token == new_implementation_client_2.token\n\n    # Third test for boundary or unexpected case\n    model_name_3 = \"\"\n    original_client_3 = get_client(model_name_3)\n    new_implementation_client_3 = get_client_new_implementation(model_name_3)\n    \n    assert original_client_3.model == new_implementation_client_3.model\n    assert original_client_3.token == new_implementation_client_3.token\n\nif __name__ == \"__main__\":\n    test_get_client()\n"
    },
    {
        "func_name": "compute_perplexity",
        "idx": "88",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/training/train.py",
        "orig_func": "def compute_perplexity(model, data_loader, eval_tokens=100000):\n    \"\"\" \n    Compute the perplexity on the data in the data_loader over a specified number of tokens.\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in data_loader:\n            xb = batch[:, :-1].to(device)\n            yb = batch[:, 1:].to(device)\n            (logits, loss) = model(xb, yb)\n            num_tokens = yb.numel()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n            if total_tokens >= eval_tokens:\n                break\n    mean_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(mean_loss)).item()\n    model.train()\n    return perplexity",
        "orig_context": "```python\n## mst/training/train.py\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef compute_perplexity(model, data_loader, eval_tokens=100000):\n    \"\"\" \n    Compute the perplexity on the data in the data_loader over a specified number of tokens.\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in data_loader:\n            xb = batch[:, :-1].to(device)\n            yb = batch[:, 1:].to(device)\n            logits, loss = model(xb, yb)\n            \n            num_tokens = yb.numel()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n            \n            if total_tokens >= eval_tokens:\n                break\n\n    mean_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(mean_loss)).item()\n\n    model.train()\n    return perplexity\n\n```\n\n\n",
        "eval_script": "## mst/training/train.py\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef compute_perplexity(model, data_loader, eval_tokens=100000):\n    \"\"\" \n    Compute the perplexity on the data in the data_loader over a specified number of tokens.\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in data_loader:\n            xb = batch[:, :-1].to(device)\n            yb = batch[:, 1:].to(device)\n            logits, loss = model(xb, yb)\n            \n            num_tokens = yb.numel()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n            \n            if total_tokens >= eval_tokens:\n                break\n\n    mean_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(mean_loss)).item()\n\n    model.train()\n    return perplexity\n\n\n\ndef test_compute_perplexity():\n    # Mock Model and DataLoader for testing purposes\n    class MockModel:\n        def eval(self):\n            pass\n        def train(self):\n            pass\n        def __call__(self, xb, yb):\n            loss = torch.tensor(2.0)  # assumed constant loss for testing simplicity\n            return None, loss\n\n    class MockDataLoader:\n        def __iter__(self):\n            # assume each batch has the same constant data\n            batch_size = 10\n            seq_length = 5\n            batch = torch.ones((batch_size, seq_length))\n            return iter([batch, batch, batch])\n\n    model = MockModel()\n    data_loader = MockDataLoader()\n\n    perplexity_old = compute_perplexity(model, data_loader)\n    perplexity_new = compute_perplexity_new_implementation(model, data_loader)\n\n    assert perplexity_old == perplexity_new, \"Mismatch in perplexity calculation (Test Case 1)\"\n    \n    # Adjusting number of evaluation tokens\n    perplexity_old = compute_perplexity(model, data_loader, eval_tokens=30)\n    perplexity_new = compute_perplexity_new_implementation(model, data_loader, eval_tokens=30)\n    \n    assert perplexity_old == perplexity_new, \"Mismatch in perplexity calculation (Test Case 2)\"\n\n    # Different assumed batch structure\n    class MockDataLoader2:\n        def __iter__(self):\n            # varying batch sizes\n            batch1 = torch.ones((20, 5))\n            batch2 = torch.ones((5, 5))\n            return iter([batch1, batch2])\n\n    data_loader2 = MockDataLoader2()\n\n    perplexity_old = compute_perplexity(model, data_loader2)\n    perplexity_new = compute_perplexity_new_implementation(model, data_loader2)\n\n    assert perplexity_old == perplexity_new, \"Mismatch in perplexity calculation (Test Case 3)\"\n\ndef __main__():\n    test_compute_perplexity()\n\nif __name__ == \"__main__\":\n    __main__()"
    },
    {
        "func_name": "get_tokenizer",
        "idx": "90",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/data/data_loader.py",
        "orig_func": "def get_tokenizer(tokenizer_name: str) -> Any:\n    \"\"\"\n    Initialize and return a tokenizer based on the tokenizer name.\n    \n    Args:\n        tokenizer_name (str): Name of the tokenizer.\n    \n    Returns:\n        Any: The tokenizer object.\n    \"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    except Exception as e:\n        raise ValueError(f\"Unknown tokenizer '{tokenizer_name}': {e}\")\n    return tokenizer",
        "orig_context": "```python\n## mst/data/data_loader.py\nfrom typing import Any, List\n\nfrom transformers import AutoTokenizer\n\ndef get_tokenizer( tokenizer_name: str ) -> Any:\n    \"\"\"\n    Initialize and return a tokenizer based on the tokenizer name.\n    \n    Args:\n        tokenizer_name (str): Name of the tokenizer.\n    \n    Returns:\n        Any: The tokenizer object.\n    \"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    except Exception as e:\n        raise ValueError(f\"Unknown tokenizer '{tokenizer_name}': {e}\")\n    return tokenizer\n\n```\n\n\n",
        "eval_script": "# mst/data/data_loader.py\nfrom typing import Any\n\nfrom transformers import AutoTokenizer\n\ndef get_tokenizer(tokenizer_name: str) -> Any:\n    \"\"\"\n    Initialize and return a tokenizer based on the tokenizer name.\n    \n    Args:\n        tokenizer_name (str): Name of the tokenizer.\n    \n    Returns:\n        Any: The tokenizer object.\n    \"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    except Exception as e:\n        raise ValueError(f\"Unknown tokenizer '{tokenizer_name}': {e}\")\n    return tokenizer\n\n# Assuming the existence of the new implementation\n\n\ndef test_get_tokenizer():\n    tokenizer_name_1 = \"bert-base-uncased\"\n    tokenizer_1_a = get_tokenizer(tokenizer_name_1)\n    tokenizer_1_b = get_tokenizer_new_implementation(tokenizer_name_1)\n\n    tokenizer_name_2 = \"gpt2\"\n    tokenizer_2_a = get_tokenizer(tokenizer_name_2)\n    tokenizer_2_b = get_tokenizer_new_implementation(tokenizer_name_2)\n\n    tokenizer_name_3 = \"roberta-base\"\n    tokenizer_3_a = get_tokenizer(tokenizer_name_3)\n    tokenizer_3_b = get_tokenizer_new_implementation(tokenizer_name_3)\n\n    assert tokenizer_1_a.pad_token == tokenizer_1_b.pad_token, \"Pad token mismatch for bert-base-uncased\"\n    assert tokenizer_2_a.pad_token == tokenizer_2_b.pad_token, \"Pad token mismatch for gpt2\"\n    assert tokenizer_3_a.pad_token == tokenizer_3_b.pad_token, \"Pad token mismatch for roberta-base\"\n\n    sample_text = \"Hello, world!\"\n    assert tokenizer_1_a.encode(sample_text) == tokenizer_1_b.encode(sample_text), \"Encoding mismatch for bert-base-uncased\"\n    assert tokenizer_2_a.encode(sample_text) == tokenizer_2_b.encode(sample_text), \"Encoding mismatch for gpt2\"\n    assert tokenizer_3_a.encode(sample_text) == tokenizer_3_b.encode(sample_text), \"Encoding mismatch for roberta-base\"\n\nif __name__ == \"__main__\":\n    test_get_tokenizer()"
    },
    {
        "func_name": "get_dataloader",
        "idx": "91",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/data/data_loader.py",
        "orig_func": "def get_dataloader(dataset_name: str, tokenizer_name: str, max_seq_length: int, batch_size: int) -> DataLoader:\n    \"\"\"\n    Get a DataLoader for the specified dataset and tokenizer.\n    \n    Args:\n        dataset_name (str):   Name of the dataset.\n        tokenizer_name (str): Name of the tokenizer.\n        max_seq_length (int): Maximum sequence length for tokenization.\n        batch_size (int):     Batch size.\n    \n    Returns:\n        DataLoader:           DataLoader for the dataset.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer_name)\n    dataset = get_dataset(dataset_name, tokenizer, max_seq_length)\n    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda batch: collate_fn(batch, tokenizer))\n    return dataloader",
        "orig_context": "```python\n## mst/data/open_web_text.py\nimport torch\n\nfrom torch.utils.data import IterableDataset, DataLoader\n\nfrom datasets import load_dataset\n\nfrom typing import Any, Iterator\n\nclass OpenWebTextDataset( IterableDataset ):\n    \"\"\"\n    Iterable Dataset for the OpenWebText dataset, used in the RoBERTa model.\n    This dataset streams data without downloading the entire dataset.\n\n    Args:\n        tokenizer (Any):               Tokenizer to tokenize the text data.\n        max_seq_length (int):          Maximum sequence length for tokenization.\n    \"\"\"\n    def __init__( self, tokenizer: Any, max_seq_length: int ) -> None:\n        super(OpenWebTextDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __iter__( self ) -> Iterator[torch.Tensor]:\n        \"\"\"\n        Iterate over the dataset and yield tokenized samples.\n\n        Yields:\n            torch.Tensor: Tokenized input_ids tensor.\n        \"\"\"\n        # Load the dataset in streaming mode\n        dataset = load_dataset('openwebtext', split='train', streaming=True, trust_remote_code=True)\n\n        for sample in dataset:\n            text = sample['text']\n            # Tokenize the text\n            tokens = self.tokenizer(\n                text, \n                truncation=True, \n                max_length=self.max_seq_length, \n                return_tensors='pt'\n            )\n            # Yield the input_ids tensor\n            yield tokens['input_ids'].squeeze(0)\n\n```\n\n\n```python\n## mst/data/data_loader.py\nimport torch\n\nfrom torch.utils.data import DataLoader\n\nfrom typing import Any, List\n\nimport torch.nn.utils.rnn as rnn_utils\n\nfrom transformers import AutoTokenizer\n\nfrom .open_web_text import OpenWebTextDataset\n\ndef get_tokenizer( tokenizer_name: str ) -> Any:\n    \"\"\"\n    Initialize and return a tokenizer based on the tokenizer name.\n    \n    Args:\n        tokenizer_name (str): Name of the tokenizer.\n    \n    Returns:\n        Any: The tokenizer object.\n    \"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    except Exception as e:\n        raise ValueError(f\"Unknown tokenizer '{tokenizer_name}': {e}\")\n    return tokenizer\n\ndef get_dataset( dataset_name: str, tokenizer: Any, max_seq_length: int ) -> torch.utils.data.Dataset:\n    \"\"\"\n    Initialize and return a dataset based on the dataset name.\n    \n    Args:\n        dataset_name (str):     Name of the dataset.\n        tokenizer (Any):        Tokenizer to use.\n        max_seq_length (int):   Maximum sequence length.\n    \n    Returns:\n        torch.utils.data.Dataset: The dataset object.\n    \"\"\"\n    if dataset_name.lower() == 'openwebtext':\n        return OpenWebTextDataset(tokenizer=tokenizer, max_seq_length=max_seq_length)\n    else:\n        raise ValueError(f\"Unknown dataset '{dataset_name}'\")\n\ndef collate_fn( batch: List[torch.Tensor], tokenizer: Any ) -> torch.Tensor:\n    \"\"\"\n    Collate function to pad variable length sequences.\n\n    Args:\n        batch (List[torch.Tensor]): List of tokenized input_ids tensors.\n\n    Returns:\n        torch.Tensor: Padded input_ids tensor.\n    \"\"\"\n    return rnn_utils.pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n\ndef get_dataloader( \n    dataset_name: str, \n    tokenizer_name: str, \n    max_seq_length: int, \n    batch_size: int \n) -> DataLoader:\n    \"\"\"\n    Get a DataLoader for the specified dataset and tokenizer.\n    \n    Args:\n        dataset_name (str):   Name of the dataset.\n        tokenizer_name (str): Name of the tokenizer.\n        max_seq_length (int): Maximum sequence length for tokenization.\n        batch_size (int):     Batch size.\n    \n    Returns:\n        DataLoader:           DataLoader for the dataset.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer_name)\n    dataset = get_dataset(dataset_name, tokenizer, max_seq_length)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        collate_fn=lambda batch: collate_fn(batch, tokenizer)\n    )\n    return dataloader\n\n```\n\n\n",
        "eval_script": "import torch\nfrom torch.utils.data import DataLoader, IterableDataset\nimport torch.nn.utils.rnn as rnn_utils\nfrom transformers import AutoTokenizer\n# from datasets import load_dataset  # Comment out this line\n\nfrom typing import Any, List, Iterator\n\n# Mocking the load_dataset due to environment library issue\ndef mock_load_dataset(*args, **kwargs):\n    return [{'text': \"This is a sample text for testing purposes.\"}] * 100\n\nclass OpenWebTextDataset(IterableDataset):\n    def __init__(self, tokenizer: Any, max_seq_length: int) -> None:\n        super(OpenWebTextDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __iter__(self) -> Iterator[torch.Tensor]:\n        # dataset = load_dataset('openwebtext', split='train', streaming=True, trust_remote_code=True)\n        dataset = mock_load_dataset()  # Use the mock load_dataset\n\n        for sample in dataset:\n            text = sample['text']\n            tokens = self.tokenizer(\n                text, \n                truncation=True, \n                max_length=self.max_seq_length, \n                return_tensors='pt'\n            )\n            yield tokens['input_ids'].squeeze(0)\n\ndef get_tokenizer(tokenizer_name: str) -> Any:\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    except Exception as e:\n        raise ValueError(f\"Unknown tokenizer '{tokenizer_name}': {e}\")\n    return tokenizer\n\ndef get_dataset(dataset_name: str, tokenizer: Any, max_seq_length: int) -> torch.utils.data.Dataset:\n    if dataset_name.lower() == 'openwebtext':\n        return OpenWebTextDataset(tokenizer=tokenizer, max_seq_length=max_seq_length)\n    else:\n        raise ValueError(f\"Unknown dataset '{dataset_name}'\")\n\ndef collate_fn(batch: List[torch.Tensor], tokenizer: Any) -> torch.Tensor:\n    return rnn_utils.pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n\ndef get_dataloader(dataset_name: str, tokenizer_name: str, max_seq_length: int, batch_size: int) -> DataLoader:\n    tokenizer = get_tokenizer(tokenizer_name)\n    dataset = get_dataset(dataset_name, tokenizer, max_seq_length)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        collate_fn=lambda batch: collate_fn(batch, tokenizer)\n    )\n    return dataloader\n\n\ndef test_get_dataloader():\n    dataset_name = \"openwebtext\"\n    tokenizer_name = \"bert-base-uncased\"\n    max_seq_length = 128\n    batch_size = 8\n\n    dataloader_original = get_dataloader(dataset_name, tokenizer_name, max_seq_length, batch_size)\n    dataloader_new = get_dataloader_new_implementation(dataset_name, tokenizer_name, max_seq_length, batch_size)\n\n    # Assert the batch size is the same\n    assert dataloader_original.batch_size == dataloader_new.batch_size, \"Batch sizes do not match.\"\n\n    # Assert the tokenizer pad_token_id is used consistently\n    original_tokenizer = get_tokenizer(tokenizer_name)\n    assert original_tokenizer.pad_token_id == get_tokenizer(tokenizer_name).pad_token_id, \"Pad token IDs do not match.\"\n\n    # Assert that both DataLoaders can iterate over the same length properly\n    num_batches = 2\n    original_batches = list(next(iter(dataloader_original)) for _ in range(num_batches))\n    new_batches = list(next(iter(dataloader_new)) for _ in range(num_batches))\n    assert len(original_batches) == len(new_batches), \"Number of batches do not match in iterable DataLoader.\"\n\nif __name__ == \"__main__\":\n    test_get_dataloader()"
    },
    {
        "func_name": "get_dataloader",
        "idx": "92",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/data/open_web_text.py",
        "orig_func": "def get_dataloader(tokenizer: Any, max_seq_length: int, batch_size: int, collate_fn: callable) -> DataLoader:\n    \"\"\"\n    Get a DataLoader for the OpenWebText dataset.\n\n    Args:\n        tokenizer (Any):           Tokenizer to tokenize the text data.\n        max_seq_length (int):      Maximum sequence length for tokenization.\n        batch_size (int):          Batch size.\n\n    Returns:\n        DataLoader:                DataLoader for the dataset.\n    \"\"\"\n    dataset = OpenWebTextDataset(tokenizer=tokenizer, max_seq_length=max_seq_length)\n    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n    return dataloader",
        "orig_context": "```python\n## mst/data/open_web_text.py\nimport torch\n\nfrom torch.utils.data import IterableDataset, DataLoader\n\nfrom datasets import load_dataset\n\nfrom typing import Any, Iterator\n\nclass OpenWebTextDataset( IterableDataset ):\n    \"\"\"\n    Iterable Dataset for the OpenWebText dataset, used in the RoBERTa model.\n    This dataset streams data without downloading the entire dataset.\n\n    Args:\n        tokenizer (Any):               Tokenizer to tokenize the text data.\n        max_seq_length (int):          Maximum sequence length for tokenization.\n    \"\"\"\n    def __init__( self, tokenizer: Any, max_seq_length: int ) -> None:\n        super(OpenWebTextDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __iter__( self ) -> Iterator[torch.Tensor]:\n        \"\"\"\n        Iterate over the dataset and yield tokenized samples.\n\n        Yields:\n            torch.Tensor: Tokenized input_ids tensor.\n        \"\"\"\n        # Load the dataset in streaming mode\n        dataset = load_dataset('openwebtext', split='train', streaming=True, trust_remote_code=True)\n\n        for sample in dataset:\n            text = sample['text']\n            # Tokenize the text\n            tokens = self.tokenizer(\n                text, \n                truncation=True, \n                max_length=self.max_seq_length, \n                return_tensors='pt'\n            )\n            # Yield the input_ids tensor\n            yield tokens['input_ids'].squeeze(0)\n\ndef get_dataloader( tokenizer: Any, max_seq_length: int, batch_size: int, collate_fn: callable ) -> DataLoader:\n    \"\"\"\n    Get a DataLoader for the OpenWebText dataset.\n\n    Args:\n        tokenizer (Any):           Tokenizer to tokenize the text data.\n        max_seq_length (int):      Maximum sequence length for tokenization.\n        batch_size (int):          Batch size.\n\n    Returns:\n        DataLoader:                DataLoader for the dataset.\n    \"\"\"\n    dataset = OpenWebTextDataset(tokenizer=tokenizer, max_seq_length=max_seq_length)\n    dataloader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        collate_fn=collate_fn\n    )\n    return dataloader\n\n```\n\n\n",
        "eval_script": "import torch\nfrom torch.utils.data import IterableDataset, DataLoader\nfrom typing import Any, Iterator\n\nclass MockDataset:\n    def __iter__(self):\n        return iter([{'text': 'sample text'}])\n\ndef load_dataset(name, split, streaming, trust_remote_code):\n    if name == 'openwebtext' and split == 'train' and streaming and trust_remote_code:\n        return MockDataset()\n\nclass OpenWebTextDataset(IterableDataset):\n    def __init__(self, tokenizer: Any, max_seq_length: int) -> None:\n        super(OpenWebTextDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __iter__(self) -> Iterator[torch.Tensor]:\n        dataset = load_dataset('openwebtext', split='train', streaming=True, trust_remote_code=True)\n        for sample in dataset:\n            text = sample['text']\n            tokens = self.tokenizer(\n                text, \n                truncation=True, \n                max_length=self.max_seq_length, \n                return_tensors='pt'\n            )\n            yield tokens['input_ids'].squeeze(0)\n\ndef get_dataloader(tokenizer: Any, max_seq_length: int, batch_size: int, collate_fn: callable) -> DataLoader:\n    dataset = OpenWebTextDataset(tokenizer=tokenizer, max_seq_length=max_seq_length)\n    dataloader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        collate_fn=collate_fn\n    )\n    return dataloader\n\n\ndef test_get_dataloader():\n    class MockTokenizer:\n        def __call__(self, text: str, truncation: bool, max_length: int, return_tensors: str):\n            # This mock tokenizer returns a fake tokenized object\n            return {'input_ids': torch.tensor([1, 2, 3])}\n        \n    tokenizer = MockTokenizer()\n    max_seq_length = 10\n    batch_size = 2\n    collate_fn = lambda x: x  # Simplest collate function that does nothing\n\n    dataloader_v1 = get_dataloader(tokenizer, max_seq_length, batch_size, collate_fn)\n    dataloader_v2 = get_dataloader_new_implementation(tokenizer, max_seq_length, batch_size, collate_fn)\n\n    batches_v1 = list(dataloader_v1)\n    batches_v2 = list(dataloader_v2)\n\n    assert len(batches_v1) == len(batches_v2), \"Dataloader lengths are different.\"\n    assert all(len(batch) == len(batches_v2[i]) for i, batch in enumerate(batches_v1)), \"Batch sizes are different.\"\n    assert all(torch.equal(batch[0], batches_v2[i][0]) for i, batch in enumerate(batches_v1)), \"Batch contents differ.\"\n\nif __name__ == \"__main__\":\n    test_get_dataloader()"
    },
    {
        "func_name": "PositionalEncoding.forward",
        "idx": "94",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/architecture/pos_encoding.py",
        "orig_func": "def forward(self, x: Tensor) -> Tensor:\n    \"\"\"\n        Args:\n            x (Tensor):     Input tensor of shape (batch_size, seq_length, d_model).\n        \n        Returns:\n            Tensor:         The input tensor with positional embeddings added.\n        \"\"\"\n    (B, T, C) = x.size()\n    pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n    pe = self.pe(pos)\n    x = x + pe\n    x = self.do(x)\n    return x",
        "orig_context": "```python\n## mst/architecture/pos_encoding.py\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nclass PositionalEncoding( nn.Module ):\n    \"\"\"\n    Absolute positional encoding module using learnable embeddings.\n    \"\"\"\n    def __init__( self, d_model: int, dropout: float = 0.1, max_length: int = 5000 ) -> None:\n        \"\"\"\n        Args:\n            d_model (int):                  Dimension of embeddings.\n            dropout (float, optional):      Dropout rate, defaults to 0.1.\n            max_length (int, optional):     Max sequence length, defaults to 5000.\n        \"\"\"\n        super().__init__()\n        self.do = nn.Dropout(p=dropout)\n        self.pe = nn.Embedding(max_length, d_model)\n\n    def forward( self, x: Tensor ) -> Tensor:\n        \"\"\"\n        Args:\n            x (Tensor):     Input tensor of shape (batch_size, seq_length, d_model).\n        \n        Returns:\n            Tensor:         The input tensor with positional embeddings added.\n        \"\"\"\n        B, T, C = x.size() # (batch_size, seq_length, d_model)\n        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T) # (0, 1, ..., seq_length-1) for each batch\n        pe = self.pe(pos)\n        x = x + pe\n        x = self.do(x)\n        return x\n\n```\n\n\n",
        "eval_script": "import torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Absolute positional encoding module using learnable embeddings.\n    \"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000) -> None:\n        \"\"\"\n        Args:\n            d_model (int):                  Dimension of embeddings.\n            dropout (float, optional):      Dropout rate, defaults to 0.1.\n            max_length (int, optional):     Max sequence length, defaults to 5000.\n        \"\"\"\n        super().__init__()\n        self.do = nn.Dropout(p=dropout)\n        self.pe = nn.Embedding(max_length, d_model)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x (Tensor):     Input tensor of shape (batch_size, seq_length, d_model).\n        \n        Returns:\n            Tensor:         The input tensor with positional embeddings added.\n        \"\"\"\n        B, T, C = x.size() # (batch_size, seq_length, d_model)\n        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T) # (0, 1, ..., seq_length-1) for each batch\n        torch.manual_seed(42)  # Reinitialize seed for pe computation consistency\n        pe = self.pe(pos)\n        x = x + pe\n        x = self.do(x)\n        return x\n\n\ndef test_forward():\n    \"\"\"\n    Test the functionality of PositionalEncoding.forward and PositionalEncoding.forward_new_implementation\n    to ensure they produce the same results.\n    \"\"\"\n    torch.manual_seed(42)  # Ensure consistency across test executions\n    d_model = 4\n    dropout = 0.1\n    max_length = 100\n\n    pos_encoding = PositionalEncoding(d_model, dropout, max_length)\n\n    # Test case 1: Small tensor\n    x1 = torch.randn(2, 10, d_model)\n    assert torch.allclose(pos_encoding.forward(x1), pos_encoding.forward_new_implementation(x1)), \"Failed on test case 1\"\n\n    # Test case 2: Medium tensor\n    x2 = torch.randn(5, 20, d_model)\n    assert torch.allclose(pos_encoding.forward(x2), pos_encoding.forward_new_implementation(x2)), \"Failed on test case 2\"\n\n    # Test case 3: Edge case with larger sequence length\n    x3 = torch.randn(3, 50, d_model)\n    assert torch.allclose(pos_encoding.forward(x3), pos_encoding.forward_new_implementation(x3)), \"Failed on test case 3\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "TransformerBlock.forward",
        "idx": "95",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/architecture/transformer_block.py",
        "orig_func": "def forward(self, x: Tensor) -> Tensor:\n    \"\"\"\n        Forward pass through the transformer block. Results from mlp and multi-head attention\n        are added to the input tensor to create residual connections.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            Tensor: Output tensor.\n        \"\"\"\n    x = x + self.attn(self.ln1(x))\n    x = x + self.do(self.mlp(self.ln2(x)))\n    return x",
        "orig_context": "```python\n## mst/architecture/multi_head_attention.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nclass MultiHeadAttention( nn.Module ):\n    \"\"\"\n    Multi-head attention mechanism.\n    \n    Args:\n        d_model (int):                  Dimensionality of the model (Embedding dimension).\n        num_heads (int):                Number of attention heads.\n        context_window (int):           Size of the context window. Defaults to d_model.\n        masked (bool):                  Whether to use causal masked attention or not.\n    \"\"\"\n    def __init__( \n            self, \n            d_model: int, \n            num_heads: int, \n            context_window: int=None, \n            masked: bool=False,\n        ) -> None:\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.nh         = num_heads\n        self.d_k        = d_model // self.nh # Embedding dimension for each head\n        self.d_model    = d_model\n        self.ctx_d      = context_window if context_window is not None else d_model\n        self.masked     = masked\n\n        self.attn = nn.Linear(d_model, 3 * d_model) \n        self.W_o = nn.Linear(d_model, d_model)\n        self.register_buffer(\n            \"bias\", \n            torch.tril(torch.ones(self.ctx_d, self.ctx_d)).view(1, 1, self.ctx_d, self.ctx_d) # (1, 1, ctx_d, ctx_d) - Causal mask\n        )\n        \n    def forward( self, x: Tensor ) -> Tensor:\n        B, T, C = x.size() # (batch_size, seq_length, d_model)\n\n        # Calculate queries, keys and values for all heads in a batch, and move head dimension to the front\n        Q, K, V = self.attn(x).split(self.d_model, dim=2)   # (B, T, 3*d_model) -> 3 * (B, T, d_model)\n        Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        K = K.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        V = V.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n\n        # Scaled dot-product attention\n        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if self.masked:\n            attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        \n        self.attn_weights = attn\n        y = attn @ V\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # Output projection\n        y = self.W_o(y)\n        return y\n\n```\n\n\n```python\n## mst/architecture/transformer_block.py\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom .multi_head_attention import MultiHeadAttention\n\nclass TransformerBlock( nn.Module ):\n    \"\"\"\n    Transformer block consisting of: \n        1. Multi-Head Attention\n        2. Add & Norm\n        3. Feed Forward (Multi-Layer Perceptron)\n        4. Add & Norm\n    \"\"\"\n    def __init__( \n            self,\n            num_heads:  int,\n            d_model:    int,\n            hidden_dim: int,\n            masked:     bool=False,\n            dropout:    float=0.1, \n        ) -> None:\n        \"\"\"\n        Args:\n            num_heads (int):                Number of attention heads.\n            d_model (int):                  Dimensionality of the model (Embedding dimension).\n            hidden_dim (int):               Hidden layer dimensionality (MLP).\n            masked (bool, optional):        Whether to use causal masked attention or not. Defaults to False.\n            dropout (float, optional):      Dropout rate. Defaults to 0.1.\n        \"\"\"\n        super(TransformerBlock, self).__init__()\n        self.attn   = MultiHeadAttention(d_model, num_heads, masked=masked)\n        self.ln1    = nn.LayerNorm(d_model)\n        self.mlp    = nn.Sequential(\n            nn.Linear(d_model, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, d_model)\n        )\n        self.ln2    = nn.LayerNorm(d_model)\n        self.do     = nn.Dropout(dropout)\n    \n    def forward( self, x: Tensor ) -> Tensor:\n        \"\"\"\n        Forward pass through the transformer block. Results from mlp and multi-head attention\n        are added to the input tensor to create residual connections.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            Tensor: Output tensor.\n        \"\"\"\n        x = x + self.attn(self.ln1(x))\n        x = x + self.do(self.mlp(self.ln2(x)))\n        return x\n\n```\n\n\n",
        "eval_script": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nclass MultiHeadAttention(nn.Module):\n    def __init__( \n            self, \n            d_model: int, \n            num_heads: int, \n            context_window: int=None, \n            masked: bool=False,\n        ) -> None:\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.nh         = num_heads\n        self.d_k        = d_model // self.nh\n        self.d_model    = d_model\n        self.ctx_d      = context_window if context_window is not None else d_model\n        self.masked     = masked\n\n        self.attn = nn.Linear(d_model, 3 * d_model) \n        self.W_o = nn.Linear(d_model, d_model)\n        self.register_buffer(\n            \"bias\", \n            torch.tril(torch.ones(self.ctx_d, self.ctx_d)).view(1, 1, self.ctx_d, self.ctx_d)\n        )\n        \n    def forward(self, x: Tensor) -> Tensor:\n        B, T, C = x.size()\n\n        Q, K, V = self.attn(x).split(self.d_model, dim=2)\n        Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2)\n        K = K.view(B, T, self.nh, self.d_k).transpose(1, 2)\n        V = V.view(B, T, self.nh, self.d_k).transpose(1, 2)\n\n        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if self.masked:\n            attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        \n        self.attn_weights = attn\n        y = attn @ V\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        y = self.W_o(y)\n        return y\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n            self,\n            num_heads:  int,\n            d_model:    int,\n            hidden_dim: int,\n            masked:     bool=False,\n            dropout:    float=0.1, \n        ) -> None:\n        super(TransformerBlock, self).__init__()\n        self.attn   = MultiHeadAttention(d_model, num_heads, masked=masked)\n        self.ln1    = nn.LayerNorm(d_model)\n        self.mlp    = nn.Sequential(\n            nn.Linear(d_model, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, d_model)\n        )\n        self.ln2    = nn.LayerNorm(d_model)\n        self.do     = nn.Dropout(dropout)\n    \n    def forward(self, x: Tensor) -> Tensor:\n        x = x + self.attn(self.ln1(x))\n        x = x + self.do(self.mlp(self.ln2(x)))\n        return x\n\n\n    \n\ndef test_forward():\n    d_model = 512\n    num_heads = 8\n    hidden_dim = 2048\n    batch_size = 2\n    seq_length = 10\n    \n    torch.manual_seed(0)  # Ensure reproducibility\n    block = TransformerBlock(num_heads, d_model, hidden_dim)\n    block.eval()  # Set model to evaluation mode\n    \n    # Test case 1\n    x1 = torch.rand(batch_size, seq_length, d_model)\n    assert torch.allclose(block.forward(x1), block.forward_new_implementation(x1)), \"Test case 1 failed!\"\n    \n    # Test case 2\n    x2 = torch.rand(batch_size, seq_length, d_model)\n    assert torch.allclose(block.forward(x2), block.forward_new_implementation(x2)), \"Test case 2 failed!\"\n    \n    # Test case 3\n    x3 = torch.rand(batch_size, seq_length, d_model)\n    assert torch.allclose(block.forward(x3), block.forward_new_implementation(x3)), \"Test case 3 failed!\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "Transformer.forward",
        "idx": "96",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/architecture/transformer.py",
        "orig_func": "def forward(self, x: Tensor, targets: Tensor=None) -> Tensor:\n    \"\"\"\n        Forward pass through the transformer.\n\n        Args:\n            x (Tensor):                     Input tensor. (batch_size, seq_length)\n            targets (Tensor, optional):     Target tensor. Defaults to None.\n\n        Returns:\n            x (Tensor):                     Decoded representation. (batch_size, seq_length, d_model)\n            loss (Tensor):                  Loss value. Defaults to None.\n            \n        \"\"\"\n    x = self.emb(x)\n    x = self.pos_enc(x)\n    x = self.blocks(x)\n    x = self.ln_f(x)\n    logits = self.lm_head(x)\n    loss = None\n    if targets is not None:\n        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n    return (x, loss)",
        "orig_context": "```python\n## mst/architecture/pos_encoding.py\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nclass PositionalEncoding( nn.Module ):\n    \"\"\"\n    Absolute positional encoding module using learnable embeddings.\n    \"\"\"\n    def __init__( self, d_model: int, dropout: float = 0.1, max_length: int = 5000 ) -> None:\n        \"\"\"\n        Args:\n            d_model (int):                  Dimension of embeddings.\n            dropout (float, optional):      Dropout rate, defaults to 0.1.\n            max_length (int, optional):     Max sequence length, defaults to 5000.\n        \"\"\"\n        super().__init__()\n        self.do = nn.Dropout(p=dropout)\n        self.pe = nn.Embedding(max_length, d_model)\n\n    def forward( self, x: Tensor ) -> Tensor:\n        \"\"\"\n        Args:\n            x (Tensor):     Input tensor of shape (batch_size, seq_length, d_model).\n        \n        Returns:\n            Tensor:         The input tensor with positional embeddings added.\n        \"\"\"\n        B, T, C = x.size() # (batch_size, seq_length, d_model)\n        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T) # (0, 1, ..., seq_length-1) for each batch\n        pe = self.pe(pos)\n        x = x + pe\n        x = self.do(x)\n        return x\n\n```\n\n\n```python\n## mst/architecture/multi_head_attention.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nclass MultiHeadAttention( nn.Module ):\n    \"\"\"\n    Multi-head attention mechanism.\n    \n    Args:\n        d_model (int):                  Dimensionality of the model (Embedding dimension).\n        num_heads (int):                Number of attention heads.\n        context_window (int):           Size of the context window. Defaults to d_model.\n        masked (bool):                  Whether to use causal masked attention or not.\n    \"\"\"\n    def __init__( \n            self, \n            d_model: int, \n            num_heads: int, \n            context_window: int=None, \n            masked: bool=False,\n        ) -> None:\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.nh         = num_heads\n        self.d_k        = d_model // self.nh # Embedding dimension for each head\n        self.d_model    = d_model\n        self.ctx_d      = context_window if context_window is not None else d_model\n        self.masked     = masked\n\n        self.attn = nn.Linear(d_model, 3 * d_model) \n        self.W_o = nn.Linear(d_model, d_model)\n        self.register_buffer(\n            \"bias\", \n            torch.tril(torch.ones(self.ctx_d, self.ctx_d)).view(1, 1, self.ctx_d, self.ctx_d) # (1, 1, ctx_d, ctx_d) - Causal mask\n        )\n        \n    def forward( self, x: Tensor ) -> Tensor:\n        B, T, C = x.size() # (batch_size, seq_length, d_model)\n\n        # Calculate queries, keys and values for all heads in a batch, and move head dimension to the front\n        Q, K, V = self.attn(x).split(self.d_model, dim=2)   # (B, T, 3*d_model) -> 3 * (B, T, d_model)\n        Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        K = K.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        V = V.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n\n        # Scaled dot-product attention\n        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if self.masked:\n            attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        \n        self.attn_weights = attn\n        y = attn @ V\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # Output projection\n        y = self.W_o(y)\n        return y\n\n```\n\n\n```python\n## mst/architecture/transformer_block.py\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom .multi_head_attention import MultiHeadAttention\n\nclass TransformerBlock( nn.Module ):\n    \"\"\"\n    Transformer block consisting of: \n        1. Multi-Head Attention\n        2. Add & Norm\n        3. Feed Forward (Multi-Layer Perceptron)\n        4. Add & Norm\n    \"\"\"\n    def __init__( \n            self,\n            num_heads:  int,\n            d_model:    int,\n            hidden_dim: int,\n            masked:     bool=False,\n            dropout:    float=0.1, \n        ) -> None:\n        \"\"\"\n        Args:\n            num_heads (int):                Number of attention heads.\n            d_model (int):                  Dimensionality of the model (Embedding dimension).\n            hidden_dim (int):               Hidden layer dimensionality (MLP).\n            masked (bool, optional):        Whether to use causal masked attention or not. Defaults to False.\n            dropout (float, optional):      Dropout rate. Defaults to 0.1.\n        \"\"\"\n        super(TransformerBlock, self).__init__()\n        self.attn   = MultiHeadAttention(d_model, num_heads, masked=masked)\n        self.ln1    = nn.LayerNorm(d_model)\n        self.mlp    = nn.Sequential(\n            nn.Linear(d_model, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, d_model)\n        )\n        self.ln2    = nn.LayerNorm(d_model)\n        self.do     = nn.Dropout(dropout)\n    \n    def forward( self, x: Tensor ) -> Tensor:\n        \"\"\"\n        Forward pass through the transformer block. Results from mlp and multi-head attention\n        are added to the input tensor to create residual connections.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            Tensor: Output tensor.\n        \"\"\"\n        x = x + self.attn(self.ln1(x))\n        x = x + self.do(self.mlp(self.ln2(x)))\n        return x\n\n```\n\n\n```python\n## mst/architecture/transformer.py\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom .pos_encoding import PositionalEncoding\n\nfrom .transformer_block import TransformerBlock\n\nclass Transformer( nn.Module ):\n    \"\"\"\n    Transformer model. Consists of:\n        1. Embedding layer\n        2. Positional encoding\n        3. Transformer blocks (masked)\n    \"\"\"\n    def __init__( \n            self, \n            vocab_size: int, \n            d_model: int, \n            num_heads: int, \n            hidden_dim: int, \n            num_blocks: int, \n            dropout: float=0.1,\n            echo_specs: bool=True\n        ) -> None:\n        \"\"\"\n        Args:\n            vocab_size (int):               Size of the vocabulary.\n            d_model (int):                  Dimensionality of the model (Embedding dimension).\n            num_heads (int):                Number of attention heads.\n            hidden_dim (int):               Hidden layer dimensionality (MLP).\n            num_blocks (int):               Number of transformer blocks.\n            dropout (float, optional):      Dropout rate. Defaults to 0.1.\n            echo_spechs (bool, optional):   Whether to print the model specifications. Defaults to True.\n        \"\"\"\n        super(Transformer, self).__init__()\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.num_blocks = num_blocks\n        self.dropout = dropout\n        \n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, dropout)\n        self.blocks = nn.Sequential(*[TransformerBlock(num_heads=num_heads, d_model=d_model, hidden_dim=hidden_dim, masked=True, dropout=dropout) for _ in range(num_blocks)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n        \n        if echo_specs: print(self)\n    \n    def __repr__( self ) -> str:\n        \"\"\"\n        Returns a string representation of the model specifications.\n        \"\"\"\n        # Calculate total trainable parameters\n        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        \n        # Build the string\n        model_str = f\"\\n\\rTransformer Model Specifications:\\n\"\n        model_str += f\"{'='*40}\\n\"\n        model_str += f\"Vocabulary Size:          {self.vocab_size}\\n\"\n        model_str += f\"Embedding Dimension:      {self.d_model}\\n\"\n        model_str += f\"Number of Heads:          {self.num_heads}\\n\"\n        model_str += f\"Number of Blocks:         {self.num_blocks}\\n\"\n        model_str += f\"Hidden Dimension (MLP):   {self.hidden_dim}\\n\"\n        model_str += f\"Dropout Rate:             {self.dropout}\\n\"\n        model_str += f\"Total Parameters:         {total_params}\\n\"\n        model_str += f\"{'='*40}\\n\"\n        model_str += f\"Trainable Parameters per Component:\\n\"\n\n        # Components and their parameter counts\n        components = [\n            ('Embedding Layer:    ', self.emb),\n            ('Positional Encoding:', self.pos_enc),\n            ('Linear Head:        ', self.lm_head),\n            ('Layer Norm:         ', self.ln_f),\n        ]\n\n        # Add Transformer Blocks\n        for i, block in enumerate(self.blocks):\n            components.append((f'Transformer Block {i+1}:', block))\n\n        # Calculate and append parameter counts for each component\n        for name, module in components:\n            num_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n            model_str += f\"  * {name} {num_params}\\n\"\n\n        model_str += f\"{'='*40}\\n\"\n        return model_str\n    \n    def forward( self, x: Tensor, targets: Tensor=None ) -> Tensor:\n        \"\"\"\n        Forward pass through the transformer.\n\n        Args:\n            x (Tensor):                     Input tensor. (batch_size, seq_length)\n            targets (Tensor, optional):     Target tensor. Defaults to None.\n\n        Returns:\n            x (Tensor):                     Decoded representation. (batch_size, seq_length, d_model)\n            loss (Tensor):                  Loss value. Defaults to None.\n            \n        \"\"\"\n        x = self.emb(x)\n        x = self.pos_enc(x)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            # targets: (batch_size, seq_length)\n            # logits:  (batch_size, seq_length, vocab_size)\n            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n\n        return x, loss\n\n```\n\n\n",
        "eval_script": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch import Tensor\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Absolute positional encoding module using learnable embeddings.\n    \"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000) -> None:\n        \"\"\"\n        Args:\n            d_model (int):                  Dimension of embeddings.\n            dropout (float, optional):      Dropout rate, defaults to 0.1.\n            max_length (int, optional):     Max sequence length, defaults to 5000.\n        \"\"\"\n        super().__init__()\n        self.do = nn.Dropout(p=dropout)\n        self.pe = nn.Embedding(max_length, d_model)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x (Tensor):     Input tensor of shape (batch_size, seq_length, d_model).\n\n        Returns:\n            Tensor:         The input tensor with positional embeddings added.\n        \"\"\"\n        B, T, C = x.size()  # (batch_size, seq_length, d_model)\n        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)  # (0, 1, ..., seq_length-1) for each batch\n        pe = self.pe(pos)\n        x = x + pe\n        x = self.do(x)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention mechanism.\n    \n    Args:\n        d_model (int):                  Dimensionality of the model (Embedding dimension).\n        num_heads (int):                Number of attention heads.\n        context_window (int):           Size of the context window. Defaults to d_model.\n        masked (bool):                  Whether to use causal masked attention or not.\n    \"\"\"\n    def __init__(self, d_model: int, num_heads: int, context_window: int=None, masked: bool=False,) -> None:\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.nh = num_heads\n        self.d_k = d_model // self.nh  # Embedding dimension for each head\n        self.d_model = d_model\n        self.ctx_d = context_window if context_window is not None else d_model\n        self.masked = masked\n\n        self.attn = nn.Linear(d_model, 3 * d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        self.register_buffer(\n            \"bias\",\n            torch.tril(torch.ones(self.ctx_d, self.ctx_d)).view(1, 1, self.ctx_d, self.ctx_d)  # (1, 1, ctx_d, ctx_d) - Causal mask\n        )\n        \n    def forward(self, x: Tensor) -> Tensor:\n        B, T, C = x.size()  # (batch_size, seq_length, d_model)\n\n        # Calculate queries, keys and values for all heads in a batch, and move head dimension to the front\n        Q, K, V = self.attn(x).split(self.d_model, dim=2)  # (B, T, 3*d_model) -> 3 * (B, T, d_model)\n        Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2)  # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        K = K.view(B, T, self.nh, self.d_k).transpose(1, 2)  # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        V = V.view(B, T, self.nh, self.d_k).transpose(1, 2)  # (B, nh, T, d_k)   -> (B, nh, T, d_k)\n\n        # Scaled dot-product attention\n        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if self.masked:\n            attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        \n        self.attn_weights = attn\n        y = attn @ V\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # Output projection\n        y = self.W_o(y)\n        return y\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    Transformer block consisting of: \n        1. Multi-Head Attention\n        2. Add & Norm\n        3. Feed Forward (Multi-Layer Perceptron)\n        4. Add & Norm\n    \"\"\"\n    def __init__(self, num_heads: int, d_model: int, hidden_dim: int, masked: bool=False, dropout: float=0.1,) -> None:\n        \"\"\"\n        Args:\n            num_heads (int):                Number of attention heads.\n            d_model (int):                  Dimensionality of the model (Embedding dimension).\n            hidden_dim (int):               Hidden layer dimensionality (MLP).\n            masked (bool, optional):        Whether to use causal masked attention or not. Defaults to False.\n            dropout (float, optional):      Dropout rate. Defaults to 0.1.\n        \"\"\"\n        super(TransformerBlock, self).__init__()\n        self.attn = MultiHeadAttention(d_model, num_heads, masked=masked)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, d_model)\n        )\n        self.ln2 = nn.LayerNorm(d_model)\n        self.do = nn.Dropout(dropout)\n    \n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Forward pass through the transformer block. Results from mlp and multi-head attention\n        are added to the input tensor to create residual connections.\n\n        Args:\n            x (Tensor): Input tensor.\n\n        Returns:\n            Tensor: Output tensor.\n        \"\"\"\n        x = x + self.attn(self.ln1(x))\n        x = x + self.do(self.mlp(self.ln2(x)))\n        return x\n\nclass Transformer(nn.Module):\n    \"\"\"\n    Transformer model. Consists of:\n        1. Embedding layer\n        2. Positional encoding\n        3. Transformer blocks (masked)\n    \"\"\"\n    def __init__(self, vocab_size: int, d_model: int, num_heads: int, hidden_dim: int, num_blocks: int, dropout: float=0.1, echo_specs: bool=True) -> None:\n        \"\"\"\n        Args:\n            vocab_size (int):               Size of the vocabulary.\n            d_model (int):                  Dimensionality of the model (Embedding dimension).\n            num_heads (int):                Number of attention heads.\n            hidden_dim (int):               Hidden layer dimensionality (MLP).\n            num_blocks (int):               Number of transformer blocks.\n            dropout (float, optional):      Dropout rate. Defaults to 0.1.\n            echo_specs (bool, optional):    Whether to print the model specifications. Defaults to True.\n        \"\"\"\n        super(Transformer, self).__init__()\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.num_blocks = num_blocks\n        self.dropout = dropout\n        \n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, dropout)\n        self.blocks = nn.Sequential(*[TransformerBlock(num_heads=num_heads, d_model=d_model, hidden_dim=hidden_dim, masked=True, dropout=dropout) for _ in range(num_blocks)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n        \n        if echo_specs: print(self)\n    \n    def __repr__(self) -> str:\n        \"\"\"\n        Returns a string representation of the model specifications.\n        \"\"\"\n        # Calculate total trainable parameters\n        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        \n        # Build the string\n        model_str = f\"\\n\\rTransformer Model Specifications:\\n\"\n        model_str += f\"{'='*40}\\n\"\n        model_str += f\"Vocabulary Size:          {self.vocab_size}\\n\"\n        model_str += f\"Embedding Dimension:      {self.d_model}\\n\"\n        model_str += f\"Number of Heads:          {self.num_heads}\\n\"\n        model_str += f\"Number of Blocks:         {self.num_blocks}\\n\"\n        model_str += f\"Hidden Dimension (MLP):   {self.hidden_dim}\\n\"\n        model_str += f\"Dropout Rate:             {self.dropout}\\n\"\n        model_str += f\"Total Parameters:         {total_params}\\n\"\n        model_str += f\"{'='*40}\\n\"\n        model_str += f\"Trainable Parameters per Component:\\n\"\n\n        # Components and their parameter counts\n        components = [\n            ('Embedding Layer:    ', self.emb),\n            ('Positional Encoding:', self.pos_enc),\n            ('Linear Head:        ', self.lm_head),\n            ('Layer Norm:         ', self.ln_f),\n        ]\n\n        # Add Transformer Blocks\n        for i, block in enumerate(self.blocks):\n            components.append((f'Transformer Block {i+1}:', block))\n\n        # Calculate and append parameter counts for each component\n        for name, module in components:\n            num_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n            model_str += f\"  * {name} {num_params}\\n\"\n\n        model_str += f\"{'='*40}\\n\"\n        return model_str\n    \n    def forward(self, x: Tensor, targets: Tensor=None) -> Tensor:\n        \"\"\"\n        Forward pass through the transformer.\n\n        Args:\n            x (Tensor):                     Input tensor. (batch_size, seq_length)\n            targets (Tensor, optional):     Target tensor. Defaults to None.\n\n        Returns:\n            x (Tensor):                     Decoded representation. (batch_size, seq_length, d_model)\n            loss (Tensor):                  Loss value. Defaults to None.\n            \n        \"\"\"\n        x = self.emb(x)\n        x = self.pos_enc(x)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            # targets: (batch_size, seq_length)\n            # logits:  (batch_size, seq_length, vocab_size)\n            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n\n        return x, loss\n\n\ndef test_forward():\n    vocab_size = 100\n    d_model = 64\n    num_heads = 4\n    hidden_dim = 256\n    num_blocks = 2\n    seq_length = 10\n    batch_size = 4\n\n    model = Transformer(vocab_size, d_model, num_heads, hidden_dim, num_blocks)\n\n    # Generate random input and targets\n    x = torch.randint(0, vocab_size, (batch_size, seq_length))\n    targets = torch.randint(0, vocab_size, (batch_size, seq_length))\n\n    # Set random seed before calling each forward method\n    seed = torch.initial_seed()\n    torch.manual_seed(seed)\n    out1, loss1 = model.forward(x, targets)\n    torch.manual_seed(seed)\n    out2, loss2 = model.forward_new_implementation(x, targets)\n\n    # Assertions to ensure equivalence\n    assert out1.shape == out2.shape, \"Output shapes do not match.\"\n    assert torch.allclose(out1, out2), \"Outputs do not match.\"\n    if targets is not None:\n        assert torch.allclose(loss1, loss2), \"Losses do not match.\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "MultiHeadAttention.forward",
        "idx": "97",
        "repo_name": "Mathiasotnes___Multi-Stream-Transformer",
        "func_path": "mst/architecture/multi_head_attention.py",
        "orig_func": "def forward(self, x: Tensor) -> Tensor:\n    (B, T, C) = x.size()\n    (Q, K, V) = self.attn(x).split(self.d_model, dim=2)\n    Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2)\n    K = K.view(B, T, self.nh, self.d_k).transpose(1, 2)\n    V = V.view(B, T, self.nh, self.d_k).transpose(1, 2)\n    attn = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n    if self.masked:\n        attn = attn.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n    attn = F.softmax(attn, dim=-1)\n    self.attn_weights = attn\n    y = attn @ V\n    y = y.transpose(1, 2).contiguous().view(B, T, C)\n    y = self.W_o(y)\n    return y",
        "orig_context": "```python\n## mst/architecture/multi_head_attention.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nclass MultiHeadAttention( nn.Module ):\n    \"\"\"\n    Multi-head attention mechanism.\n    \n    Args:\n        d_model (int):                  Dimensionality of the model (Embedding dimension).\n        num_heads (int):                Number of attention heads.\n        context_window (int):           Size of the context window. Defaults to d_model.\n        masked (bool):                  Whether to use causal masked attention or not.\n    \"\"\"\n    def __init__( \n            self, \n            d_model: int, \n            num_heads: int, \n            context_window: int=None, \n            masked: bool=False,\n        ) -> None:\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.nh         = num_heads\n        self.d_k        = d_model // self.nh # Embedding dimension for each head\n        self.d_model    = d_model\n        self.ctx_d      = context_window if context_window is not None else d_model\n        self.masked     = masked\n\n        self.attn = nn.Linear(d_model, 3 * d_model) \n        self.W_o = nn.Linear(d_model, d_model)\n        self.register_buffer(\n            \"bias\", \n            torch.tril(torch.ones(self.ctx_d, self.ctx_d)).view(1, 1, self.ctx_d, self.ctx_d) # (1, 1, ctx_d, ctx_d) - Causal mask\n        )\n        \n    def forward( self, x: Tensor ) -> Tensor:\n        B, T, C = x.size() # (batch_size, seq_length, d_model)\n\n        # Calculate queries, keys and values for all heads in a batch, and move head dimension to the front\n        Q, K, V = self.attn(x).split(self.d_model, dim=2)   # (B, T, 3*d_model) -> 3 * (B, T, d_model)\n        Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        K = K.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n        V = V.view(B, T, self.nh, self.d_k).transpose(1, 2) # (B, T, nh, d_k)   -> (B, nh, T, d_k)\n\n        # Scaled dot-product attention\n        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if self.masked:\n            attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        \n        self.attn_weights = attn\n        y = attn @ V\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # Output projection\n        y = self.W_o(y)\n        return y\n\n```\n\n\n",
        "eval_script": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nclass MultiHeadAttention( nn.Module ):\n    def __init__( \n            self, \n            d_model: int, \n            num_heads: int, \n            context_window: int=None, \n            masked: bool=False,\n        ) -> None:\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.nh         = num_heads\n        self.d_k        = d_model // self.nh\n        self.d_model    = d_model\n        self.ctx_d      = context_window if context_window is not None else d_model\n        self.masked     = masked\n\n        self.attn = nn.Linear(d_model, 3 * d_model) \n        self.W_o = nn.Linear(d_model, d_model)\n        self.register_buffer(\n            \"bias\", \n            torch.tril(torch.ones(self.ctx_d, self.ctx_d)).view(1, 1, self.ctx_d, self.ctx_d) \n        )\n        \n    def forward( self, x: Tensor ) -> Tensor:\n        B, T, C = x.size()\n\n        Q, K, V = self.attn(x).split(self.d_model, dim=2)\n        Q = Q.view(B, T, self.nh, self.d_k).transpose(1, 2)\n        K = K.view(B, T, self.nh, self.d_k).transpose(1, 2)\n        V = V.view(B, T, self.nh, self.d_k).transpose(1, 2)\n\n        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if self.masked:\n            attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        \n        self.attn_weights = attn\n        y = attn @ V\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        y = self.W_o(y)\n        return y\n        \n\n\ndef test_forward():\n    d_model = 16\n    num_heads = 4\n    batch_size = 2\n    seq_length = 10\n\n    attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n    attention_new = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n\n    attention.attn.weight.data = attention_new.attn.weight.data.clone()\n    attention.attn.bias.data = attention_new.attn.bias.data.clone()\n    attention.W_o.weight.data = attention_new.W_o.weight.data.clone()\n    attention.W_o.bias.data = attention_new.W_o.bias.data.clone()\n\n    x = torch.randn(batch_size, seq_length, d_model)\n\n    output_old = attention.forward(x)\n    output_new = attention_new.forward_new_implementation(x)\n\n    assert output_old.shape == output_new.shape, \"Shape mismatch\"\n    assert torch.allclose(output_old, output_new, atol=1e-6), \"Output mismatch in values\"\n    assert torch.allclose(attention.attn_weights, attention_new.attn_weights, atol=1e-6), \"Attention weights mismatch\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "save_dataframe",
        "idx": "98",
        "repo_name": "datuadiatma___mass_spec",
        "func_path": "Element2/load_asc_files.py",
        "orig_func": "def save_dataframe(df, base_path):\n    \"\"\"\n    Save the DataFrame in both CSV and Excel formats\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to save\n    base_path : str\n        Base path for saving files (without extension)\n    \"\"\"\n    base_path = os.path.splitext(base_path)[0]\n    csv_path = f'{base_path}.csv'\n    df.to_csv(csv_path, index=None)\n    xlsx_path = f'{base_path}.xlsx'\n    df.to_excel(xlsx_path, index=None)\n    return (csv_path, xlsx_path)",
        "orig_context": "```python\n## Element2/load_asc_files.py\nimport os\n\ndef save_dataframe(df, base_path):\n    \"\"\"\n    Save the DataFrame in both CSV and Excel formats\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to save\n    base_path : str\n        Base path for saving files (without extension)\n    \"\"\"\n    # Remove any extension from the base path\n    base_path = os.path.splitext(base_path)[0]\n    \n    # Save as CSV\n    csv_path = f\"{base_path}.csv\"\n    df.to_csv(csv_path, index=None)\n    \n    # Save as Excel\n    xlsx_path = f\"{base_path}.xlsx\"\n    df.to_excel(xlsx_path, index=None)\n    \n    return csv_path, xlsx_path\n\n```\n\n\n",
        "eval_script": "## Element2/load_asc_files.py\nimport os\nimport pandas as pd\n\ndef save_dataframe(df, base_path):\n    \"\"\"\n    Save the DataFrame in both CSV and Excel formats\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame to save\n    base_path : str\n        Base path for saving files (without extension)\n    \"\"\"\n    # Remove any extension from the base path\n    base_path = os.path.splitext(base_path)[0]\n    \n    # Save as CSV\n    csv_path = f\"{base_path}.csv\"\n    df.to_csv(csv_path, index=None)\n    \n    # Save as Excel\n    xlsx_path = f\"{base_path}.xlsx\"\n    df.to_excel(xlsx_path, index=None)\n    \n    return csv_path, xlsx_path\n\n\n\ndef test_save_dataframe():\n    # Create a sample DataFrame\n    df = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': ['x', 'y', 'z']\n    })\n    base_path_old = '/home/user/tmp/test_old'\n    base_path_new = '/home/user/tmp/test_new'\n    \n    # Get file paths from both implementations\n    old_csv_path, old_xlsx_path = save_dataframe(df, base_path_old)\n    new_csv_path, new_xlsx_path = save_dataframe_new_implementation(df, base_path_new)\n    \n    # Assert that both function return the same file paths\n    assert os.path.exists(old_csv_path)\n    assert os.path.exists(new_csv_path)\n    assert os.path.exists(old_xlsx_path)\n    assert os.path.exists(new_xlsx_path)\n\n    # Compare CSV contents\n    old_csv_df = pd.read_csv(old_csv_path)\n    new_csv_df = pd.read_csv(new_csv_path)\n    assert old_csv_df.equals(new_csv_df), \"CSV files content mismatch\"\n    \n    # Compare Excel contents\n    old_xlsx_df = pd.read_excel(old_xlsx_path)\n    new_xlsx_df = pd.read_excel(new_xlsx_path)\n    assert old_xlsx_df.equals(new_xlsx_df), \"Excel files content mismatch\"\n\nif __name__ == \"__main__\":\n    test_save_dataframe()"
    },
    {
        "func_name": "get_latest_date",
        "idx": "99",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "cnpj_download.py",
        "orig_func": "def get_latest_date(soup):\n    \"\"\"\n    Encontra a data mais recente entre as existentes no conte\u00fado HTML \n    Par\u00e2metros:\n        soup (BeautifulSoup): objeto BeautifulSoup\n    Retorno:\n        str: data mais recente no formato AAAA-MM-DD\n    \"\"\"\n    data = re.findall('\\\\d{4}-\\\\d{2}-\\\\d{2}', soup.text)\n    return max((datetime.datetime.strptime(d, '%Y-%m-%d') for d in data))",
        "orig_context": "```python\n## cnpj_download.py\nimport re\n\nimport datetime\n\ndef get_latest_date(soup):\n    \"\"\"\n    Encontra a data mais recente entre as existentes no conte\u00fado HTML \n    Par\u00e2metros:\n        soup (BeautifulSoup): objeto BeautifulSoup\n    Retorno:\n        str: data mais recente no formato AAAA-MM-DD\n    \"\"\"\n\n    # Gera lista de datas localizadas no texto da p\u00e1gina HTML\n    data = re.findall(r'\\d{4}-\\d{2}-\\d{2}', soup.text)\n\n    # Retorna data mais recente (string)\n    return max(datetime.datetime.strptime(d, '%Y-%m-%d') for d in data)\n\n```\n\n\n",
        "eval_script": "## cnpj_download.py\nimport re\nimport datetime\nfrom bs4 import BeautifulSoup\n\ndef get_latest_date(soup):\n    \"\"\"\n    Encontra a data mais recente entre as existentes no conte\u00fado HTML \n    Par\u00e2metros:\n        soup (BeautifulSoup): objeto BeautifulSoup\n    Retorno:\n        str: data mais recente no formato AAAA-MM-DD\n    \"\"\"\n\n    # Gera lista de datas localizadas no texto da p\u00e1gina HTML\n    data = re.findall(r'\\d{4}-\\d{2}-\\d{2}', soup.text)\n\n    # Retorna data mais recente (string)\n    return max(datetime.datetime.strptime(d, '%Y-%m-%d') for d in data)\n\n\n\ndef test_get_latest_date():\n    # Test case 1: Multiple dates, including the latest one\n    html_1 = \"<html><body>2023-10-01 2023-09-15 2023-10-05</body></html>\"\n    soup_1 = BeautifulSoup(html_1, 'html.parser')\n    assert get_latest_date(soup_1) == get_latest_date_new_implementation(soup_1)\n\n    # Test case 2: Different formatting and order\n    html_2 = \"<html><body>Random text 2022-08-10 Some text 2022-08-12</body></html>\"\n    soup_2 = BeautifulSoup(html_2, 'html.parser')\n    assert get_latest_date(soup_2) == get_latest_date_new_implementation(soup_2)\n\n    # Test case 3: Single date\n    html_3 = \"<html><body>2021-07-23</body></html>\"\n    soup_3 = BeautifulSoup(html_3, 'html.parser')\n    assert get_latest_date(soup_3) == get_latest_date_new_implementation(soup_3)\n\nif __name__ == \"__main__\":\n    test_get_latest_date()"
    },
    {
        "func_name": "get_zip_links",
        "idx": "100",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "cnpj_download.py",
        "orig_func": "def get_zip_links(soup):\n    \"\"\"\n    Extrai URLs para download dos arquivos ZIP\n    Par\u00e2metros:\n        soup (BeautifulSoup): objeto com conte\u00fado HTML\n    Retorno:\n        List[str]: lista de URLs \n    \"\"\"\n    links = []\n    for (n, link) in enumerate(soup.find_all('a')):\n        if link['href'].endswith('.zip'):\n            url = f\"{BASE_URL}{link['href']}\" if not link['href'].startswith('http') else link['href']\n            links.append(url)\n    return links",
        "orig_context": "```python\n## cnpj_download.py\nBASE_URL = 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/2024-10/'\n\ndef get_zip_links(soup):\n    \"\"\"\n    Extrai URLs para download dos arquivos ZIP\n    Par\u00e2metros:\n        soup (BeautifulSoup): objeto com conte\u00fado HTML\n    Retorno:\n        List[str]: lista de URLs \n    \"\"\"\n\n    # Cria lista para armazenar URLs dos arquivos ZIP\n    links = []\n\n    # Realiza loop sobre todos os elementos 'a' presentes no arquivo HTML\n    for n, link in enumerate(soup.find_all('a')):\n\n        # Verifica se o atributo 'href' do elemento 'a' termina com '.zip'\n        if link['href'].endswith('.zip'):\n\n            # Gera URL completa\n            url = f'{BASE_URL}{link[\"href\"]}' if not link['href'].startswith('http') else link['href']\n\n            # Adiciona URL completa \u00e0 lista 'links'\n            links.append(url)\n\n            # Para teste apenas\n            # if n == 0:\n            #     break\n\n    # Retorna lista de URLs\n    return links\n\n```\n\n\n",
        "eval_script": "## cnpj_download.py\nfrom bs4 import BeautifulSoup\n\nBASE_URL = 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/2024-10/'\n\ndef get_zip_links(soup):\n    \"\"\"\n    Extrai URLs para download dos arquivos ZIP\n    Par\u00e2metros:\n        soup (BeautifulSoup): objeto com conte\u00fado HTML\n    Retorno:\n        List[str]: lista de URLs \n    \"\"\"\n\n    # Cria lista para armazenar URLs dos arquivos ZIP\n    links = []\n\n    # Realiza loop sobre todos os elementos 'a' presentes no arquivo HTML\n    for n, link in enumerate(soup.find_all('a')):\n\n        # Verifica se o atributo 'href' do elemento 'a' termina com '.zip'\n        if link['href'].endswith('.zip'):\n\n            # Gera URL completa\n            url = f'{BASE_URL}{link[\"href\"]}' if not link['href'].startswith('http') else link['href']\n\n            # Adiciona URL completa \u00e0 lista 'links'\n            links.append(url)\n\n            # Para teste apenas\n            # if n == 0:\n            #     break\n\n    # Retorna lista de URLs\n    return links\n\n\n\ndef test_get_zip_links():\n    \"\"\"\n    Test function to compare get_zip_links and get_zip_links_new_implementation.\n    \"\"\"\n    html_content = '''\n    <html>\n        <body>\n            <a href=\"file1.zip\">Download 1</a>\n            <a href=\"http://example.com/file2.zip\">Download 2</a>\n            <a href=\"document.pdf\">Other File</a>\n            <a href=\"file3.zip\">Download 3</a>\n            <a href=\"script.js\">Non-Zip File</a>\n        </body>\n    </html>\n    '''\n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    original_links = get_zip_links(soup)\n    new_links = get_zip_links_new_implementation(soup)\n\n    # Assert statements to ensure the functions are equivalent\n    assert original_links == new_links, \"Test 1 Failed: Outputs are not the same!\"\n    \n    # Another scenario with an empty HTML content\n    empty_soup = BeautifulSoup('', 'html.parser')\n    assert get_zip_links(empty_soup) == get_zip_links_new_implementation(empty_soup), \"Test 2 Failed: Outputs not identical for empty input!\"\n    \n    # Testing with no .zip links\n    no_zip_html = '<html><body><a href=\"document.pdf\">Doc</a></body></html>'\n    no_zip_soup = BeautifulSoup(no_zip_html, 'html.parser')\n    assert get_zip_links(no_zip_soup) == get_zip_links_new_implementation(no_zip_soup), \"Test 3 Failed: Outputs not identical for no-zip input!\"\n\nif __name__ == \"__main__\":\n    test_get_zip_links()"
    },
    {
        "func_name": "get_soup",
        "idx": "101",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "cnpj_download.py",
        "orig_func": "def get_soup(url):\n    \"\"\"\n    Faz request HTTP para a URL e retorna objeto BeautifulSoup     \n    Par\u00e2metros:\n        url (str): URL a ser requisitada\n    Retorno:\n        objeto BeautifulSoup com conte\u00fado HTML da URL\n    \"\"\"\n    response = requests.get(url, verify=False)\n    return BeautifulSoup(response.text, 'html.parser')",
        "orig_context": "```python\n## cnpj_download.py\nimport requests\n\nfrom bs4 import BeautifulSoup\n\ndef get_soup(url):\n    \"\"\"\n    Faz request HTTP para a URL e retorna objeto BeautifulSoup     \n    Par\u00e2metros:\n        url (str): URL a ser requisitada\n    Retorno:\n        objeto BeautifulSoup com conte\u00fado HTML da URL\n    \"\"\"\n\n    # Gera objeto 'response'\n    response = requests.get(url, verify=False)\n\n    # Retorna texto da URL\n    return BeautifulSoup(response.text, 'html.parser')\n\n```\n\n\n",
        "eval_script": "## cnpj_download.py\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_soup(url):\n    \"\"\"\n    Faz request HTTP para a URL e retorna objeto BeautifulSoup     \n    Par\u00e2metros:\n        url (str): URL a ser requisitada\n    Retorno:\n        objeto BeautifulSoup com conte\u00fado HTML da URL\n    \"\"\"\n\n    # Gera objeto 'response'\n    response = requests.get(url, verify=False)\n\n    # Retorna texto da URL\n    return BeautifulSoup(response.text, 'html.parser')\n\n\n\ndef test_get_soup():\n    url1 = \"http://example.com\"\n    url2 = \"http://httpbin.org/html\"\n    url3 = \"https://www.wikipedia.org\"\n\n    # Original function results\n    soup1_original = get_soup(url1)\n    soup2_original = get_soup(url2)\n    soup3_original = get_soup(url3)\n\n    # New function results\n    soup1_new = get_soup_new_implementation(url1)\n    soup2_new = get_soup_new_implementation(url2)\n    soup3_new = get_soup_new_implementation(url3)\n\n    # Assertions\n    assert soup1_original == soup1_new, \"The implementations differ for URL 1.\"\n    assert soup2_original == soup2_new, \"The implementations differ for URL 2.\"\n    assert soup3_original == soup3_new, \"The implementations differ for URL 3.\"\n\nif __name__ == \"__main__\":\n    test_get_soup()"
    },
    {
        "func_name": "sqlCriaTabela",
        "idx": "102",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "dados_cnpj_para_sqlite.py",
        "orig_func": "def sqlCriaTabela(nomeTabela, colunas):\n    sql = 'CREATE TABLE ' + nomeTabela + ' ('\n    for (k, coluna) in enumerate(colunas):\n        sql += '\\n' + coluna + ' TEXT'\n        if k + 1 < len(colunas):\n            sql += ','\n    sql += ')\\n'\n    return sql",
        "orig_context": "```python\n## dados_cnpj_para_sqlite.py\ndef sqlCriaTabela(nomeTabela, colunas):\n    sql = 'CREATE TABLE ' + nomeTabela + ' ('\n    for k, coluna in enumerate(colunas):\n        sql += '\\n' + coluna + ' TEXT'\n        if k+1<len(colunas):\n            sql+= ',' #'\\n'\n    sql += ')\\n'\n    return sql\n\n```\n\n\n",
        "eval_script": "## dados_cnpj_para_sqlite.py\ndef sqlCriaTabela(nomeTabela, colunas):\n    sql = 'CREATE TABLE ' + nomeTabela + ' ('\n    for k, coluna in enumerate(colunas):\n        sql += '\\n' + coluna + ' TEXT'\n        if k+1<len(colunas):\n            sql+= ',' #'\\n'\n    sql += ')\\n'\n    return sql\n\n\n\ndef test_sqlCriaTabela():\n    # Test case 1: Single column\n    assert sqlCriaTabela('testTable1', ['column1']) == sqlCriaTabela_new_implementation('testTable1', ['column1'])\n    # Test case 2: Multiple columns\n    assert sqlCriaTabela('testTable2', ['column1', 'column2', 'column3']) == sqlCriaTabela_new_implementation('testTable2', ['column1', 'column2', 'column3'])\n    # Test case 3: No columns\n    assert sqlCriaTabela('testTable3', []) == sqlCriaTabela_new_implementation('testTable3', [])\n\nif __name__ == \"__main__\":\n    test_sqlCriaTabela()"
    },
    {
        "func_name": "Network.add_nodes",
        "idx": "103",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "network.py",
        "orig_func": "def add_nodes(self, nodes, **kwargs):\n    \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n    valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n    for k in kwargs:\n        assert k in valid_args, \"invalid arg '\" + k + \"'\"\n    nd = defaultdict(dict)\n    for i in range(len(nodes)):\n        for (k, v) in kwargs.items():\n            assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n            nd[nodes[i]].update({k: v[i]})\n    for node in nodes:\n        try:\n            node = int(node)\n            self.add_node(node, **nd[node])\n        except:\n            assert isinstance(node, str)\n            self.add_node(node, **nd[node])",
        "orig_context": "```python\n# network.py\n\nclass Network(object):\n    \"\"\"\n    The Network class is the focus of this library. All viz functionality\n    should be implemented off of a Network instance.\n\n    To instantiate:\n\n    >>> nt = Network()\n    \"\"\"\n\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        \"\"\"\n        :param height: The height of the canvas\n        :param width: The width of the canvas\n        :param directed: Whether or not to use a directed graph. This is false\n                         by default.\n        :param notebook: True if using jupyter notebook.\n        :param select_menu: sets the option to highlight nodes and the neighborhood\n        :param filter_menu: sets the option to filter nodes and edges based on attributes\n        :param bgcolor: The background color of the canvas.\n        :param cdn_resources: Where to pull resources for css and js files. Defaults to local.\n            Options ['local','in_line','remote'].\n            local: pull resources from local lib folder.\n            in_line: insert lib resources as inline script tags.\n            remote: pull resources from hash checked cdns.\n        :font_color: The color of the node labels text\n        :layout: Use hierarchical layout if this is set\n\n        :type height: num or str\n        :type width: num or str\n        :type directed: bool\n        :type notebook: bool\n        :type select_menu: bool\n        :type filter_menu: bool\n        :type bgcolor: str\n        :type font_color: str\n        :type layout: bool\n        :type cdn_resources: str\n        \"\"\"\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        \"\"\"\n        override print to show readable graph data\n        \"\"\"\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        \"\"\"\n        This method adds a node to the network, given a mandatory node ID.\n        Node labels default to node ids if no label is specified during the\n        call.\n\n        >>> nt = Network(\"500px\", \"500px\")\n        >>> nt.add_node(0, label=\"Node 0\")\n        >>> nt.add_node(1, label=\"Node 1\", color = \"blue\")\n\n        :param n_id: The id of the node. The id is mandatory for nodes and\n                     they have to be unique. This should obviously be set per\n                     node, not globally.\n\n        :param label: The label is the piece of text shown in or under the\n                      node, depending on the shape.\n\n        :param borderWidth:\tThe width of the border of the node.\n\n        :param borderWidthSelected:\tThe width of the border of the node when\n                                    it is selected. When undefined, the\n                                    borderWidth * 2 is used.\n\n        :param brokenImage:\tWhen the shape is set to image or circularImage,\n                            this option can be an URL to a backup image in\n                            case the URL supplied in the image option cannot\n                            be resolved.\n\n        :param group: When not undefined, the node will belong to the defined\n                      group. Styling information of that group will apply to\n                      this node. Node specific styling overrides group styling.\n\n        :param hidden: When true, the node will not be shown. It will still be\n                       part of the physics simulation though!\n\n        :param image: When the shape is set to image or circularImage, this\n                      option should be the URL to an image. If the image\n                      cannot be found, the brokenImage option can be used.\n\n        :param labelHighlightBold: Determines whether or not the label becomes\n                                   bold when the node is selected.\n\n        :param level: When using the hierarchical layout, the level determines\n                      where the node is going to be positioned.\n\n        :param mass: The barnesHut physics model (which is enabled by default)\n                     is based on an inverted gravity model. By increasing\n                     the mass of a node, you increase it's repulsion. Values\n                     lower than 1 are not recommended.\n\n        :param physics:\tWhen false, the node is not part of the physics\n                        simulation. It will not move except for from\n                        manual dragging.\n\n        :param shape: The shape defines what the node looks like. There are\n                      two types of nodes. One type has the label inside of\n                      it and the other type has the label underneath it. The\n                      types with the label inside of it are: ellipse, circle,\n                      database, box, text. The ones with the label outside of\n                      it are: image, circularImage, diamond, dot, star,\n                      triangle, triangleDown, square and icon.\n\n        :param size: The size is used to determine the size of node shapes that\n                     do not have the label inside of them. These shapes are:\n                     image, circularImage, diamond, dot, star, triangle,\n                     triangleDown, square and icon.\n\n        :param title: Title to be displayed when the user hovers over the node.\n                      The title can be an HTML element or a string containing\n                      plain text or HTML.\n\n        :param value: When a value is set, the nodes will be scaled using the\n                      options in the scaling object defined above.\n\n        :param x: This gives a node an initial x position. When using the\n                  hierarchical layout, either the x or y position is set by the\n                  layout engine depending on the type of view. The other value\n                  remains untouched. When using stabilization, the stabilized\n                  position may be different from the initial one. To lock the\n                  node to that position use the physics or fixed options.\n\n        :param y: This gives a node an initial y position. When using the\n                  hierarchical layout,either the x or y position is set by\n                  the layout engine depending on the type of view. The\n                  other value remains untouched. When using stabilization,\n                  the stabilized position may be different from the initial\n                  one. To lock the node to that position use the physics or\n                  fixed options.\n\n        :type n_id: str or int\n        :type label: str or int\n        :type borderWidth: num (optional)\n        :type borderWidthSelected: num (optional)\n        :type brokenImage: str (optional)\n        :type group: str (optional)\n        :type hidden: bool (optional)\n        :type image: str (optional)\n        :type labelHighlightBold: bool (optional)\n        :type level: num (optional)\n        :type mass: num (optional)\n        :type physics: bool (optional)\n        :type shape: str (optional)\n        :type size: num (optional)\n        :type title: str or html element (optional)\n        :type value: num (optional)\n        :type x: num (optional)\n        :type y: num (optional)\n        \"\"\"\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def num_nodes(self):\n        \"\"\"\n        Return number of nodes\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.node_ids)\n\n    def num_edges(self):\n        \"\"\"\n        Return number of edges\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        \"\"\"\n\n        Adding edges is done based off of the IDs of the nodes. Order does\n        not matter unless dealing with a directed graph.\n\n        >>> nt.add_edge(0, 1) # adds an edge from node ID 0 to node ID\n        >>> nt.add_edge(0, 1, value = 4) # adds an edge with a width of 4\n\n\n        :param arrowStrikethrough: When false, the edge stops at the arrow.\n                                   This can be useful if you have thick lines\n                                   and you want the arrow to end in a point.\n                                   Middle arrows are not affected by this.\n\n        :param from: Edges are between two nodes, one to and one from. This\n                     is where you define the from node. You have to supply\n                     the corresponding node ID. This naturally only applies\n                     to individual edges.\n\n        :param hidden: When true, the edge is not drawn. It is part still part\n                       of the physics simulation however!\n\n        :param physics:\tWhen true, the edge is part of the physics simulation.\n                        When false, it will not act as a spring.\n\n        :param title: The title is shown in a pop-up when the mouse moves over\n                      the edge.\n\n        :param to: Edges are between two nodes, one to and one from. This is\n                   where you define the to node. You have to supply the\n                   corresponding node ID. This naturally only applies to\n                   individual edges.\n\n        :param value: When a value is set, the edges' width will be scaled\n                      using the options in the scaling object defined above.\n\n        :param width: The width of the edge. If value is set, this is not used.\n\n\n        :type arrowStrikethrough: bool\n        :type from: str or num\n        :type hidden: bool\n        :type physics: bool\n        :type title: str\n        :type to: str or num\n        :type value: num\n        :type width: num\n        \"\"\"\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        \"\"\"\n        This method serves to add multiple edges between existing nodes\n        in the network instance. Adding of the edges is done based off\n        of the IDs of the nodes. Order does not matter unless dealing with a\n        directed graph.\n\n        :param edges: A list of tuples, each tuple consists of source of edge,\n                      edge destination and and optional width.\n\n        :type arrowStrikethrough: list of tuples\n        \"\"\"\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        \"\"\"\n        Extract relevant information about this network in order to inject into\n        a Jinja2 template.\n\n        Returns:\n                nodes (list), edges (list), height (\n                    string), width (string), options (object)\n\n        Usage:\n\n        >>> nodes, edges, heading, height, width, options = net.get_network_data()\n        \"\"\"\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        \"\"\"\n        Save the graph as html in the current directory with name.\n\n        :param name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        \"\"\"\n        Writes a static HTML file and saves it locally before opening.\n\n        :param: name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        \"\"\"\n        Loads the template data into the template attribute of the network.\n        This should be done in a jupyter notebook environment before showing\n        the network.\n\n        Example:\n                >>> net.prep_notebook()\n                >>> net.show(\"nb.html\")\n\n\n        :param path: the relative path pointing to a template html file\n        :type path: string\n        \"\"\"\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        \"\"\"\n            Path to full template assumes that it exists inside of a template directory.\n            Use `set_template_dir` to set the relative template path to the template directory along with the directory location itself\n            to change both values otherwise this function will infer the results.\n            :path_to_template path: full os path string value of the template directory\n        \"\"\"\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        \"\"\"\n            Path to template directory along with the location of the template file.\n            :template_directory path: template directory\n            :template_file path: name of the template file that is going to be used to generate the html doc.\n\n        \"\"\"\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        \"\"\"\n        This method takes the contents of .DOT file and converts it\n        to a PyVis visualization.\n\n        Assuming the contents of test.dot contains:\n        digraph sample3 {\n        A -> {B ; C ; D}\n        C -> {B ; A}\n        }\n\n        Usage:\n\n        >>> nt.Network(\"500px\", \"500px\")\n        >>> nt.from_DOT(\"test.dot\")\n        >>> nt.show(\"dot.html\")\n\n        :param dot: The path of the dotfile being converted.\n        :type dot: .dot file\n\n        \"\"\"\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        \"\"\"\n        This method returns the user an adjacency list representation\n        of the network.\n\n        :returns: dictionary mapping of Node ID to list of Node IDs it\n        is connected to.\n        \"\"\"\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def neighbors(self, node):\n        \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        \"\"\"\n        This method takes an exisitng Networkx graph and translates\n        it to a PyVis graph format that can be accepted by the VisJs\n        API in the Jinja2 template. This operation is done in place.\n\n        :param nx_graph: The Networkx graph object that is to be translated.\n        :type nx_graph: networkx.Graph instance\n        :param node_size_transf: function to transform the node size for plotting\n        :type node_size_transf: func\n        :param edge_weight_transf: function to transform the edge weight for plotting\n        :type edge_weight_transf: func\n        :param default_node_size: default node size if not specified\n        :param default_edge_weight: default edge weight if not specified\n        >>> nx_graph = nx.cycle_graph(10)\n        >>> nx_graph.nodes[1]['title'] = 'Number 1'\n        >>> nx_graph.nodes[1]['group'] = 1\n        >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n        >>> nx_graph.nodes[3]['group'] = 10\n        >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n        >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n        >>> nx_graph.add_edge(20, 21, weight=5)\n        >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n        >>> nt = Network(\"500px\", \"500px\")\n        # populates the nodes and edges data structures\n        >>> nt.from_nx(nx_graph)\n        >>> nt.show(\"nx.html\")\n        \"\"\"\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        \"\"\"\n        This method returns an iterable list of node ids\n\n        :returns: list\n        \"\"\"\n        return self.node_ids\n\n    def get_node(self, n_id):\n        \"\"\"\n        Lookup node by ID and return it.\n\n        :param n_id: The ID given to the node.\n\n        :returns: dict containing node properties\n        \"\"\"\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        \"\"\"\n        This method returns an iterable list of edge objects\n\n        :returns: list\n        \"\"\"\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        \"\"\"\n        BarnesHut is a quadtree based gravity model. It is the fastest. default\n        and recommended solver for non-hierarchical layouts.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        \"\"\"\n        Set the physics attribute of the entire network to repulsion.\n        When called, it sets the solver attribute of physics to repulsion.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        \"\"\"\n        This model is based on the repulsion solver but the levels are\n        taken into account and the forces are normalized.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        \"\"\"\n        The forceAtlas2Based solver makes use of some of the equations provided\n        by them and makes use of the barnesHut implementation in vis. The main\n        differences are the central gravity model, which is here distance\n        independent, and the repulsion being linear instead of quadratic. Finally,\n        all node masses have a multiplier based on the amount of connected edges\n        plus one.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def set_edge_smooth(self, smooth_type):\n        \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n    def toggle_hide_edges_on_drag(self, status):\n        \"\"\"\n        Displays or hides edges while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: True if edges should be hidden on drag\n        \n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        \"\"\"\n        Displays or hides nodes while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: When set to True, the nodes will hide on drag.\n                       Default is set to False.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        \"\"\"\n        Edges take on the color of the node they are coming from.\n\n        :param status: True if edges should adopt color coming from.\n        :type status: bool\n        \"\"\"\n        self.options.edges.inherit_colors(status)\n\n    def show_buttons(self, filter_=None):\n        \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n    def toggle_physics(self, status):\n        \"\"\"\n        Toggles physics simulation \n\n        :param status: When False, nodes are not part of the physics\n                       simulation. They will not move except for from\n                       manual dragging.\n                       Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        \"\"\"\n        Toggles the dragging of the nodes in the network.\n\n        :param status: When set to True, the nodes can be dragged around\n                       in the network. Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        \"\"\"\n        Toggles the stablization of the network.\n\n        :param status: Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        \"\"\"\n        Overrides the default options object passed to the VisJS framework.\n        Delegates to the :meth:`options.Options.set` routine.\n\n        :param options: The string representation of the Javascript-like object\n                        to be used to override default options.\n        \n        :type options: str\n        \"\"\"\n        self.options = self.options.set(options)\n\n    def add_nodes(self, nodes, **kwargs):\n        \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n```\n",
        "eval_script": "# network.py\n\nfrom collections import defaultdict\nimport os\nimport json\nfrom jinja2 import Environment, FileSystemLoader\n\nclass Options:\n    def __init__(self, layout=None):\n        self.physics = Physics()\n        self.edges = Edges()\n        self.interaction = Interaction()\n    \n    def to_json(self):\n        return json.dumps({\n            \"physics\": self.physics.to_json(),\n            \"edges\": self.edges.to_json(),\n            \"interaction\": self.interaction.to_json()\n        })\n\n# Placeholder classes for Physics, Edges, and Interaction\nclass Physics:\n    def __init__(self):\n        self.enabled = True\n\n    def to_json(self):\n        return {\"enabled\": self.enabled}\n\n    def use_barnes_hut(self, args): pass\n    def use_repulsion(self, args): pass\n    def use_hrepulsion(self, args): pass\n    def use_force_atlas_2based(self, args): pass\n    def toggle_stabilization(self, status): pass\n\nclass Edges:\n    def __init__(self):\n        self.smooth = Smooth()\n\n    def to_json(self):\n        return {\"smooth\": self.smooth.to_json()}\n\n    def inherit_colors(self, status): pass\n\nclass Smooth:\n    def __init__(self):\n        self.enabled = False\n        self.type = 'continuous'\n    \n    def to_json(self):\n        return {\"enabled\": self.enabled, \"type\": self.type}\n\nclass Interaction:\n    def __init__(self):\n        self.hideEdgesOnDrag = False\n        self.hideNodesOnDrag = False\n        self.dragNodes = True\n    \n    def to_json(self):\n        return {\n            \"hideEdgesOnDrag\": self.hideEdgesOnDrag,\n            \"hideNodesOnDrag\": self.hideNodesOnDrag,\n            \"dragNodes\": self.dragNodes\n        }\n\nclass Node:\n    def __init__(self, n_id, shape='dot', label=None, color='#97c2fc', font_color=None, **options):\n        self.id = n_id\n        self.shape = shape\n        self.label = label\n        self.color = color\n        self.font_color = font_color\n        self.options = {\n            'id': n_id,\n            'shape': shape,\n            'label': label,\n            'color': color,\n            'font_color': font_color,\n            **options\n        }\n\nclass Network(object):\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n    \n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def num_nodes(self):\n        return len(self.node_ids)\n\n    def num_edges(self):\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def neighbors(self, node):\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        return self.node_ids\n\n    def get_node(self, n_id):\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def set_edge_smooth(self, smooth_type):\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n    def toggle_hide_edges_on_drag(self, status):\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        self.options.edges.inherit_colors(status)\n\n    def show_buttons(self, filter_=None):\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n    def toggle_physics(self, status):\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        self.options = self.options.set(options)\n\n    def add_nodes(self, nodes, **kwargs):\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n\n# Test function\ndef test_add_nodes():\n    # Sample data\n    nodes = [1, 2, 3]\n    sizes = [10, 20, 30]\n    titles = [\"Node 1\", \"Node 2\", \"Node 3\"]\n    \n    # Create networks\n    original_net = Network()\n    new_impl_net = Network()\n    \n    # Add nodes using the original method\n    original_net.add_nodes(nodes, size=sizes, title=titles)\n    \n    # Add nodes using the new implementation\n    new_impl_net.add_nodes_new_implementation(nodes, size=sizes, title=titles)\n    \n    # Run assertions\n    assert original_net.nodes == new_impl_net.nodes, \"Node lists differ!\"\n    assert original_net.node_ids == new_impl_net.node_ids, \"Node IDs differ!\"\n    assert original_net.node_map == new_impl_net.node_map, \"Node maps differ!\"\n\n# Main function\nif __name__ == \"__main__\":\n    test_add_nodes()"
    },
    {
        "func_name": "Network.show_buttons",
        "idx": "105",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "network.py",
        "orig_func": "def show_buttons(self, filter_=None):\n    \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n    self.conf = True\n    self.options.configure = Configure(enabled=True, filter_=filter_)\n    self.widget = True",
        "orig_context": "```python\n# network.py\n\nclass Network(object):\n    \"\"\"\n    The Network class is the focus of this library. All viz functionality\n    should be implemented off of a Network instance.\n\n    To instantiate:\n\n    >>> nt = Network()\n    \"\"\"\n\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        \"\"\"\n        :param height: The height of the canvas\n        :param width: The width of the canvas\n        :param directed: Whether or not to use a directed graph. This is false\n                         by default.\n        :param notebook: True if using jupyter notebook.\n        :param select_menu: sets the option to highlight nodes and the neighborhood\n        :param filter_menu: sets the option to filter nodes and edges based on attributes\n        :param bgcolor: The background color of the canvas.\n        :param cdn_resources: Where to pull resources for css and js files. Defaults to local.\n            Options ['local','in_line','remote'].\n            local: pull resources from local lib folder.\n            in_line: insert lib resources as inline script tags.\n            remote: pull resources from hash checked cdns.\n        :font_color: The color of the node labels text\n        :layout: Use hierarchical layout if this is set\n\n        :type height: num or str\n        :type width: num or str\n        :type directed: bool\n        :type notebook: bool\n        :type select_menu: bool\n        :type filter_menu: bool\n        :type bgcolor: str\n        :type font_color: str\n        :type layout: bool\n        :type cdn_resources: str\n        \"\"\"\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        \"\"\"\n        override print to show readable graph data\n        \"\"\"\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        \"\"\"\n        This method adds a node to the network, given a mandatory node ID.\n        Node labels default to node ids if no label is specified during the\n        call.\n\n        >>> nt = Network(\"500px\", \"500px\")\n        >>> nt.add_node(0, label=\"Node 0\")\n        >>> nt.add_node(1, label=\"Node 1\", color = \"blue\")\n\n        :param n_id: The id of the node. The id is mandatory for nodes and\n                     they have to be unique. This should obviously be set per\n                     node, not globally.\n\n        :param label: The label is the piece of text shown in or under the\n                      node, depending on the shape.\n\n        :param borderWidth:\tThe width of the border of the node.\n\n        :param borderWidthSelected:\tThe width of the border of the node when\n                                    it is selected. When undefined, the\n                                    borderWidth * 2 is used.\n\n        :param brokenImage:\tWhen the shape is set to image or circularImage,\n                            this option can be an URL to a backup image in\n                            case the URL supplied in the image option cannot\n                            be resolved.\n\n        :param group: When not undefined, the node will belong to the defined\n                      group. Styling information of that group will apply to\n                      this node. Node specific styling overrides group styling.\n\n        :param hidden: When true, the node will not be shown. It will still be\n                       part of the physics simulation though!\n\n        :param image: When the shape is set to image or circularImage, this\n                      option should be the URL to an image. If the image\n                      cannot be found, the brokenImage option can be used.\n\n        :param labelHighlightBold: Determines whether or not the label becomes\n                                   bold when the node is selected.\n\n        :param level: When using the hierarchical layout, the level determines\n                      where the node is going to be positioned.\n\n        :param mass: The barnesHut physics model (which is enabled by default)\n                     is based on an inverted gravity model. By increasing\n                     the mass of a node, you increase it's repulsion. Values\n                     lower than 1 are not recommended.\n\n        :param physics:\tWhen false, the node is not part of the physics\n                        simulation. It will not move except for from\n                        manual dragging.\n\n        :param shape: The shape defines what the node looks like. There are\n                      two types of nodes. One type has the label inside of\n                      it and the other type has the label underneath it. The\n                      types with the label inside of it are: ellipse, circle,\n                      database, box, text. The ones with the label outside of\n                      it are: image, circularImage, diamond, dot, star,\n                      triangle, triangleDown, square and icon.\n\n        :param size: The size is used to determine the size of node shapes that\n                     do not have the label inside of them. These shapes are:\n                     image, circularImage, diamond, dot, star, triangle,\n                     triangleDown, square and icon.\n\n        :param title: Title to be displayed when the user hovers over the node.\n                      The title can be an HTML element or a string containing\n                      plain text or HTML.\n\n        :param value: When a value is set, the nodes will be scaled using the\n                      options in the scaling object defined above.\n\n        :param x: This gives a node an initial x position. When using the\n                  hierarchical layout, either the x or y position is set by the\n                  layout engine depending on the type of view. The other value\n                  remains untouched. When using stabilization, the stabilized\n                  position may be different from the initial one. To lock the\n                  node to that position use the physics or fixed options.\n\n        :param y: This gives a node an initial y position. When using the\n                  hierarchical layout,either the x or y position is set by\n                  the layout engine depending on the type of view. The\n                  other value remains untouched. When using stabilization,\n                  the stabilized position may be different from the initial\n                  one. To lock the node to that position use the physics or\n                  fixed options.\n\n        :type n_id: str or int\n        :type label: str or int\n        :type borderWidth: num (optional)\n        :type borderWidthSelected: num (optional)\n        :type brokenImage: str (optional)\n        :type group: str (optional)\n        :type hidden: bool (optional)\n        :type image: str (optional)\n        :type labelHighlightBold: bool (optional)\n        :type level: num (optional)\n        :type mass: num (optional)\n        :type physics: bool (optional)\n        :type shape: str (optional)\n        :type size: num (optional)\n        :type title: str or html element (optional)\n        :type value: num (optional)\n        :type x: num (optional)\n        :type y: num (optional)\n        \"\"\"\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def add_nodes(self, nodes, **kwargs):\n        \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n    def num_nodes(self):\n        \"\"\"\n        Return number of nodes\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.node_ids)\n\n    def num_edges(self):\n        \"\"\"\n        Return number of edges\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        \"\"\"\n\n        Adding edges is done based off of the IDs of the nodes. Order does\n        not matter unless dealing with a directed graph.\n\n        >>> nt.add_edge(0, 1) # adds an edge from node ID 0 to node ID\n        >>> nt.add_edge(0, 1, value = 4) # adds an edge with a width of 4\n\n\n        :param arrowStrikethrough: When false, the edge stops at the arrow.\n                                   This can be useful if you have thick lines\n                                   and you want the arrow to end in a point.\n                                   Middle arrows are not affected by this.\n\n        :param from: Edges are between two nodes, one to and one from. This\n                     is where you define the from node. You have to supply\n                     the corresponding node ID. This naturally only applies\n                     to individual edges.\n\n        :param hidden: When true, the edge is not drawn. It is part still part\n                       of the physics simulation however!\n\n        :param physics:\tWhen true, the edge is part of the physics simulation.\n                        When false, it will not act as a spring.\n\n        :param title: The title is shown in a pop-up when the mouse moves over\n                      the edge.\n\n        :param to: Edges are between two nodes, one to and one from. This is\n                   where you define the to node. You have to supply the\n                   corresponding node ID. This naturally only applies to\n                   individual edges.\n\n        :param value: When a value is set, the edges' width will be scaled\n                      using the options in the scaling object defined above.\n\n        :param width: The width of the edge. If value is set, this is not used.\n\n\n        :type arrowStrikethrough: bool\n        :type from: str or num\n        :type hidden: bool\n        :type physics: bool\n        :type title: str\n        :type to: str or num\n        :type value: num\n        :type width: num\n        \"\"\"\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        \"\"\"\n        This method serves to add multiple edges between existing nodes\n        in the network instance. Adding of the edges is done based off\n        of the IDs of the nodes. Order does not matter unless dealing with a\n        directed graph.\n\n        :param edges: A list of tuples, each tuple consists of source of edge,\n                      edge destination and and optional width.\n\n        :type arrowStrikethrough: list of tuples\n        \"\"\"\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        \"\"\"\n        Extract relevant information about this network in order to inject into\n        a Jinja2 template.\n\n        Returns:\n                nodes (list), edges (list), height (\n                    string), width (string), options (object)\n\n        Usage:\n\n        >>> nodes, edges, heading, height, width, options = net.get_network_data()\n        \"\"\"\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        \"\"\"\n        Save the graph as html in the current directory with name.\n\n        :param name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        \"\"\"\n        Writes a static HTML file and saves it locally before opening.\n\n        :param: name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        \"\"\"\n        Loads the template data into the template attribute of the network.\n        This should be done in a jupyter notebook environment before showing\n        the network.\n\n        Example:\n                >>> net.prep_notebook()\n                >>> net.show(\"nb.html\")\n\n\n        :param path: the relative path pointing to a template html file\n        :type path: string\n        \"\"\"\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        \"\"\"\n            Path to full template assumes that it exists inside of a template directory.\n            Use `set_template_dir` to set the relative template path to the template directory along with the directory location itself\n            to change both values otherwise this function will infer the results.\n            :path_to_template path: full os path string value of the template directory\n        \"\"\"\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        \"\"\"\n            Path to template directory along with the location of the template file.\n            :template_directory path: template directory\n            :template_file path: name of the template file that is going to be used to generate the html doc.\n\n        \"\"\"\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        \"\"\"\n        This method takes the contents of .DOT file and converts it\n        to a PyVis visualization.\n\n        Assuming the contents of test.dot contains:\n        digraph sample3 {\n        A -> {B ; C ; D}\n        C -> {B ; A}\n        }\n\n        Usage:\n\n        >>> nt.Network(\"500px\", \"500px\")\n        >>> nt.from_DOT(\"test.dot\")\n        >>> nt.show(\"dot.html\")\n\n        :param dot: The path of the dotfile being converted.\n        :type dot: .dot file\n\n        \"\"\"\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        \"\"\"\n        This method returns the user an adjacency list representation\n        of the network.\n\n        :returns: dictionary mapping of Node ID to list of Node IDs it\n        is connected to.\n        \"\"\"\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def neighbors(self, node):\n        \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        \"\"\"\n        This method takes an exisitng Networkx graph and translates\n        it to a PyVis graph format that can be accepted by the VisJs\n        API in the Jinja2 template. This operation is done in place.\n\n        :param nx_graph: The Networkx graph object that is to be translated.\n        :type nx_graph: networkx.Graph instance\n        :param node_size_transf: function to transform the node size for plotting\n        :type node_size_transf: func\n        :param edge_weight_transf: function to transform the edge weight for plotting\n        :type edge_weight_transf: func\n        :param default_node_size: default node size if not specified\n        :param default_edge_weight: default edge weight if not specified\n        >>> nx_graph = nx.cycle_graph(10)\n        >>> nx_graph.nodes[1]['title'] = 'Number 1'\n        >>> nx_graph.nodes[1]['group'] = 1\n        >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n        >>> nx_graph.nodes[3]['group'] = 10\n        >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n        >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n        >>> nx_graph.add_edge(20, 21, weight=5)\n        >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n        >>> nt = Network(\"500px\", \"500px\")\n        # populates the nodes and edges data structures\n        >>> nt.from_nx(nx_graph)\n        >>> nt.show(\"nx.html\")\n        \"\"\"\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        \"\"\"\n        This method returns an iterable list of node ids\n\n        :returns: list\n        \"\"\"\n        return self.node_ids\n\n    def get_node(self, n_id):\n        \"\"\"\n        Lookup node by ID and return it.\n\n        :param n_id: The ID given to the node.\n\n        :returns: dict containing node properties\n        \"\"\"\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        \"\"\"\n        This method returns an iterable list of edge objects\n\n        :returns: list\n        \"\"\"\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        \"\"\"\n        BarnesHut is a quadtree based gravity model. It is the fastest. default\n        and recommended solver for non-hierarchical layouts.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        \"\"\"\n        Set the physics attribute of the entire network to repulsion.\n        When called, it sets the solver attribute of physics to repulsion.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        \"\"\"\n        This model is based on the repulsion solver but the levels are\n        taken into account and the forces are normalized.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        \"\"\"\n        The forceAtlas2Based solver makes use of some of the equations provided\n        by them and makes use of the barnesHut implementation in vis. The main\n        differences are the central gravity model, which is here distance\n        independent, and the repulsion being linear instead of quadratic. Finally,\n        all node masses have a multiplier based on the amount of connected edges\n        plus one.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def set_edge_smooth(self, smooth_type):\n        \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n    def toggle_hide_edges_on_drag(self, status):\n        \"\"\"\n        Displays or hides edges while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: True if edges should be hidden on drag\n        \n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        \"\"\"\n        Displays or hides nodes while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: When set to True, the nodes will hide on drag.\n                       Default is set to False.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        \"\"\"\n        Edges take on the color of the node they are coming from.\n\n        :param status: True if edges should adopt color coming from.\n        :type status: bool\n        \"\"\"\n        self.options.edges.inherit_colors(status)\n\n    def toggle_physics(self, status):\n        \"\"\"\n        Toggles physics simulation \n\n        :param status: When False, nodes are not part of the physics\n                       simulation. They will not move except for from\n                       manual dragging.\n                       Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        \"\"\"\n        Toggles the dragging of the nodes in the network.\n\n        :param status: When set to True, the nodes can be dragged around\n                       in the network. Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        \"\"\"\n        Toggles the stablization of the network.\n\n        :param status: Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        \"\"\"\n        Overrides the default options object passed to the VisJS framework.\n        Delegates to the :meth:`options.Options.set` routine.\n\n        :param options: The string representation of the Javascript-like object\n                        to be used to override default options.\n        \n        :type options: str\n        \"\"\"\n        self.options = self.options.set(options)\n\n    def show_buttons(self, filter_=None):\n        \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n```\n",
        "eval_script": "# network.py\n\nimport os\nimport json\nfrom collections import defaultdict\nimport shutil\nimport tempfile\nfrom jinja2 import Environment, FileSystemLoader\n\nclass Options:\n    def __init__(self, layout=None):\n        self.layout = layout\n        self.configure = None\n        self.physics = PhysicsOptions()\n        self.interaction = InteractionOptions()\n        self.edges = EdgeOptions()\n\n    def set(self, options):\n        return self\n\n    def to_json(self):\n        return json.dumps(self.__dict__)\n\nclass Configure:\n    def __init__(self, enabled=True, filter_=None):\n        self.enabled = enabled\n        self.filter_ = filter_\n\nclass PhysicsOptions:\n    def use_barnes_hut(self, params):\n        pass\n\n    def use_repulsion(self, params):\n        pass\n\n    def use_hrepulsion(self, params):\n        pass\n\n    def use_force_atlas_2based(self, params):\n        pass\n\n    def toggle_stabilization(self, status):\n        pass\n\n    @property\n    def enabled(self):\n        return True\n\n    @enabled.setter\n    def enabled(self, value):\n        pass\n\nclass InteractionOptions:\n    def __init__(self):\n        self.hideEdgesOnDrag = False\n        self.hideNodesOnDrag = False\n        self.dragNodes = True\n\nclass EdgeOptions:\n    def __init__(self):\n        self.smooth = SmoothOptions()\n\n    def inherit_colors(self, status):\n        pass\n\nclass SmoothOptions:\n    def __init__(self):\n        self.enabled = False\n        self.type = None\n\nclass Network(object):\n    \"\"\"\n    The Network class is the focus of this library. All viz functionality\n    should be implemented off of a Network instance.\n\n    To instantiate:\n\n    >>> nt = Network()\n    \"\"\"\n\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        \"\"\"\n        :param height: The height of the canvas\n        :param width: The width of the canvas\n        :param directed: Whether or not to use a directed graph. This is false\n                         by default.\n        :param notebook: True if using jupyter notebook.\n        :param select_menu: sets the option to highlight nodes and the neighborhood\n        :param filter_menu: sets the option to filter nodes and edges based on attributes\n        :param bgcolor: The background color of the canvas.\n        :param cdn_resources: Where to pull resources for css and js files. Defaults to local.\n            Options ['local','in_line','remote'].\n            local: pull resources from local lib folder.\n            in_line: insert lib resources as inline script tags.\n            remote: pull resources from hash checked cdns.\n        :font_color: The color of the node labels text\n        :layout: Use hierarchical layout if this is set\n\n        :type height: num or str\n        :type width: num or str\n        :type directed: bool\n        :type notebook: bool\n        :type select_menu: bool\n        :type filter_menu: bool\n        :type bgcolor: str\n        :type font_color: str\n        :type layout: bool\n        :type cdn_resources: str\n        \"\"\"\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        \"\"\"\n        override print to show readable graph data\n        \"\"\"\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        \"\"\"\n        This method adds a node to the network, given a mandatory node ID.\n        Node labels default to node ids if no label is specified during the\n        call.\n\n        >>> nt = Network(\"500px\", \"500px\")\n        >>> nt.add_node(0, label=\"Node 0\")\n        >>> nt.add_node(1, label=\"Node 1\", color = \"blue\")\n\n        :param n_id: The id of the node. The id is mandatory for nodes and\n                     they have to be unique. This should obviously be set per\n                     node, not globally.\n\n        :param label: The label is the piece of text shown in or under the\n                      node, depending on the shape.\n\n        :param borderWidth:\tThe width of the border of the node.\n\n        :param borderWidthSelected:\tThe width of the border of the node when\n                                    it is selected. When undefined, the\n                                    borderWidth * 2 is used.\n\n        :param brokenImage:\tWhen the shape is set to image or circularImage,\n                            this option can be an URL to a backup image in\n                            case the URL supplied in the image option cannot\n                            be resolved.\n\n        :param group: When not undefined, the node will belong to the defined\n                      group. Styling information of that group will apply to\n                      this node. Node specific styling overrides group styling.\n\n        :param hidden: When true, the node will not be shown. It will still be\n                       part of the physics simulation though!\n\n        :param image: When the shape is set to image or circularImage, this\n                      option should be the URL to an image. If the image\n                      cannot be found, the brokenImage option can be used.\n\n        :param labelHighlightBold: Determines whether or not the label becomes\n                                   bold when the node is selected.\n\n        :param level: When using the hierarchical layout, the level determines\n                      where the node is going to be positioned.\n\n        :param mass: The barnesHut physics model (which is enabled by default)\n                     is based on an inverted gravity model. By increasing\n                     the mass of a node, you increase it's repulsion. Values\n                     lower than 1 are not recommended.\n\n        :param physics:\tWhen false, the node is not part of the physics\n                        simulation. It will not move except for from\n                        manual dragging.\n\n        :param shape: The shape defines what the node looks like. There are\n                      two types of nodes. One type has the label inside of\n                      it and the other type has the label underneath it. The\n                      types with the label inside of it are: ellipse, circle,\n                      database, box, text. The ones with the label outside of\n                      it are: image, circularImage, diamond, dot, star,\n                      triangle, triangleDown, square and icon.\n\n        :param size: The size is used to determine the size of node shapes that\n                     do not have the label inside of them. These shapes are:\n                     image, circularImage, diamond, dot, star, triangle,\n                     triangleDown, square and icon.\n\n        :param title: Title to be displayed when the user hovers over the node.\n                      The title can be an HTML element or a string containing\n                      plain text or HTML.\n\n        :param value: When a value is set, the nodes will be scaled using the\n                      options in the scaling object defined above.\n\n        :param x: This gives a node an initial x position. When using the\n                  hierarchical layout, either the x or y position is set by the\n                  layout engine depending on the type of view. The other value\n                  remains untouched. When using stabilization, the stabilized\n                  position may be different from the initial one. To lock the\n                  node to that position use the physics or fixed options.\n\n        :param y: This gives a node an initial y position. When using the\n                  hierarchical layout,either the x or y position is set by\n                  the layout engine depending on the type of view. The\n                  other value remains untouched. When using stabilization,\n                  the stabilized position may be different from the initial\n                  one. To lock the node to that position use the physics or\n                  fixed options.\n\n        :type n_id: str or int\n        :type label: str or int\n        :type borderWidth: num (optional)\n        :type borderWidthSelected: num (optional)\n        :type brokenImage: str (optional)\n        :type group: str (optional)\n        :type hidden: bool (optional)\n        :type image: str (optional)\n        :type labelHighlightBold: bool (optional)\n        :type level: num (optional)\n        :type mass: num (optional)\n        :type physics: bool (optional)\n        :type shape: str (optional)\n        :type size: num (optional)\n        :type title: str or html element (optional)\n        :type value: num (optional)\n        :type x: num (optional)\n        :type y: num (optional)\n        \"\"\"\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def add_nodes(self, nodes, **kwargs):\n        \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n    def num_nodes(self):\n        \"\"\"\n        Return number of nodes\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.node_ids)\n\n    def num_edges(self):\n        \"\"\"\n        Return number of edges\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        \"\"\"\n\n        Adding edges is done based off of the IDs of the nodes. Order does\n        not matter unless dealing with a directed graph.\n\n        >>> nt.add_edge(0, 1) # adds an edge from node ID 0 to node ID\n        >>> nt.add_edge(0, 1, value = 4) # adds an edge with a width of 4\n\n\n        :param arrowStrikethrough: When false, the edge stops at the arrow.\n                                   This can be useful if you have thick lines\n                                   and you want the arrow to end in a point.\n                                   Middle arrows are not affected by this.\n\n        :param from: Edges are between two nodes, one to and one from. This\n                     is where you define the from node. You have to supply\n                     the corresponding node ID. This naturally only applies\n                     to individual edges.\n\n        :param hidden: When true, the edge is not drawn. It is part still part\n                       of the physics simulation however!\n\n        :param physics:\tWhen true, the edge is part of the physics simulation.\n                        When false, it will not act as a spring.\n\n        :param title: The title is shown in a pop-up when the mouse moves over\n                      the edge.\n\n        :param to: Edges are between two nodes, one to and one from. This is\n                   where you define the to node. You have to supply the\n                   corresponding node ID. This naturally only applies to\n                   individual edges.\n\n        :param value: When a value is set, the edges' width will be scaled\n                      using the options in the scaling object defined above.\n\n        :param width: The width of the edge. If value is set, this is not used.\n\n\n        :type arrowStrikethrough: bool\n        :type from: str or num\n        :type hidden: bool\n        :type physics: bool\n        :type title: str\n        :type to: str or num\n        :type value: num\n        :type width: num\n        \"\"\"\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        \"\"\"\n        This method serves to add multiple edges between existing nodes\n        in the network instance. Adding of the edges is done based off\n        of the IDs of the nodes. Order does not matter unless dealing with a\n        directed graph.\n\n        :param edges: A list of tuples, each tuple consists of source of edge,\n                      edge destination and and optional width.\n\n        :type arrowStrikethrough: list of tuples\n        \"\"\"\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        \"\"\"\n        Extract relevant information about this network in order to inject into\n        a Jinja2 template.\n\n        Returns:\n                nodes (list), edges (list), height (\n                    string), width (string), options (object)\n\n        Usage:\n\n        >>> nodes, edges, heading, height, width, options = net.get_network_data()\n        \"\"\"\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        \"\"\"\n        Save the graph as html in the current directory with name.\n\n        :param name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        \"\"\"\n        Writes a static HTML file and saves it locally before opening.\n\n        :param: name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        \"\"\"\n        Loads the template data into the template attribute of the network.\n        This should be done in a jupyter notebook environment before showing\n        the network.\n\n        Example:\n                >>> net.prep_notebook()\n                >>> net.show(\"nb.html\")\n\n\n        :param path: the relative path pointing to a template html file\n        :type path: string\n        \"\"\"\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        \"\"\"\n            Path to full template assumes that it exists inside of a template directory.\n            Use `set_template_dir` to set the relative template path to the template directory along with the directory location itself\n            to change both values otherwise this function will infer the results.\n            :path_to_template path: full os path string value of the template directory\n        \"\"\"\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        \"\"\"\n            Path to template directory along with the location of the template file.\n            :template_directory path: template directory\n            :template_file path: name of the template file that is going to be used to generate the html doc.\n\n        \"\"\"\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        \"\"\"\n        This method takes the contents of .DOT file and converts it\n        to a PyVis visualization.\n\n        Assuming the contents of test.dot contains:\n        digraph sample3 {\n        A -> {B ; C ; D}\n        C -> {B ; A}\n        }\n\n        Usage:\n\n        >>> nt.Network(\"500px\", \"500px\")\n        >>> nt.from_DOT(\"test.dot\")\n        >>> nt.show(\"dot.html\")\n\n        :param dot: The path of the dotfile being converted.\n        :type dot: .dot file\n\n        \"\"\"\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        \"\"\"\n        This method returns the user an adjacency list representation\n        of the network.\n\n        :returns: dictionary mapping of Node ID to list of Node IDs it\n        is connected to.\n        \"\"\"\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def neighbors(self, node):\n        \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        \"\"\"\n        This method takes an exisitng Networkx graph and translates\n        it to a PyVis graph format that can be accepted by the VisJs\n        API in the Jinja2 template. This operation is done in place.\n\n        :param nx_graph: The Networkx graph object that is to be translated.\n        :type nx_graph: networkx.Graph instance\n        :param node_size_transf: function to transform the node size for plotting\n        :type node_size_transf: func\n        :param edge_weight_transf: function to transform the edge weight for plotting\n        :type edge_weight_transf: func\n        :param default_node_size: default node size if not specified\n        :param default_edge_weight: default edge weight if not specified\n        >>> nx_graph = nx.cycle_graph(10)\n        >>> nx_graph.nodes[1]['title'] = 'Number 1'\n        >>> nx_graph.nodes[1]['group'] = 1\n        >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n        >>> nx_graph.nodes[3]['group'] = 10\n        >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n        >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n        >>> nx_graph.add_edge(20, 21, weight=5)\n        >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n        >>> nt = Network(\"500px\", \"500px\")\n        # populates the nodes and edges data structures\n        >>> nt.from_nx(nx_graph)\n        >>> nt.show(\"nx.html\")\n        \"\"\"\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        \"\"\"\n        This method returns an iterable list of node ids\n\n        :returns: list\n        \"\"\"\n        return self.node_ids\n\n    def get_node(self, n_id):\n        \"\"\"\n        Lookup node by ID and return it.\n\n        :param n_id: The ID given to the node.\n\n        :returns: dict containing node properties\n        \"\"\"\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        \"\"\"\n        This method returns an iterable list of edge objects\n\n        :returns: list\n        \"\"\"\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        \"\"\"\n        BarnesHut is a quadtree based gravity model. It is the fastest. default\n        and recommended solver for non-hierarchical layouts.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        \"\"\"\n        Set the physics attribute of the entire network to repulsion.\n        When called, it sets the solver attribute of physics to repulsion.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        \"\"\"\n        This model is based on the repulsion solver but the levels are\n        taken into account and the forces are normalized.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        \"\"\"\n        The forceAtlas2Based solver makes use of some of the equations provided\n        by them and makes use of the barnesHut implementation in vis. The main\n        differences are the central gravity model, which is here distance\n        independent, and the repulsion being linear instead of quadratic. Finally,\n        all node masses have a multiplier based on the amount of connected edges\n        plus one.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def set_edge_smooth(self, smooth_type):\n        \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n    def toggle_hide_edges_on_drag(self, status):\n        \"\"\"\n        Displays or hides edges while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: True if edges should be hidden on drag\n        \n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        \"\"\"\n        Displays or hides nodes while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: When set to True, the nodes will hide on drag.\n                       Default is set to False.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        \"\"\"\n        Edges take on the color of the node they are coming from.\n\n        :param status: True if edges should adopt color coming from.\n        :type status: bool\n        \"\"\"\n        self.options.edges.inherit_colors(status)\n\n    def toggle_physics(self, status):\n        \"\"\"\n        Toggles physics simulation \n\n        :param status: When False, nodes are not part of the physics\n                       simulation. They will not move except for from\n                       manual dragging.\n                       Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        \"\"\"\n        Toggles the dragging of the nodes in the network.\n\n        :param status: When set to True, the nodes can be dragged around\n                       in the network. Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        \"\"\"\n        Toggles the stablization of the network.\n\n        :param status: Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        \"\"\"\n        Overrides the default options object passed to the VisJS framework.\n        Delegates to the :meth:`options.Options.set` routine.\n\n        :param options: The string representation of the Javascript-like object\n                        to be used to override default options.\n        \n        :type options: str\n        \"\"\"\n        self.options = self.options.set(options)\n\n    def show_buttons(self, filter_=None):\n        \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n\ndef test_show_buttons():\n    # Create two Network objects.\n    network1 = Network()\n    network2 = Network()\n\n    # Use the original show_buttons method on network1.\n    network1.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n    # Use the new implementation on network2.\n    network2.show_buttons_new_implementation(filter_=['nodes', 'edges', 'physics'])\n\n    # Assert that both networks have the same configuration.\n    assert network1.conf == network2.conf\n    assert network1.options.configure.enabled == network2.options.configure.enabled\n    assert network1.options.configure.filter_ == network2.options.configure.filter_\n    assert network1.widget == network2.widget\n\n    # Additional comparison with empty filter.\n    network1.show_buttons()\n    network2.show_buttons_new_implementation()\n\n    # Assertions for the second comparison.\n    assert network1.conf == network2.conf\n    assert network1.options.configure.enabled == network2.options.configure.enabled\n    assert network1.widget == network2.widget\n\nif __name__ == \"__main__\":\n    test_show_buttons()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "Network.neighbors",
        "idx": "106",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "network.py",
        "orig_func": "def neighbors(self, node):\n    \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n    assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n    assert node in self.node_ids, 'error: %s node not in network' % node\n    return self.get_adj_list()[node]",
        "orig_context": "```python\n# network.py\n\nclass Network(object):\n    \"\"\"\n    The Network class is the focus of this library. All viz functionality\n    should be implemented off of a Network instance.\n\n    To instantiate:\n\n    >>> nt = Network()\n    \"\"\"\n\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        \"\"\"\n        :param height: The height of the canvas\n        :param width: The width of the canvas\n        :param directed: Whether or not to use a directed graph. This is false\n                         by default.\n        :param notebook: True if using jupyter notebook.\n        :param select_menu: sets the option to highlight nodes and the neighborhood\n        :param filter_menu: sets the option to filter nodes and edges based on attributes\n        :param bgcolor: The background color of the canvas.\n        :param cdn_resources: Where to pull resources for css and js files. Defaults to local.\n            Options ['local','in_line','remote'].\n            local: pull resources from local lib folder.\n            in_line: insert lib resources as inline script tags.\n            remote: pull resources from hash checked cdns.\n        :font_color: The color of the node labels text\n        :layout: Use hierarchical layout if this is set\n\n        :type height: num or str\n        :type width: num or str\n        :type directed: bool\n        :type notebook: bool\n        :type select_menu: bool\n        :type filter_menu: bool\n        :type bgcolor: str\n        :type font_color: str\n        :type layout: bool\n        :type cdn_resources: str\n        \"\"\"\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        \"\"\"\n        override print to show readable graph data\n        \"\"\"\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        \"\"\"\n        This method adds a node to the network, given a mandatory node ID.\n        Node labels default to node ids if no label is specified during the\n        call.\n\n        >>> nt = Network(\"500px\", \"500px\")\n        >>> nt.add_node(0, label=\"Node 0\")\n        >>> nt.add_node(1, label=\"Node 1\", color = \"blue\")\n\n        :param n_id: The id of the node. The id is mandatory for nodes and\n                     they have to be unique. This should obviously be set per\n                     node, not globally.\n\n        :param label: The label is the piece of text shown in or under the\n                      node, depending on the shape.\n\n        :param borderWidth:\tThe width of the border of the node.\n\n        :param borderWidthSelected:\tThe width of the border of the node when\n                                    it is selected. When undefined, the\n                                    borderWidth * 2 is used.\n\n        :param brokenImage:\tWhen the shape is set to image or circularImage,\n                            this option can be an URL to a backup image in\n                            case the URL supplied in the image option cannot\n                            be resolved.\n\n        :param group: When not undefined, the node will belong to the defined\n                      group. Styling information of that group will apply to\n                      this node. Node specific styling overrides group styling.\n\n        :param hidden: When true, the node will not be shown. It will still be\n                       part of the physics simulation though!\n\n        :param image: When the shape is set to image or circularImage, this\n                      option should be the URL to an image. If the image\n                      cannot be found, the brokenImage option can be used.\n\n        :param labelHighlightBold: Determines whether or not the label becomes\n                                   bold when the node is selected.\n\n        :param level: When using the hierarchical layout, the level determines\n                      where the node is going to be positioned.\n\n        :param mass: The barnesHut physics model (which is enabled by default)\n                     is based on an inverted gravity model. By increasing\n                     the mass of a node, you increase it's repulsion. Values\n                     lower than 1 are not recommended.\n\n        :param physics:\tWhen false, the node is not part of the physics\n                        simulation. It will not move except for from\n                        manual dragging.\n\n        :param shape: The shape defines what the node looks like. There are\n                      two types of nodes. One type has the label inside of\n                      it and the other type has the label underneath it. The\n                      types with the label inside of it are: ellipse, circle,\n                      database, box, text. The ones with the label outside of\n                      it are: image, circularImage, diamond, dot, star,\n                      triangle, triangleDown, square and icon.\n\n        :param size: The size is used to determine the size of node shapes that\n                     do not have the label inside of them. These shapes are:\n                     image, circularImage, diamond, dot, star, triangle,\n                     triangleDown, square and icon.\n\n        :param title: Title to be displayed when the user hovers over the node.\n                      The title can be an HTML element or a string containing\n                      plain text or HTML.\n\n        :param value: When a value is set, the nodes will be scaled using the\n                      options in the scaling object defined above.\n\n        :param x: This gives a node an initial x position. When using the\n                  hierarchical layout, either the x or y position is set by the\n                  layout engine depending on the type of view. The other value\n                  remains untouched. When using stabilization, the stabilized\n                  position may be different from the initial one. To lock the\n                  node to that position use the physics or fixed options.\n\n        :param y: This gives a node an initial y position. When using the\n                  hierarchical layout,either the x or y position is set by\n                  the layout engine depending on the type of view. The\n                  other value remains untouched. When using stabilization,\n                  the stabilized position may be different from the initial\n                  one. To lock the node to that position use the physics or\n                  fixed options.\n\n        :type n_id: str or int\n        :type label: str or int\n        :type borderWidth: num (optional)\n        :type borderWidthSelected: num (optional)\n        :type brokenImage: str (optional)\n        :type group: str (optional)\n        :type hidden: bool (optional)\n        :type image: str (optional)\n        :type labelHighlightBold: bool (optional)\n        :type level: num (optional)\n        :type mass: num (optional)\n        :type physics: bool (optional)\n        :type shape: str (optional)\n        :type size: num (optional)\n        :type title: str or html element (optional)\n        :type value: num (optional)\n        :type x: num (optional)\n        :type y: num (optional)\n        \"\"\"\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def add_nodes(self, nodes, **kwargs):\n        \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n    def num_nodes(self):\n        \"\"\"\n        Return number of nodes\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.node_ids)\n\n    def num_edges(self):\n        \"\"\"\n        Return number of edges\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        \"\"\"\n\n        Adding edges is done based off of the IDs of the nodes. Order does\n        not matter unless dealing with a directed graph.\n\n        >>> nt.add_edge(0, 1) # adds an edge from node ID 0 to node ID\n        >>> nt.add_edge(0, 1, value = 4) # adds an edge with a width of 4\n\n\n        :param arrowStrikethrough: When false, the edge stops at the arrow.\n                                   This can be useful if you have thick lines\n                                   and you want the arrow to end in a point.\n                                   Middle arrows are not affected by this.\n\n        :param from: Edges are between two nodes, one to and one from. This\n                     is where you define the from node. You have to supply\n                     the corresponding node ID. This naturally only applies\n                     to individual edges.\n\n        :param hidden: When true, the edge is not drawn. It is part still part\n                       of the physics simulation however!\n\n        :param physics:\tWhen true, the edge is part of the physics simulation.\n                        When false, it will not act as a spring.\n\n        :param title: The title is shown in a pop-up when the mouse moves over\n                      the edge.\n\n        :param to: Edges are between two nodes, one to and one from. This is\n                   where you define the to node. You have to supply the\n                   corresponding node ID. This naturally only applies to\n                   individual edges.\n\n        :param value: When a value is set, the edges' width will be scaled\n                      using the options in the scaling object defined above.\n\n        :param width: The width of the edge. If value is set, this is not used.\n\n\n        :type arrowStrikethrough: bool\n        :type from: str or num\n        :type hidden: bool\n        :type physics: bool\n        :type title: str\n        :type to: str or num\n        :type value: num\n        :type width: num\n        \"\"\"\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        \"\"\"\n        This method serves to add multiple edges between existing nodes\n        in the network instance. Adding of the edges is done based off\n        of the IDs of the nodes. Order does not matter unless dealing with a\n        directed graph.\n\n        :param edges: A list of tuples, each tuple consists of source of edge,\n                      edge destination and and optional width.\n\n        :type arrowStrikethrough: list of tuples\n        \"\"\"\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        \"\"\"\n        Extract relevant information about this network in order to inject into\n        a Jinja2 template.\n\n        Returns:\n                nodes (list), edges (list), height (\n                    string), width (string), options (object)\n\n        Usage:\n\n        >>> nodes, edges, heading, height, width, options = net.get_network_data()\n        \"\"\"\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        \"\"\"\n        Save the graph as html in the current directory with name.\n\n        :param name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        \"\"\"\n        Writes a static HTML file and saves it locally before opening.\n\n        :param: name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        \"\"\"\n        Loads the template data into the template attribute of the network.\n        This should be done in a jupyter notebook environment before showing\n        the network.\n\n        Example:\n                >>> net.prep_notebook()\n                >>> net.show(\"nb.html\")\n\n\n        :param path: the relative path pointing to a template html file\n        :type path: string\n        \"\"\"\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        \"\"\"\n            Path to full template assumes that it exists inside of a template directory.\n            Use `set_template_dir` to set the relative template path to the template directory along with the directory location itself\n            to change both values otherwise this function will infer the results.\n            :path_to_template path: full os path string value of the template directory\n        \"\"\"\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        \"\"\"\n            Path to template directory along with the location of the template file.\n            :template_directory path: template directory\n            :template_file path: name of the template file that is going to be used to generate the html doc.\n\n        \"\"\"\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        \"\"\"\n        This method takes the contents of .DOT file and converts it\n        to a PyVis visualization.\n\n        Assuming the contents of test.dot contains:\n        digraph sample3 {\n        A -> {B ; C ; D}\n        C -> {B ; A}\n        }\n\n        Usage:\n\n        >>> nt.Network(\"500px\", \"500px\")\n        >>> nt.from_DOT(\"test.dot\")\n        >>> nt.show(\"dot.html\")\n\n        :param dot: The path of the dotfile being converted.\n        :type dot: .dot file\n\n        \"\"\"\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        \"\"\"\n        This method returns the user an adjacency list representation\n        of the network.\n\n        :returns: dictionary mapping of Node ID to list of Node IDs it\n        is connected to.\n        \"\"\"\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        \"\"\"\n        This method takes an exisitng Networkx graph and translates\n        it to a PyVis graph format that can be accepted by the VisJs\n        API in the Jinja2 template. This operation is done in place.\n\n        :param nx_graph: The Networkx graph object that is to be translated.\n        :type nx_graph: networkx.Graph instance\n        :param node_size_transf: function to transform the node size for plotting\n        :type node_size_transf: func\n        :param edge_weight_transf: function to transform the edge weight for plotting\n        :type edge_weight_transf: func\n        :param default_node_size: default node size if not specified\n        :param default_edge_weight: default edge weight if not specified\n        >>> nx_graph = nx.cycle_graph(10)\n        >>> nx_graph.nodes[1]['title'] = 'Number 1'\n        >>> nx_graph.nodes[1]['group'] = 1\n        >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n        >>> nx_graph.nodes[3]['group'] = 10\n        >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n        >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n        >>> nx_graph.add_edge(20, 21, weight=5)\n        >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n        >>> nt = Network(\"500px\", \"500px\")\n        # populates the nodes and edges data structures\n        >>> nt.from_nx(nx_graph)\n        >>> nt.show(\"nx.html\")\n        \"\"\"\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        \"\"\"\n        This method returns an iterable list of node ids\n\n        :returns: list\n        \"\"\"\n        return self.node_ids\n\n    def get_node(self, n_id):\n        \"\"\"\n        Lookup node by ID and return it.\n\n        :param n_id: The ID given to the node.\n\n        :returns: dict containing node properties\n        \"\"\"\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        \"\"\"\n        This method returns an iterable list of edge objects\n\n        :returns: list\n        \"\"\"\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        \"\"\"\n        BarnesHut is a quadtree based gravity model. It is the fastest. default\n        and recommended solver for non-hierarchical layouts.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        \"\"\"\n        Set the physics attribute of the entire network to repulsion.\n        When called, it sets the solver attribute of physics to repulsion.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        \"\"\"\n        This model is based on the repulsion solver but the levels are\n        taken into account and the forces are normalized.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        \"\"\"\n        The forceAtlas2Based solver makes use of some of the equations provided\n        by them and makes use of the barnesHut implementation in vis. The main\n        differences are the central gravity model, which is here distance\n        independent, and the repulsion being linear instead of quadratic. Finally,\n        all node masses have a multiplier based on the amount of connected edges\n        plus one.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def set_edge_smooth(self, smooth_type):\n        \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n    def toggle_hide_edges_on_drag(self, status):\n        \"\"\"\n        Displays or hides edges while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: True if edges should be hidden on drag\n        \n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        \"\"\"\n        Displays or hides nodes while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: When set to True, the nodes will hide on drag.\n                       Default is set to False.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        \"\"\"\n        Edges take on the color of the node they are coming from.\n\n        :param status: True if edges should adopt color coming from.\n        :type status: bool\n        \"\"\"\n        self.options.edges.inherit_colors(status)\n\n    def show_buttons(self, filter_=None):\n        \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n    def toggle_physics(self, status):\n        \"\"\"\n        Toggles physics simulation \n\n        :param status: When False, nodes are not part of the physics\n                       simulation. They will not move except for from\n                       manual dragging.\n                       Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        \"\"\"\n        Toggles the dragging of the nodes in the network.\n\n        :param status: When set to True, the nodes can be dragged around\n                       in the network. Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        \"\"\"\n        Toggles the stablization of the network.\n\n        :param status: Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        \"\"\"\n        Overrides the default options object passed to the VisJS framework.\n        Delegates to the :meth:`options.Options.set` routine.\n\n        :param options: The string representation of the Javascript-like object\n                        to be used to override default options.\n        \n        :type options: str\n        \"\"\"\n        self.options = self.options.set(options)\n\n    def neighbors(self, node):\n        \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n```\n",
        "eval_script": "import json\nimport os\nfrom jinja2 import Environment, FileSystemLoader\nfrom collections import defaultdict\n\n# Mock implementations for classes and functions not provided\nclass IFrame:  # Mock implementation\n    def __init__(self, name, width, height):\n        pass\n\ndef check_html(name):  # Mock implementation\n    pass\n\nclass Node:  # Mock implementation\n    def __init__(self, n_id, shape, label, font_color=None, **options):\n        self.options = {'id': n_id, 'shape': shape, 'label': label, 'font_color': font_color}\n\nclass Edge:  # Mock implementation\n    def __init__(self, source, to, directed, **options):\n        self.options = {'from': source, 'to': to, **options}\n\nclass Options:  # Mock implementation\n    def __init__(self, layout):\n        self.layout = layout\n        self.physics = Options.Physics()\n        self.edges = Options.Edges()\n        self.interaction = Options.Interaction()\n        \n    class Physics:\n        def use_barnes_hut(self, params): pass\n        def use_repulsion(self, params): pass\n        def use_hrepulsion(self, params): pass\n        def use_force_atlas_2based(self, params): pass\n        def toggle_stabilization(self, status): pass\n    \n    class Edges:\n        def __init__(self):\n            self.smooth = Options.Edges.Smooth()  # Correct the scope of Smooth\n        \n        def inherit_colors(self, status): pass\n\n        class Smooth:\n            def __init__(self):\n                self.enabled = None\n                self.type = None\n    \n    class Interaction:\n        def __init__(self):\n            self.dragNodes = None\n            self.hideEdgesOnDrag = None\n            self.hideNodesOnDrag = None\n\n    def set(self, options): return self\n\nclass Configure:  \n    def __init__(self, enabled, filter_=None):\n        pass\n\nclass Network(object):\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def add_nodes(self, nodes, **kwargs):\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n    def num_nodes(self):\n        return len(self.node_ids)\n\n    def num_edges(self):\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        return self.node_ids\n\n    def get_node(self, n_id):\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def set_edge_smooth(self, smooth_type):\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n    def toggle_hide_edges_on_drag(self, status):\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        self.options.edges.inherit_colors(status)\n\n    def show_buttons(self, filter_=None):\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n    def toggle_physics(self, status):\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        self.options = self.options.set(options)\n\n    def neighbors(self, node):\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n\n    # New implementation of neighbors (mocked to be the same as neighbors for test purposes)\n\n\ndef test_neighbors():\n    # Create a Network\n    net = Network()\n\n    # Add nodes\n    net.add_node(1)\n    net.add_node(2)\n    net.add_node(3)\n    net.add_node(4)\n\n    # Add edges\n    net.add_edge(1, 2)\n    net.add_edge(2, 3)\n    net.add_edge(3, 4)\n    \n    # Test neighbors\n    assert net.neighbors(1) == net.neighbors_new_implementation(1)\n    assert net.neighbors(2) == net.neighbors_new_implementation(2)\n    assert net.neighbors(3) == net.neighbors_new_implementation(3)\n\nif __name__ == '__main__':\n    test_neighbors()"
    },
    {
        "func_name": "Network.set_edge_smooth",
        "idx": "111",
        "repo_name": "controlecidadao___sinarc",
        "func_path": "network.py",
        "orig_func": "def set_edge_smooth(self, smooth_type):\n    \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n    self.options.edges.smooth.enabled = True\n    self.options.edges.smooth.type = smooth_type",
        "orig_context": "```python\n# network.py\n\nclass Network(object):\n    \"\"\"\n    The Network class is the focus of this library. All viz functionality\n    should be implemented off of a Network instance.\n\n    To instantiate:\n\n    >>> nt = Network()\n    \"\"\"\n\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        \"\"\"\n        :param height: The height of the canvas\n        :param width: The width of the canvas\n        :param directed: Whether or not to use a directed graph. This is false\n                         by default.\n        :param notebook: True if using jupyter notebook.\n        :param select_menu: sets the option to highlight nodes and the neighborhood\n        :param filter_menu: sets the option to filter nodes and edges based on attributes\n        :param bgcolor: The background color of the canvas.\n        :param cdn_resources: Where to pull resources for css and js files. Defaults to local.\n            Options ['local','in_line','remote'].\n            local: pull resources from local lib folder.\n            in_line: insert lib resources as inline script tags.\n            remote: pull resources from hash checked cdns.\n        :font_color: The color of the node labels text\n        :layout: Use hierarchical layout if this is set\n\n        :type height: num or str\n        :type width: num or str\n        :type directed: bool\n        :type notebook: bool\n        :type select_menu: bool\n        :type filter_menu: bool\n        :type bgcolor: str\n        :type font_color: str\n        :type layout: bool\n        :type cdn_resources: str\n        \"\"\"\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        \"\"\"\n        override print to show readable graph data\n        \"\"\"\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        \"\"\"\n        This method adds a node to the network, given a mandatory node ID.\n        Node labels default to node ids if no label is specified during the\n        call.\n\n        >>> nt = Network(\"500px\", \"500px\")\n        >>> nt.add_node(0, label=\"Node 0\")\n        >>> nt.add_node(1, label=\"Node 1\", color = \"blue\")\n\n        :param n_id: The id of the node. The id is mandatory for nodes and\n                     they have to be unique. This should obviously be set per\n                     node, not globally.\n\n        :param label: The label is the piece of text shown in or under the\n                      node, depending on the shape.\n\n        :param borderWidth:\tThe width of the border of the node.\n\n        :param borderWidthSelected:\tThe width of the border of the node when\n                                    it is selected. When undefined, the\n                                    borderWidth * 2 is used.\n\n        :param brokenImage:\tWhen the shape is set to image or circularImage,\n                            this option can be an URL to a backup image in\n                            case the URL supplied in the image option cannot\n                            be resolved.\n\n        :param group: When not undefined, the node will belong to the defined\n                      group. Styling information of that group will apply to\n                      this node. Node specific styling overrides group styling.\n\n        :param hidden: When true, the node will not be shown. It will still be\n                       part of the physics simulation though!\n\n        :param image: When the shape is set to image or circularImage, this\n                      option should be the URL to an image. If the image\n                      cannot be found, the brokenImage option can be used.\n\n        :param labelHighlightBold: Determines whether or not the label becomes\n                                   bold when the node is selected.\n\n        :param level: When using the hierarchical layout, the level determines\n                      where the node is going to be positioned.\n\n        :param mass: The barnesHut physics model (which is enabled by default)\n                     is based on an inverted gravity model. By increasing\n                     the mass of a node, you increase it's repulsion. Values\n                     lower than 1 are not recommended.\n\n        :param physics:\tWhen false, the node is not part of the physics\n                        simulation. It will not move except for from\n                        manual dragging.\n\n        :param shape: The shape defines what the node looks like. There are\n                      two types of nodes. One type has the label inside of\n                      it and the other type has the label underneath it. The\n                      types with the label inside of it are: ellipse, circle,\n                      database, box, text. The ones with the label outside of\n                      it are: image, circularImage, diamond, dot, star,\n                      triangle, triangleDown, square and icon.\n\n        :param size: The size is used to determine the size of node shapes that\n                     do not have the label inside of them. These shapes are:\n                     image, circularImage, diamond, dot, star, triangle,\n                     triangleDown, square and icon.\n\n        :param title: Title to be displayed when the user hovers over the node.\n                      The title can be an HTML element or a string containing\n                      plain text or HTML.\n\n        :param value: When a value is set, the nodes will be scaled using the\n                      options in the scaling object defined above.\n\n        :param x: This gives a node an initial x position. When using the\n                  hierarchical layout, either the x or y position is set by the\n                  layout engine depending on the type of view. The other value\n                  remains untouched. When using stabilization, the stabilized\n                  position may be different from the initial one. To lock the\n                  node to that position use the physics or fixed options.\n\n        :param y: This gives a node an initial y position. When using the\n                  hierarchical layout,either the x or y position is set by\n                  the layout engine depending on the type of view. The\n                  other value remains untouched. When using stabilization,\n                  the stabilized position may be different from the initial\n                  one. To lock the node to that position use the physics or\n                  fixed options.\n\n        :type n_id: str or int\n        :type label: str or int\n        :type borderWidth: num (optional)\n        :type borderWidthSelected: num (optional)\n        :type brokenImage: str (optional)\n        :type group: str (optional)\n        :type hidden: bool (optional)\n        :type image: str (optional)\n        :type labelHighlightBold: bool (optional)\n        :type level: num (optional)\n        :type mass: num (optional)\n        :type physics: bool (optional)\n        :type shape: str (optional)\n        :type size: num (optional)\n        :type title: str or html element (optional)\n        :type value: num (optional)\n        :type x: num (optional)\n        :type y: num (optional)\n        \"\"\"\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def add_nodes(self, nodes, **kwargs):\n        \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n    def num_nodes(self):\n        \"\"\"\n        Return number of nodes\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.node_ids)\n\n    def num_edges(self):\n        \"\"\"\n        Return number of edges\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        \"\"\"\n\n        Adding edges is done based off of the IDs of the nodes. Order does\n        not matter unless dealing with a directed graph.\n\n        >>> nt.add_edge(0, 1) # adds an edge from node ID 0 to node ID\n        >>> nt.add_edge(0, 1, value = 4) # adds an edge with a width of 4\n\n\n        :param arrowStrikethrough: When false, the edge stops at the arrow.\n                                   This can be useful if you have thick lines\n                                   and you want the arrow to end in a point.\n                                   Middle arrows are not affected by this.\n\n        :param from: Edges are between two nodes, one to and one from. This\n                     is where you define the from node. You have to supply\n                     the corresponding node ID. This naturally only applies\n                     to individual edges.\n\n        :param hidden: When true, the edge is not drawn. It is part still part\n                       of the physics simulation however!\n\n        :param physics:\tWhen true, the edge is part of the physics simulation.\n                        When false, it will not act as a spring.\n\n        :param title: The title is shown in a pop-up when the mouse moves over\n                      the edge.\n\n        :param to: Edges are between two nodes, one to and one from. This is\n                   where you define the to node. You have to supply the\n                   corresponding node ID. This naturally only applies to\n                   individual edges.\n\n        :param value: When a value is set, the edges' width will be scaled\n                      using the options in the scaling object defined above.\n\n        :param width: The width of the edge. If value is set, this is not used.\n\n\n        :type arrowStrikethrough: bool\n        :type from: str or num\n        :type hidden: bool\n        :type physics: bool\n        :type title: str\n        :type to: str or num\n        :type value: num\n        :type width: num\n        \"\"\"\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        \"\"\"\n        This method serves to add multiple edges between existing nodes\n        in the network instance. Adding of the edges is done based off\n        of the IDs of the nodes. Order does not matter unless dealing with a\n        directed graph.\n\n        :param edges: A list of tuples, each tuple consists of source of edge,\n                      edge destination and and optional width.\n\n        :type arrowStrikethrough: list of tuples\n        \"\"\"\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        \"\"\"\n        Extract relevant information about this network in order to inject into\n        a Jinja2 template.\n\n        Returns:\n                nodes (list), edges (list), height (\n                    string), width (string), options (object)\n\n        Usage:\n\n        >>> nodes, edges, heading, height, width, options = net.get_network_data()\n        \"\"\"\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        \"\"\"\n        Save the graph as html in the current directory with name.\n\n        :param name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        \"\"\"\n        Writes a static HTML file and saves it locally before opening.\n\n        :param: name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        \"\"\"\n        Loads the template data into the template attribute of the network.\n        This should be done in a jupyter notebook environment before showing\n        the network.\n\n        Example:\n                >>> net.prep_notebook()\n                >>> net.show(\"nb.html\")\n\n\n        :param path: the relative path pointing to a template html file\n        :type path: string\n        \"\"\"\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        \"\"\"\n            Path to full template assumes that it exists inside of a template directory.\n            Use `set_template_dir` to set the relative template path to the template directory along with the directory location itself\n            to change both values otherwise this function will infer the results.\n            :path_to_template path: full os path string value of the template directory\n        \"\"\"\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        \"\"\"\n            Path to template directory along with the location of the template file.\n            :template_directory path: template directory\n            :template_file path: name of the template file that is going to be used to generate the html doc.\n\n        \"\"\"\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        \"\"\"\n        This method takes the contents of .DOT file and converts it\n        to a PyVis visualization.\n\n        Assuming the contents of test.dot contains:\n        digraph sample3 {\n        A -> {B ; C ; D}\n        C -> {B ; A}\n        }\n\n        Usage:\n\n        >>> nt.Network(\"500px\", \"500px\")\n        >>> nt.from_DOT(\"test.dot\")\n        >>> nt.show(\"dot.html\")\n\n        :param dot: The path of the dotfile being converted.\n        :type dot: .dot file\n\n        \"\"\"\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        \"\"\"\n        This method returns the user an adjacency list representation\n        of the network.\n\n        :returns: dictionary mapping of Node ID to list of Node IDs it\n        is connected to.\n        \"\"\"\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def neighbors(self, node):\n        \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        \"\"\"\n        This method takes an exisitng Networkx graph and translates\n        it to a PyVis graph format that can be accepted by the VisJs\n        API in the Jinja2 template. This operation is done in place.\n\n        :param nx_graph: The Networkx graph object that is to be translated.\n        :type nx_graph: networkx.Graph instance\n        :param node_size_transf: function to transform the node size for plotting\n        :type node_size_transf: func\n        :param edge_weight_transf: function to transform the edge weight for plotting\n        :type edge_weight_transf: func\n        :param default_node_size: default node size if not specified\n        :param default_edge_weight: default edge weight if not specified\n        >>> nx_graph = nx.cycle_graph(10)\n        >>> nx_graph.nodes[1]['title'] = 'Number 1'\n        >>> nx_graph.nodes[1]['group'] = 1\n        >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n        >>> nx_graph.nodes[3]['group'] = 10\n        >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n        >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n        >>> nx_graph.add_edge(20, 21, weight=5)\n        >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n        >>> nt = Network(\"500px\", \"500px\")\n        # populates the nodes and edges data structures\n        >>> nt.from_nx(nx_graph)\n        >>> nt.show(\"nx.html\")\n        \"\"\"\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        \"\"\"\n        This method returns an iterable list of node ids\n\n        :returns: list\n        \"\"\"\n        return self.node_ids\n\n    def get_node(self, n_id):\n        \"\"\"\n        Lookup node by ID and return it.\n\n        :param n_id: The ID given to the node.\n\n        :returns: dict containing node properties\n        \"\"\"\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        \"\"\"\n        This method returns an iterable list of edge objects\n\n        :returns: list\n        \"\"\"\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        \"\"\"\n        BarnesHut is a quadtree based gravity model. It is the fastest. default\n        and recommended solver for non-hierarchical layouts.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        \"\"\"\n        Set the physics attribute of the entire network to repulsion.\n        When called, it sets the solver attribute of physics to repulsion.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        \"\"\"\n        This model is based on the repulsion solver but the levels are\n        taken into account and the forces are normalized.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        \"\"\"\n        The forceAtlas2Based solver makes use of some of the equations provided\n        by them and makes use of the barnesHut implementation in vis. The main\n        differences are the central gravity model, which is here distance\n        independent, and the repulsion being linear instead of quadratic. Finally,\n        all node masses have a multiplier based on the amount of connected edges\n        plus one.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def toggle_hide_edges_on_drag(self, status):\n        \"\"\"\n        Displays or hides edges while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: True if edges should be hidden on drag\n        \n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        \"\"\"\n        Displays or hides nodes while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: When set to True, the nodes will hide on drag.\n                       Default is set to False.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        \"\"\"\n        Edges take on the color of the node they are coming from.\n\n        :param status: True if edges should adopt color coming from.\n        :type status: bool\n        \"\"\"\n        self.options.edges.inherit_colors(status)\n\n    def show_buttons(self, filter_=None):\n        \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n    def toggle_physics(self, status):\n        \"\"\"\n        Toggles physics simulation \n\n        :param status: When False, nodes are not part of the physics\n                       simulation. They will not move except for from\n                       manual dragging.\n                       Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        \"\"\"\n        Toggles the dragging of the nodes in the network.\n\n        :param status: When set to True, the nodes can be dragged around\n                       in the network. Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        \"\"\"\n        Toggles the stablization of the network.\n\n        :param status: Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        \"\"\"\n        Overrides the default options object passed to the VisJS framework.\n        Delegates to the :meth:`options.Options.set` routine.\n\n        :param options: The string representation of the Javascript-like object\n                        to be used to override default options.\n        \n        :type options: str\n        \"\"\"\n        self.options = self.options.set(options)\n\n    def set_edge_smooth(self, smooth_type):\n        \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n```\n",
        "eval_script": "import os\nimport json\nimport shutil\nimport tempfile\nfrom collections import defaultdict\nfrom jinja2 import Environment, FileSystemLoader\n\n# The mock Options class\n\nclass Options:\n    def __init__(self, layout=None):\n        self.edges = self.EdgeOptions()\n        self.physics = self.Physics()\n        self.interaction = self.Interaction()\n        self.configure = None\n        self.layout = layout\n\n    class EdgeOptions:\n        def __init__(self):\n            self.smooth = self.SmoothOptions()\n            self.inherit_colors = None\n\n        class SmoothOptions:\n            def __init__(self):\n                self.enabled = False\n                self.type = 'continuous'\n    \n    class Physics:\n        def __init__(self):\n            self.enabled = True\n    \n        def use_barnes_hut(self, params):\n            pass\n        \n        def use_repulsion(self, params):\n            pass\n        \n        def use_hrepulsion(self, params):\n            pass\n        \n        def use_force_atlas_2based(self, params):\n            pass\n        \n        def toggle_stabilization(self, status):\n            pass\n    \n    class Interaction:\n        def __init__(self):\n            self.hideEdgesOnDrag = False\n            self.hideNodesOnDrag = False\n            self.dragNodes = True\n\n# network.py\n\nclass Network(object):\n    \"\"\"\n    The Network class is the focus of this library. All viz functionality\n    should be implemented off of a Network instance.\n\n    To instantiate:\n\n    >>> nt = Network()\n    \"\"\"\n\n    def __init__(self, height='600px', width='100%', directed=False, notebook=False, neighborhood_highlight=False, select_menu=False, filter_menu=False, bgcolor='#ffffff', font_color=False, layout=None, heading='', cdn_resources='local'):\n        \"\"\"\n        :param height: The height of the canvas\n        :param width: The width of the canvas\n        :param directed: Whether or not to use a directed graph. This is false\n                         by default.\n        :param notebook: True if using jupyter notebook.\n        :param select_menu: sets the option to highlight nodes and the neighborhood\n        :param filter_menu: sets the option to filter nodes and edges based on attributes\n        :param bgcolor: The background color of the canvas.\n        :param cdn_resources: Where to pull resources for css and js files. Defaults to local.\n            Options ['local','in_line','remote'].\n            local: pull resources from local lib folder.\n            in_line: insert lib resources as inline script tags.\n            remote: pull resources from hash checked cdns.\n        :font_color: The color of the node labels text\n        :layout: Use hierarchical layout if this is set\n\n        :type height: num or str\n        :type width: num or str\n        :type directed: bool\n        :type notebook: bool\n        :type select_menu: bool\n        :type filter_menu: bool\n        :type bgcolor: str\n        :type font_color: str\n        :type layout: bool\n        :type cdn_resources: str\n        \"\"\"\n        self.nodes = []\n        self.edges = []\n        self.height = height\n        self.width = width\n        self.heading = heading\n        self.html = ''\n        self.shape = 'dot'\n        self.font_color = font_color\n        self.directed = directed\n        self.bgcolor = bgcolor\n        self.use_DOT = False\n        self.dot_lang = ''\n        self.options = Options(layout)\n        self.widget = False\n        self.node_ids = []\n        self.node_map = {}\n        self.template = None\n        self.conf = False\n        self.path = 'template.html'\n        self.neighborhood_highlight = neighborhood_highlight\n        self.select_menu = select_menu\n        self.filter_menu = filter_menu\n        assert cdn_resources in ['local', 'in_line', 'remote'], 'cdn_resources not in [local, in_line, remote].'\n        self.template_dir = os.path.dirname(__file__) + '/templates/'\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n        if cdn_resources == 'local' and notebook == True:\n            print('Local cdn resources have problems on chrome/safari when used in jupyter-notebook. ')\n        self.cdn_resources = cdn_resources\n        if notebook:\n            self.prep_notebook()\n\n    def __str__(self):\n        \"\"\"\n        override print to show readable graph data\n        \"\"\"\n        return str(json.dumps({'Nodes': self.node_ids, 'Edges': self.edges, 'Height': self.height, 'Width': self.width, 'Heading': self.heading}, indent=4))\n\n    def __repr__(self):\n        return '{} |N|={} |E|={:,}'.format(self.__class__, self.num_nodes(), self.num_edges())\n\n    def add_node(self, n_id, label=None, shape='dot', color='#97c2fc', **options):\n        \"\"\"\n        This method adds a node to the network, given a mandatory node ID.\n        Node labels default to node ids if no label is specified during the\n        call.\n\n        >>> nt = Network(\"500px\", \"500px\")\n        >>> nt.add_node(0, label=\"Node 0\")\n        >>> nt.add_node(1, label=\"Node 1\", color = \"blue\")\n\n        :param n_id: The id of the node. The id is mandatory for nodes and\n                     they have to be unique. This should obviously be set per\n                     node, not globally.\n\n        :param label: The label is the piece of text shown in or under the\n                      node, depending on the shape.\n\n        :param borderWidth:\tThe width of the border of the node.\n\n        :param borderWidthSelected:\tThe width of the border of the node when\n                                    it is selected. When undefined, the\n                                    borderWidth * 2 is used.\n\n        :param brokenImage:\tWhen the shape is set to image or circularImage,\n                            this option can be an URL to a backup image in\n                            case the URL supplied in the image option cannot\n                            be resolved.\n\n        :param group: When not undefined, the node will belong to the defined\n                      group. Styling information of that group will apply to\n                      this node. Node specific styling overrides group styling.\n\n        :param hidden: When true, the node will not be shown. It will still be\n                       part of the physics simulation though!\n\n        :param image: When the shape is set to image or circularImage, this\n                      option should be the URL to an image. If the image\n                      cannot be found, the brokenImage option can be used.\n\n        :param labelHighlightBold: Determines whether or not the label becomes\n                                   bold when the node is selected.\n\n        :param level: When using the hierarchical layout, the level determines\n                      where the node is going to be positioned.\n\n        :param mass: The barnesHut physics model (which is enabled by default)\n                     is based on an inverted gravity model. By increasing\n                     the mass of a node, you increase it's repulsion. Values\n                     lower than 1 are not recommended.\n\n        :param physics:\tWhen false, the node is not part of the physics\n                        simulation. It will not move except for from\n                        manual dragging.\n\n        :param shape: The shape defines what the node looks like. There are\n                      two types of nodes. One type has the label inside of\n                      it and the other type has the label underneath it. The\n                      types with the label inside of it are: ellipse, circle,\n                      database, box, text. The ones with the label outside of\n                      it are: image, circularImage, diamond, dot, star,\n                      triangle, triangleDown, square and icon.\n\n        :param size: The size is used to determine the size of node shapes that\n                     do not have the label inside of them. These shapes are:\n                     image, circularImage, diamond, dot, star, triangle,\n                     triangleDown, square and icon.\n\n        :param title: Title to be displayed when the user hovers over the node.\n                      The title can be an HTML element or a string containing\n                      plain text or HTML.\n\n        :param value: When a value is set, the nodes will be scaled using the\n                      options in the scaling object defined above.\n\n        :param x: This gives a node an initial x position. When using the\n                  hierarchical layout, either the x or y position is set by the\n                  layout engine depending on the type of view. The other value\n                  remains untouched. When using stabilization, the stabilized\n                  position may be different from the initial one. To lock the\n                  node to that position use the physics or fixed options.\n\n        :param y: This gives a node an initial y position. When using the\n                  hierarchical layout,either the x or y position is set by\n                  the layout engine depending on the type of view. The\n                  other value remains untouched. When using stabilization,\n                  the stabilized position may be different from the initial\n                  one. To lock the node to that position use the physics or\n                  fixed options.\n\n        :type n_id: str or int\n        :type label: str or int\n        :type borderWidth: num (optional)\n        :type borderWidthSelected: num (optional)\n        :type brokenImage: str (optional)\n        :type group: str (optional)\n        :type hidden: bool (optional)\n        :type image: str (optional)\n        :type labelHighlightBold: bool (optional)\n        :type level: num (optional)\n        :type mass: num (optional)\n        :type physics: bool (optional)\n        :type shape: str (optional)\n        :type size: num (optional)\n        :type title: str or html element (optional)\n        :type value: num (optional)\n        :type x: num (optional)\n        :type y: num (optional)\n        \"\"\"\n        assert isinstance(n_id, str) or isinstance(n_id, int)\n        if label:\n            node_label = label\n        else:\n            node_label = n_id\n        if n_id not in self.node_ids:\n            if 'group' in options:\n                n = Node(n_id, shape, label=node_label, font_color=self.font_color, **options)\n            else:\n                n = Node(n_id, shape, label=node_label, color=color, font_color=self.font_color, **options)\n            self.nodes.append(n.options)\n            self.node_ids.append(n_id)\n            self.node_map[n_id] = n.options\n\n    def add_nodes(self, nodes, **kwargs):\n        \"\"\"\n        This method adds multiple nodes to the network from a list.\n        Default behavior uses values of 'nodes' for node ID and node label\n        properties. You can also specify other lists of properties to go\n        along each node.\n\n        Example:\n\n        >>> g = net.Network()\n        >>> g.add_nodes([1, 2, 3], size=[2, 4, 6], title=[\"n1\", \"n2\", \"n3\"])\n        >>> g.nodes\n        >>> [{'id': 1, 'label': 1, 'shape': 'dot', 'size': 2, 'title': 'n1'},\n\n        Output:\n\n        >>> {'id': 2, 'label': 2, 'shape': 'dot', 'size': 4, 'title': 'n2'},\n        >>> {'id': 3, 'label': 3, 'shape': 'dot', 'size': 6, 'title': 'n3'}]\n\n\n        :param nodes: A list of nodes.\n\n        :type nodes: list\n        \"\"\"\n        valid_args = ['size', 'value', 'title', 'x', 'y', 'label', 'color', 'shape']\n        for k in kwargs:\n            assert k in valid_args, \"invalid arg '\" + k + \"'\"\n        nd = defaultdict(dict)\n        for i in range(len(nodes)):\n            for k, v in kwargs.items():\n                assert len(v) == len(nodes), 'keyword arg %s [length %s] does not match[length %s] of nodes' % (k, len(v), len(nodes))\n                nd[nodes[i]].update({k: v[i]})\n        for node in nodes:\n            try:\n                node = int(node)\n                self.add_node(node, **nd[node])\n            except:\n                assert isinstance(node, str)\n                self.add_node(node, **nd[node])\n\n    def num_nodes(self):\n        \"\"\"\n        Return number of nodes\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.node_ids)\n\n    def num_edges(self):\n        \"\"\"\n        Return number of edges\n\n        :returns: :py:class:`int`\n        \"\"\"\n        return len(self.edges)\n\n    def add_edge(self, source, to, **options):\n        \"\"\"\n\n        Adding edges is done based off of the IDs of the nodes. Order does\n        not matter unless dealing with a directed graph.\n\n        >>> nt.add_edge(0, 1) # adds an edge from node ID 0 to node ID\n        >>> nt.add_edge(0, 1, value = 4) # adds an edge with a width of 4\n\n\n        :param arrowStrikethrough: When false, the edge stops at the arrow.\n                                   This can be useful if you have thick lines\n                                   and you want the arrow to end in a point.\n                                   Middle arrows are not affected by this.\n\n        :param from: Edges are between two nodes, one to and one from. This\n                     is where you define the from node. You have to supply\n                     the corresponding node ID. This naturally only applies\n                     to individual edges.\n\n        :param hidden: When true, the edge is not drawn. It is part still part\n                       of the physics simulation however!\n\n        :param physics:\tWhen true, the edge is part of the physics simulation.\n                        When false, it will not act as a spring.\n\n        :param title: The title is shown in a pop-up when the mouse moves over\n                      the edge.\n\n        :param to: Edges are between two nodes, one to and one from. This is\n                   where you define the to node. You have to supply the\n                   corresponding node ID. This naturally only applies to\n                   individual edges.\n\n        :param value: When a value is set, the edges' width will be scaled\n                      using the options in the scaling object defined above.\n\n        :param width: The width of the edge. If value is set, this is not used.\n\n\n        :type arrowStrikethrough: bool\n        :type from: str or num\n        :type hidden: bool\n        :type physics: bool\n        :type title: str\n        :type to: str or num\n        :type value: num\n        :type width: num\n        \"\"\"\n        edge_exists = False\n        assert source in self.get_nodes(), \"non existent node '\" + str(source) + \"'\"\n        assert to in self.get_nodes(), \"non existent node '\" + str(to) + \"'\"\n        if not self.directed:\n            for e in self.edges:\n                frm = e['from']\n                dest = e['to']\n                if source == dest and to == frm or (source == frm and to == dest):\n                    edge_exists = True\n        if not edge_exists:\n            e = Edge(source, to, self.directed, **options)\n            self.edges.append(e.options)\n\n    def add_edges(self, edges):\n        \"\"\"\n        This method serves to add multiple edges between existing nodes\n        in the network instance. Adding of the edges is done based off\n        of the IDs of the nodes. Order does not matter unless dealing with a\n        directed graph.\n\n        :param edges: A list of tuples, each tuple consists of source of edge,\n                      edge destination and and optional width.\n\n        :type arrowStrikethrough: list of tuples\n        \"\"\"\n        for edge in edges:\n            if len(edge) == 3:\n                self.add_edge(edge[0], edge[1], width=edge[2])\n            else:\n                self.add_edge(edge[0], edge[1])\n\n    def get_network_data(self):\n        \"\"\"\n        Extract relevant information about this network in order to inject into\n        a Jinja2 template.\n\n        Returns:\n                nodes (list), edges (list), height (\n                    string), width (string), options (object)\n\n        Usage:\n\n        >>> nodes, edges, heading, height, width, options = net.get_network_data()\n        \"\"\"\n        if isinstance(self.options, dict):\n            return (self.nodes, self.edges, self.heading, self.height, self.width, json.dumps(self.options))\n        else:\n            return (self.nodes, self.edges, self.heading, self.height, self.width, self.options.to_json())\n\n    def save_graph(self, name):\n        \"\"\"\n        Save the graph as html in the current directory with name.\n\n        :param name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        self.write_html(name)\n\n    def generate_html(self, name='index.html', local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        use_link_template = False\n        for n in self.nodes:\n            title = n.get('title', None)\n            if title:\n                if 'href' in title:\n                    '\\n                    this tells the template to override default hover\\n                    mechanic, as the tooltip would move with the mouse\\n                    cursor which made interacting with hover data useless.\\n                    '\n                    use_link_template = True\n                    break\n        if not notebook:\n            template = self.templateEnv.get_template(self.path)\n        else:\n            template = self.template\n        nodes, edges, heading, height, width, options = self.get_network_data()\n        if isinstance(self.options, dict):\n            if 'physics' in self.options and 'enabled' in self.options['physics']:\n                physics_enabled = self.options['physics']['enabled']\n            else:\n                physics_enabled = True\n        else:\n            physics_enabled = self.options.physics.enabled\n        self.html = template.render(height=height, width=width, nodes=nodes, edges=edges, heading=heading, options=options, physics_enabled=physics_enabled, use_DOT=self.use_DOT, dot_lang=self.dot_lang, widget=self.widget, bgcolor=self.bgcolor, conf=self.conf, tooltip_link=use_link_template, neighborhood_highlight=self.neighborhood_highlight, select_menu=self.select_menu, filter_menu=self.filter_menu, notebook=notebook, cdn_resources=self.cdn_resources)\n        return self.html\n\n    def write_html(self, name, local=True, notebook=False):\n        \"\"\"\n        This method gets the data structures supporting the nodes, edges,\n        and options and updates the template to write the HTML holding\n        the visualization.\n        :type name_html: str\n        \"\"\"\n        check_html(name)\n        self.html = self.generate_html(notebook=notebook)\n        with open(name, 'w+') as out:\n            out.write(self.html)\n        if notebook:\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        elif notebook and local:\n            if not os.path.exists('lib'):\n                os.makedirs(os.path.dirname('lib'))\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/bindings', 'lib/bindings')\n            if not os.path.exists('lib/tom-select'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/tom-select', 'lib/tom-select')\n            if not os.path.exists('lib/bindings'):\n                shutil.copytree(f'{os.path.dirname(__file__)}/templates/lib/vis-9.1.2', 'lib/vis-9.1.2')\n            with open(name, 'w+') as out:\n                out.write(self.html)\n            return IFrame(name, width=self.width, height=self.height)\n        else:\n            if local:\n                tempdir = '.'\n            else:\n                tempdir = tempfile.mkdtemp()\n            source = '.\\\\pyvis'\n            print('source:', source)\n            destination = f'{os.path.dirname(__file__)}'\n            print('destination:', destination)\n            if not os.path.exists(destination):\n                shutil.copytree(source, destination)\n            with open(f'{tempdir}/{name}', 'w+') as out:\n                out.write(self.html)\n\n    def show(self, name, local=True):\n        \"\"\"\n        Writes a static HTML file and saves it locally before opening.\n\n        :param: name: the name of the html file to save as\n        :type name: str\n        \"\"\"\n        check_html(name)\n        if self.template is not None:\n            return self.write_html(name, local, notebook=True)\n        else:\n            self.write_html(name, local)\n\n    def prep_notebook(self, custom_template=False, custom_template_path=None):\n        \"\"\"\n        Loads the template data into the template attribute of the network.\n        This should be done in a jupyter notebook environment before showing\n        the network.\n\n        Example:\n                >>> net.prep_notebook()\n                >>> net.show(\"nb.html\")\n\n\n        :param path: the relative path pointing to a template html file\n        :type path: string\n        \"\"\"\n        if custom_template and custom_template_path:\n            self.set_template(custom_template_path)\n        self.template = self.templateEnv.get_template(self.path)\n\n    def set_template(self, path_to_template: str):\n        \"\"\"\n            Path to full template assumes that it exists inside of a template directory.\n            Use `set_template_dir` to set the relative template path to the template directory along with the directory location itself\n            to change both values otherwise this function will infer the results.\n            :path_to_template path: full os path string value of the template directory\n        \"\"\"\n        str_parts = path_to_template.split('/')\n        self.set_template_dir('/'.join(str_parts[:-1]) + '/', str_parts[-1])\n\n    def set_template_dir(self, template_directory, template_file='template.html'):\n        \"\"\"\n            Path to template directory along with the location of the template file.\n            :template_directory path: template directory\n            :template_file path: name of the template file that is going to be used to generate the html doc.\n\n        \"\"\"\n        self.path = template_file\n        self.template_dir = template_directory\n        self.templateEnv = Environment(loader=FileSystemLoader(self.template_dir))\n\n    def from_DOT(self, dot):\n        \"\"\"\n        This method takes the contents of .DOT file and converts it\n        to a PyVis visualization.\n\n        Assuming the contents of test.dot contains:\n        digraph sample3 {\n        A -> {B ; C ; D}\n        C -> {B ; A}\n        }\n\n        Usage:\n\n        >>> nt.Network(\"500px\", \"500px\")\n        >>> nt.from_DOT(\"test.dot\")\n        >>> nt.show(\"dot.html\")\n\n        :param dot: The path of the dotfile being converted.\n        :type dot: .dot file\n\n        \"\"\"\n        self.use_DOT = True\n        file = open(dot, 'r')\n        s = str(file.read())\n        self.dot_lang = ' '.join(s.splitlines())\n        self.dot_lang = self.dot_lang.replace('\"', '\\\\\"')\n\n    def get_adj_list(self):\n        \"\"\"\n        This method returns the user an adjacency list representation\n        of the network.\n\n        :returns: dictionary mapping of Node ID to list of Node IDs it\n        is connected to.\n        \"\"\"\n        a_list = {}\n        for i in self.nodes:\n            a_list[i['id']] = set()\n        if self.directed:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                a_list[source].add(dest)\n        else:\n            for e in self.edges:\n                source = e['from']\n                dest = e['to']\n                if dest not in a_list[source] and source not in a_list[dest]:\n                    a_list[source].add(dest)\n                    a_list[dest].add(source)\n        return a_list\n\n    def neighbors(self, node):\n        \"\"\"\n        Given a node id, return the set of neighbors of this particular node.\n\n        :param node: The node to get the neighbors from\n        :type node: str or int\n\n        :returns: set\n        \"\"\"\n        assert isinstance(node, str) or isinstance(node, int), 'error: expected int or str for node but got %s' % type(node)\n        assert node in self.node_ids, 'error: %s node not in network' % node\n        return self.get_adj_list()[node]\n\n    def from_nx(self, nx_graph, node_size_transf=lambda x: x, edge_weight_transf=lambda x: x, default_node_size=10, default_edge_weight=1, show_edge_weights=True, edge_scaling=False):\n        \"\"\"\n        This method takes an exisitng Networkx graph and translates\n        it to a PyVis graph format that can be accepted by the VisJs\n        API in the Jinja2 template. This operation is done in place.\n\n        :param nx_graph: The Networkx graph object that is to be translated.\n        :type nx_graph: networkx.Graph instance\n        :param node_size_transf: function to transform the node size for plotting\n        :type node_size_transf: func\n        :param edge_weight_transf: function to transform the edge weight for plotting\n        :type edge_weight_transf: func\n        :param default_node_size: default node size if not specified\n        :param default_edge_weight: default edge weight if not specified\n        >>> nx_graph = nx.cycle_graph(10)\n        >>> nx_graph.nodes[1]['title'] = 'Number 1'\n        >>> nx_graph.nodes[1]['group'] = 1\n        >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n        >>> nx_graph.nodes[3]['group'] = 10\n        >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n        >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n        >>> nx_graph.add_edge(20, 21, weight=5)\n        >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n        >>> nt = Network(\"500px\", \"500px\")\n        # populates the nodes and edges data structures\n        >>> nt.from_nx(nx_graph)\n        >>> nt.show(\"nx.html\")\n        \"\"\"\n        assert isinstance(nx_graph, nx.Graph)\n        edges = nx_graph.edges(data=True)\n        nodes = nx_graph.nodes(data=True)\n        if len(edges) > 0:\n            for e in edges:\n                if 'size' not in nodes[e[0]].keys():\n                    nodes[e[0]]['size'] = default_node_size\n                nodes[e[0]]['size'] = int(node_size_transf(nodes[e[0]]['size']))\n                if 'size' not in nodes[e[1]].keys():\n                    nodes[e[1]]['size'] = default_node_size\n                nodes[e[1]]['size'] = int(node_size_transf(nodes[e[1]]['size']))\n                self.add_node(e[0], **nodes[e[0]])\n                self.add_node(e[1], **nodes[e[1]])\n                if 'value' not in e[2] or 'width' not in e[2]:\n                    if edge_scaling:\n                        width_type = 'value'\n                    else:\n                        width_type = 'width'\n                    if 'weight' not in e[2].keys():\n                        e[2]['weight'] = default_edge_weight\n                    e[2][width_type] = edge_weight_transf(e[2]['weight'])\n                    e[2][width_type] = e[2].pop('weight')\n                self.add_edge(e[0], e[1], **e[2])\n        for node in nx.isolates(nx_graph):\n            if 'size' not in nodes[node].keys():\n                nodes[node]['size'] = default_node_size\n            self.add_node(node, **nodes[node])\n\n    def get_nodes(self):\n        \"\"\"\n        This method returns an iterable list of node ids\n\n        :returns: list\n        \"\"\"\n        return self.node_ids\n\n    def get_node(self, n_id):\n        \"\"\"\n        Lookup node by ID and return it.\n\n        :param n_id: The ID given to the node.\n\n        :returns: dict containing node properties\n        \"\"\"\n        return self.node_map[n_id]\n\n    def get_edges(self):\n        \"\"\"\n        This method returns an iterable list of edge objects\n\n        :returns: list\n        \"\"\"\n        return self.edges\n\n    def barnes_hut(self, gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001, damping=0.09, overlap=0):\n        \"\"\"\n        BarnesHut is a quadtree based gravity model. It is the fastest. default\n        and recommended solver for non-hierarchical layouts.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_barnes_hut(locals())\n\n    def repulsion(self, node_distance=100, central_gravity=0.2, spring_length=200, spring_strength=0.05, damping=0.09):\n        \"\"\"\n        Set the physics attribute of the entire network to repulsion.\n        When called, it sets the solver attribute of physics to repulsion.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_repulsion(locals())\n\n    def hrepulsion(self, node_distance=120, central_gravity=0.0, spring_length=100, spring_strength=0.01, damping=0.09):\n        \"\"\"\n        This model is based on the repulsion solver but the levels are\n        taken into account and the forces are normalized.\n\n        :param node_distance: This is the range of influence for the repulsion.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center.\n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n\n        :type node_distance: int\n        :type central_gravity float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        \"\"\"\n        self.options.physics.use_hrepulsion(locals())\n\n    def force_atlas_2based(self, gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.08, damping=0.4, overlap=0):\n        \"\"\"\n        The forceAtlas2Based solver makes use of some of the equations provided\n        by them and makes use of the barnesHut implementation in vis. The main\n        differences are the central gravity model, which is here distance\n        independent, and the repulsion being linear instead of quadratic. Finally,\n        all node masses have a multiplier based on the amount of connected edges\n        plus one.\n\n        :param gravity: The more negative the gravity value is, the stronger the\n                        repulsion is.\n        :param central_gravity: The gravity attractor to pull the entire network\n                                to the center. \n        :param spring_length: The rest length of the edges\n        :param spring_strength: The strong the edges springs are\n        :param damping: A value ranging from 0 to 1 of how much of the velocity\n                        from the previous physics simulation iteration carries\n                        over to the next iteration.\n        :param overlap: When larger than 0, the size of the node is taken into\n                        account. The distance will be calculated from the radius\n                        of the encompassing circle of the node for both the\n                        gravity model. Value 1 is maximum overlap avoidance.\n\n        :type gravity: int\n        :type central_gravity: float\n        :type spring_length: int\n        :type spring_strength: float\n        :type damping: float\n        :type overlap: float\n        \"\"\"\n        self.options.physics.use_force_atlas_2based(locals())\n\n    def to_json(self, max_depth=1, **args):\n        return jsonpickle.encode(self, max_depth=max_depth, **args)\n\n    def toggle_hide_edges_on_drag(self, status):\n        \"\"\"\n        Displays or hides edges while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: True if edges should be hidden on drag\n        \n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideEdgesOnDrag = status\n\n    def toggle_hide_nodes_on_drag(self, status):\n        \"\"\"\n        Displays or hides nodes while dragging the network. This makes\n        panning of the network easy.\n\n        :param status: When set to True, the nodes will hide on drag.\n                       Default is set to False.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.hideNodesOnDrag = status\n\n    def inherit_edge_colors(self, status):\n        \"\"\"\n        Edges take on the color of the node they are coming from.\n\n        :param status: True if edges should adopt color coming from.\n        :type status: bool\n        \"\"\"\n        self.options.edges.inherit_colors(status)\n\n    def show_buttons(self, filter_=None):\n        \"\"\"\n        Displays or hides certain widgets to dynamically modify the\n        network.\n\n        Usage:\n        >>> g.show_buttons(filter_=['nodes', 'edges', 'physics'])\n\n        Or to show all options:\n        >>> g.show_buttons()\n\n        :param status: When set to True, the widgets will be shown.\n                       Default is set to False.\n        :param filter_: Only include widgets specified by `filter_`.\n                        Valid options: True (gives all widgets)\n                                       List of `nodes`, `edges`,\n                                       `layout`, `interaction`,\n                                       `manipulation`, `physics`,\n                                       `selection`, `renderer`.\n\n        :type status: bool\n        :type filter_: bool or list:\n        \"\"\"\n        self.conf = True\n        self.options.configure = Configure(enabled=True, filter_=filter_)\n        self.widget = True\n\n    def toggle_physics(self, status):\n        \"\"\"\n        Toggles physics simulation \n\n        :param status: When False, nodes are not part of the physics\n                       simulation. They will not move except for from\n                       manual dragging.\n                       Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.enabled = status\n\n    def toggle_drag_nodes(self, status):\n        \"\"\"\n        Toggles the dragging of the nodes in the network.\n\n        :param status: When set to True, the nodes can be dragged around\n                       in the network. Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.interaction.dragNodes = status\n\n    def toggle_stabilization(self, status):\n        \"\"\"\n        Toggles the stablization of the network.\n\n        :param status: Default is set to True.\n\n        :type status: bool\n        \"\"\"\n        self.options.physics.toggle_stabilization(status)\n\n    def set_options(self, options):\n        \"\"\"\n        Overrides the default options object passed to the VisJS framework.\n        Delegates to the :meth:`options.Options.set` routine.\n\n        :param options: The string representation of the Javascript-like object\n                        to be used to override default options.\n        \n        :type options: str\n        \"\"\"\n        self.options = self.options.set(options)\n\n    def set_edge_smooth(self, smooth_type):\n        \"\"\"\n        Sets the smooth.type attribute of the edges.\n\n        :param smooth_type: Possible options: 'dynamic', 'continuous',\n                            'discrete', 'diagonalCross', 'straightCross',\n                            'horizontal', 'vertical', 'curvedCW',\n                            'curvedCCW', 'cubicBezier'.\n                            When using dynamic, the edges will have an\n                            invisible support node guiding the shape.\n                            This node is part of the physics simulation.\n                            Default is set to continous.\n\n        :type smooth_type: string\n        \"\"\"\n        self.options.edges.smooth.enabled = True\n        self.options.edges.smooth.type = smooth_type\n\n\ndef test_set_edge_smooth():\n    network1 = Network()\n    network2 = Network()\n\n    # Test case 1: Use 'dynamic'\n    network1.set_edge_smooth('dynamic')\n    network2.set_edge_smooth_new_implementation('dynamic')\n    assert network1.options.edges.smooth.type == network2.options.edges.smooth.type\n    assert network1.options.edges.smooth.enabled == network2.options.edges.smooth.enabled\n\n    # Test case 2: Use 'continuous'\n    network1.set_edge_smooth('continuous')\n    network2.set_edge_smooth_new_implementation('continuous')\n    assert network1.options.edges.smooth.type == network2.options.edges.smooth.type\n    assert network1.options.edges.smooth.enabled == network2.options.edges.smooth.enabled\n\n    # Test case 3: Use 'discrete'\n    network1.set_edge_smooth('discrete')\n    network2.set_edge_smooth_new_implementation('discrete')\n    assert network1.options.edges.smooth.type == network2.options.edges.smooth.type\n    assert network1.options.edges.smooth.enabled == network2.options.edges.smooth.enabled\n\nif __name__ == \"__main__\":\n    test_set_edge_smooth()"
    },
    {
        "func_name": "filter_tempfile_ipynb",
        "idx": "113",
        "repo_name": "manzt___juv",
        "func_path": "tests/test_juv.py",
        "orig_func": "def filter_tempfile_ipynb(output: str) -> str:\n    \"\"\"Replace the temporary directory in the output with <TEMPDIR> for snapshotting.\"\"\"\n    pattern = '`([^`\\\\n]+\\\\n?[^`\\\\n]+/)([^/\\\\n]+\\\\.ipynb)`'\n    replacement = '`<TEMPDIR>/\\\\2`'\n    return re.sub(pattern, replacement, output)",
        "orig_context": "```python\n## tests/test_juv.py\nimport re\n\ndef filter_tempfile_ipynb(output: str) -> str:\n    \"\"\"Replace the temporary directory in the output with <TEMPDIR> for snapshotting.\"\"\"\n    pattern = r\"`([^`\\n]+\\n?[^`\\n]+/)([^/\\n]+\\.ipynb)`\"\n    replacement = r\"`<TEMPDIR>/\\2`\"\n    return re.sub(pattern, replacement, output)\n\n```\n\n\n",
        "eval_script": "## tests/test_juv.py\nimport re\n\ndef filter_tempfile_ipynb(output: str) -> str:\n    \"\"\"Replace the temporary directory in the output with <TEMPDIR> for snapshotting.\"\"\"\n    pattern = r\"`([^`\\n]+\\n?[^`\\n]+/)([^/\\n]+\\.ipynb)`\"\n    replacement = r\"`<TEMPDIR>/\\2`\"\n    return re.sub(pattern, replacement, output)\n\n\n\ndef test_filter_tempfile_ipynb():\n    # Test case 1: Basic replacement\n    input_str = \"`/home/user/tmp/notebook.ipynb`\"\n    assert filter_tempfile_ipynb(input_str) == filter_tempfile_ipynb_new_implementation(input_str)\n\n    # Test case 2: No match in the string\n    input_str = \"This is a string without any matching pattern.\"\n    assert filter_tempfile_ipynb(input_str) == filter_tempfile_ipynb_new_implementation(input_str)\n\n    # Test case 3: Multiple matches\n    input_str = \"`/home/user/tmp/notebook1.ipynb` text `/home/user/tmp/notebook2.ipynb`\"\n    assert filter_tempfile_ipynb(input_str) == filter_tempfile_ipynb_new_implementation(input_str)\n\nif __name__ == \"__main__\":\n    test_filter_tempfile_ipynb()"
    },
    {
        "func_name": "extract_inline_meta",
        "idx": "114",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_pep723.py",
        "orig_func": "def extract_inline_meta(script: str) -> tuple[str | None, str]:\n    \"\"\"Extract PEP 723 metadata from an inline script.\n\n    Parameters\n    ----------\n    script : str\n        A Python script that may contain a PEP 723 metadata block\n\n    Returns\n    -------\n    tuple[str | None, str]\n        The extracted metadata block and the script with the metadata block removed\n\n    \"\"\"\n    if (match := re.search(REGEX, script)):\n        meta_comment = match.group(0)\n        return (meta_comment, script.replace(meta_comment, '').strip())\n    return (None, script)",
        "orig_context": "```python\n## src/juv/_pep723.py\nimport re\n\nREGEX = r\"(?m)^# /// (?P<type>[a-zA-Z0-9-]+)$\\s(?P<content>(^#(| .*)$\\s)+)^# ///$\"\n\ndef extract_inline_meta(script: str) -> tuple[str | None, str]:\n    \"\"\"Extract PEP 723 metadata from an inline script.\n\n    Parameters\n    ----------\n    script : str\n        A Python script that may contain a PEP 723 metadata block\n\n    Returns\n    -------\n    tuple[str | None, str]\n        The extracted metadata block and the script with the metadata block removed\n\n    \"\"\"\n    if match := re.search(REGEX, script):\n        meta_comment = match.group(0)\n        return meta_comment, script.replace(meta_comment, \"\").strip()\n    return None, script\n\n```\n\n\n",
        "eval_script": "## src/juv/_pep723.py\nimport re\n\nREGEX = r\"(?m)^# /// (?P<type>[a-zA-Z0-9-]+)$\\s(?P<content>(^#(| .*)$\\s)+)^# ///$\"\n\ndef extract_inline_meta(script: str) -> tuple[str | None, str]:\n    \"\"\"Extract PEP 723 metadata from an inline script.\n\n    Parameters\n    ----------\n    script : str\n        A Python script that may contain a PEP 723 metadata block\n\n    Returns\n    -------\n    tuple[str | None, str]\n        The extracted metadata block and the script with the metadata block removed\n\n    \"\"\"\n    if match := re.search(REGEX, script):\n        meta_comment = match.group(0)\n        return meta_comment, script.replace(meta_comment, \"\").strip()\n    return None, script\n\n\n\ndef test_extract_inline_meta():\n    # Test cases\n    script_with_meta = \"\"\"# /// meta-type\n# Some metadata\n# ///\nprint(\"Hello World\")\n\"\"\"\n    script_without_meta = \"\"\"print(\"Hello World\")\n\"\"\"\n    script_with_similar_comments = \"\"\"# // meta-type\n# Some metadata\n# ///\nprint(\"Hello World\")\n\"\"\"\n\n    # Assertions\n    assert extract_inline_meta(script_with_meta) == extract_inline_meta_new_implementation(script_with_meta), \"Test with metadata failed\"\n    assert extract_inline_meta(script_without_meta) == extract_inline_meta_new_implementation(script_without_meta), \"Test without metadata failed\"\n    assert extract_inline_meta(script_with_similar_comments) == extract_inline_meta_new_implementation(script_with_similar_comments), \"Test with similar comments failed\"\n\nif __name__ == \"__main__\":\n    test_extract_inline_meta()"
    },
    {
        "func_name": "init",
        "idx": "115",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_init.py",
        "orig_func": "def init(path: Path | None, python: str | None, packages: typing.Sequence[str]=[]) -> Path:\n    \"\"\"Initialize a new notebook.\n\n    Parameters\n    ----------\n    path : pathlib.Path | None\n        The path to the new notebook. If None, a new Untitled.ipynb is created.\n    python : str | None\n        The version of Python to use. Passed as `--python` to uv.\n    packages : Sequence[str]\n        A list of packages to install in the new notebook.\n\n    Returns\n    -------\n    pathlib.Path\n        The path to the new notebook.\n\n    \"\"\"\n    if not path:\n        path = get_first_non_conflicting_untitled_ipynb(Path.cwd())\n    if path.suffix != '.ipynb':\n        rich.print('File must have a `[cyan].ipynb[/cyan]` extension.', file=sys.stderr)\n        sys.exit(1)\n    notebook = new_notebook_with_inline_metadata(path.parent, python)\n    write_ipynb(notebook, path)\n    if len(packages) > 0:\n        from ._add import add\n        add(path=path, packages=packages)\n    return path",
        "orig_context": "```python\n## src/juv/_nbutils.py\nimport jupytext\n\nimport nbformat.v4.nbbase as nb\n\nfrom pathlib import Path\n\ndef code_cell(source: str, *, hidden: bool = False) -> dict:\n    kwargs = {}\n    if hidden:\n        kwargs[\"metadata\"] = {\"jupyter\": {\"source_hidden\": hidden}}\n\n    return nb.new_code_cell(source, **kwargs)\n\ndef new_notebook(cells: list[dict]) -> dict:\n    return nb.new_notebook(cells=cells)\n\ndef write_ipynb(nb: dict, file: Path) -> None:\n    file.write_text(jupytext.writes(nb, fmt=\"ipynb\"))\n\n```\n\n\n```python\n## src/juv/_uv.py\nimport os\n\nimport subprocess\n\nfrom uv import find_uv_bin\n\ndef uv(args: list[str], *, check: bool) -> subprocess.CompletedProcess:\n    \"\"\"Invoke a uv subprocess and return the result.\n\n    Parameters\n    ----------\n    args : list[str]\n        The arguments to pass to the subprocess.\n\n    check : bool\n        Whether to raise an exception if the subprocess returns a non-zero exit code.\n\n    Returns\n    -------\n    subprocess.CompletedProcess\n        The result of the subprocess.\n\n    \"\"\"\n    uv = os.fsdecode(find_uv_bin())\n    return subprocess.run([uv, *args], capture_output=True, check=check, env=os.environ)\n\n```\n\n\n```python\n## src/juv/_init.py\nimport sys\n\nimport tempfile\n\nimport typing\n\nfrom pathlib import Path\n\nimport rich\n\nfrom ._nbutils import code_cell, new_notebook, write_ipynb\n\nfrom ._uv import uv\n\ndef new_notebook_with_inline_metadata(\n    directory: Path,\n    python: str | None = None,\n) -> dict:\n    \"\"\"Create a new notebook with inline metadata.\n\n    Parameters\n    ----------\n    directory : pathlib.Path\n        A directory for uv to run `uv init` in. This is used so that we can\n        defer the selection of Python (if not specified) to uv.\n    python : str, optional\n        A version of the Python interpreter. Provided as `--python` to uv if specified.\n\n    Returns\n    -------\n    dict\n        A new notebook with a single code cell containing the contents of the\n        script generated by `uv init`.\n\n    \"\"\"\n    with tempfile.NamedTemporaryFile(\n        mode=\"w+\",\n        suffix=\".py\",\n        delete=True,\n        dir=directory,\n        encoding=\"utf-8\",\n    ) as f:\n        uv(\n            [\"init\", *([\"--python\", python] if python else []), \"--script\", f.name],\n            check=True,\n        )\n        contents = f.read().strip()\n        return new_notebook(cells=[code_cell(contents, hidden=True), code_cell(\"\")])\n\ndef get_first_non_conflicting_untitled_ipynb(directory: Path) -> Path:\n    if not (directory / \"Untitled.ipynb\").exists():\n        return directory / \"Untitled.ipynb\"\n\n    for i in range(1, 100):\n        if not (directory / f\"Untitled{i}.ipynb\").exists():\n            return directory / f\"Untitled{i}.ipynb\"\n\n    msg = \"Could not find an available UntitledX.ipynb\"\n    raise ValueError(msg)\n\ndef init(\n    path: Path | None,\n    python: str | None,\n    packages: typing.Sequence[str] = [],\n) -> Path:\n    \"\"\"Initialize a new notebook.\n\n    Parameters\n    ----------\n    path : pathlib.Path | None\n        The path to the new notebook. If None, a new Untitled.ipynb is created.\n    python : str | None\n        The version of Python to use. Passed as `--python` to uv.\n    packages : Sequence[str]\n        A list of packages to install in the new notebook.\n\n    Returns\n    -------\n    pathlib.Path\n        The path to the new notebook.\n\n    \"\"\"\n    if not path:\n        path = get_first_non_conflicting_untitled_ipynb(Path.cwd())\n\n    if path.suffix != \".ipynb\":\n        rich.print(\"File must have a `[cyan].ipynb[/cyan]` extension.\", file=sys.stderr)\n        sys.exit(1)\n\n    notebook = new_notebook_with_inline_metadata(path.parent, python)\n    write_ipynb(notebook, path)\n\n    if len(packages) > 0:\n        from ._add import add\n\n        add(path=path, packages=packages)\n\n    return path\n\n```\n\n\n",
        "eval_script": "import os\nimport sys\nimport subprocess\nimport tempfile\nimport typing\nfrom pathlib import Path\nimport rich\n\n# Mock functionality for 'uv'\ndef find_uv_bin():\n    return \"/usr/bin/uv\"  # Assume a path for the 'uv' binary for mock purposes\n\ndef uv(args: list[str], *, check: bool) -> subprocess.CompletedProcess:\n    \"\"\"Invoke a uv subprocess and return the result.\"\"\"\n    uv_path = os.fsdecode(find_uv_bin())\n    result = subprocess.CompletedProcess(args, 0, stdout=b'Mock output')\n    if check and result.returncode != 0:\n        raise subprocess.CalledProcessError(result.returncode, args)\n    return result\n\nimport jupytext\nimport nbformat.v4.nbbase as nb\n\ndef code_cell(source: str, *, hidden: bool = False) -> dict:\n    kwargs = {}\n    if hidden:\n        kwargs[\"metadata\"] = {\"jupyter\": {\"source_hidden\": hidden}}\n\n    return nb.new_code_cell(source, **kwargs)\n\ndef new_notebook(cells: list[dict]) -> dict:\n    return nb.new_notebook(cells=cells)\n\ndef write_ipynb(nb: dict, file: Path) -> None:\n    file.write_text(jupytext.writes(nb, fmt=\"ipynb\"))\n\ndef new_notebook_with_inline_metadata(directory: Path, python: str | None = None) -> dict:\n    \"\"\"Create a new notebook with inline metadata.\"\"\"\n    with tempfile.NamedTemporaryFile(\n        mode=\"w+\",\n        suffix=\".py\",\n        delete=True,\n        dir=directory,\n        encoding=\"utf-8\",\n    ) as f:\n        uv(\n            [\"init\", *([\"--python\", python] if python else []), \"--script\", f.name],\n            check=True,\n        )\n        contents = f.read().strip()\n        return new_notebook(cells=[code_cell(contents, hidden=True), code_cell(\"\")])\n\ndef get_first_non_conflicting_untitled_ipynb(directory: Path) -> Path:\n    if not (directory / \"Untitled.ipynb\").exists():\n        return directory / \"Untitled.ipynb\"\n\n    for i in range(1, 100):\n        if not (directory / f\"Untitled{i}.ipynb\").exists():\n            return directory / f\"Untitled{i}.ipynb\"\n\n    msg = \"Could not find an available UntitledX.ipynb\"\n    raise ValueError(msg)\n\ndef init(path: Path | None, python: str | None, packages: typing.Sequence[str] = []) -> Path:\n    \"\"\"Initialize a new notebook.\"\"\"\n    if not path:\n        path = get_first_non_conflicting_untitled_ipynb(Path.cwd())\n\n    if path.suffix != \".ipynb\":\n        rich.print(\"File must have a `[cyan].ipynb[/cyan]` extension.\", file=sys.stderr)\n        sys.exit(1)\n\n    notebook = new_notebook_with_inline_metadata(path.parent, python)\n    write_ipynb(notebook, path)\n\n    if len(packages) > 0:\n        # Mocking the 'add' function from the '.add' module\n        def add(path: Path, packages: typing.Sequence[str]):\n            print(f\"Mock add: Adding packages {packages} to {path}\")\n\n        add(path=path, packages=packages)\n\n    return path\n\n\n# Mock implementation of 'init_new_implementation' to test with\n\n\ndef test_init():\n    temp_dir = Path(\"/home/user/tmp\")\n    # Ensure the temporary directory exists\n    temp_dir.mkdir(parents=True, exist_ok=True)\n\n    # Test 1: No path provided\n    result_1 = init(None, None)\n    # Remove created file to allow init_new_implementation to reuse the name\n    if result_1.exists():\n        result_1.unlink()\n    result_1_new = init_new_implementation(None, None)\n    assert result_1 == result_1_new, \"Test failed: Result paths do not match for no path provided.\"\n\n    # Test 2: Path with python version\n    test_path_2 = temp_dir / \"Test2.ipynb\"\n    result_2 = init(test_path_2, \"3.8\")\n    result_2_new = init_new_implementation(test_path_2, \"3.8\")\n    assert result_2 == result_2_new, \"Test failed: Result paths do not match for path with specific python version.\"\n\n    # Test 3: Path with packages\n    test_path_3 = temp_dir / \"Test3.ipynb\"\n    result_3 = init(test_path_3, None, [\"numpy\", \"pandas\"])\n    result_3_new = init_new_implementation(test_path_3, None, [\"numpy\", \"pandas\"])\n    assert result_3 == result_3_new, \"Test failed: Result paths do not match for path with packages.\"\n\nif __name__ == \"__main__\":\n    test_init()"
    },
    {
        "func_name": "strip_python_frontmatter_comment",
        "idx": "116",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_cat.py",
        "orig_func": "def strip_python_frontmatter_comment(content: str) -> tuple[str, str]:\n    \"\"\"Remove frontmatter comment block from beginning of Python script.\n\n    Looks for content between # --- markers at start of file.\n\n    Args:\n        content: Full content of Python file\n\n    Returns:\n        tuple[str, str]: (frontmatter, remaining_content)\n\n    \"\"\"\n    lines = content.splitlines(keepends=True)\n    if not lines or lines[0].strip() != '# ---':\n        return ('', content)\n    for (i, line) in enumerate(lines[1:], 1):\n        if line.strip() == '# ---':\n            return (''.join(lines[:i + 1]), ''.join(lines[i + 1:]))\n    return ('', content)",
        "orig_context": "```python\n## src/juv/_cat.py\ndef strip_python_frontmatter_comment(content: str) -> tuple[str, str]:\n    \"\"\"Remove frontmatter comment block from beginning of Python script.\n\n    Looks for content between # --- markers at start of file.\n\n    Args:\n        content: Full content of Python file\n\n    Returns:\n        tuple[str, str]: (frontmatter, remaining_content)\n\n    \"\"\"\n    lines = content.splitlines(keepends=True)\n    if not lines or lines[0].strip() != \"# ---\":\n        return \"\", content\n\n    for i, line in enumerate(lines[1:], 1):\n        if line.strip() == \"# ---\":\n            return \"\".join(lines[: i + 1]), \"\".join(lines[i + 1 :])\n\n    return \"\", content\n\n```\n\n\n",
        "eval_script": "## src/juv/_cat.py\ndef strip_python_frontmatter_comment(content: str) -> tuple[str, str]:\n    \"\"\"Remove frontmatter comment block from beginning of Python script.\n\n    Looks for content between # --- markers at start of file.\n\n    Args:\n        content: Full content of Python file\n\n    Returns:\n        tuple[str, str]: (frontmatter, remaining_content)\n\n    \"\"\"\n    lines = content.splitlines(keepends=True)\n    if not lines or lines[0].strip() != \"# ---\":\n        return \"\", content\n\n    for i, line in enumerate(lines[1:], 1):\n        if line.strip() == \"# ---\":\n            return \"\".join(lines[: i + 1]), \"\".join(lines[i + 1 :])\n\n    return \"\", content\n\n\n\ndef test_strip_python_frontmatter_comment():\n    # Test case 1: Proper frontmatter\n    content1 = \"# ---\\n# Author: Example\\n# ---\\nprint('Hello World')\\n\"\n    assert strip_python_frontmatter_comment(content1) == strip_python_frontmatter_comment_new_implementation(content1)\n\n    # Test case 2: No frontmatter\n    content2 = \"print('Hello World')\\n\"\n    assert strip_python_frontmatter_comment(content2) == strip_python_frontmatter_comment_new_implementation(content2)\n\n    # Test case 3: Incorrect frontmatter\n    content3 = \"# ---\\n# Author: Example\\nprint('Hello World')\\n\"\n    assert strip_python_frontmatter_comment(content3) == strip_python_frontmatter_comment_new_implementation(content3)\n\nif __name__ == \"__main__\":\n    test_strip_python_frontmatter_comment()"
    },
    {
        "func_name": "parse_date",
        "idx": "117",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_stamp.py",
        "orig_func": "def parse_date(date_str: str) -> OffsetDateTime:\n    \"\"\"Parse a common ISO 8601 date string (using the system's local timezone).\n\n    Defaults to midnight in the local timezone.\n    \"\"\"\n    try:\n        date = Date.parse_common_iso(date_str).add(days=1)\n    except ValueError as err:\n        msg = f\"'{date_str}' could not be parsed as a valid date.\"\n        raise ValueError(msg) from err\n    if 'JUV_TZ' in os.environ:\n        dt = ZonedDateTime(date.year, date.month, date.day, tz=os.environ['JUV_TZ'])\n    else:\n        dt = SystemDateTime(date.year, date.month, date.day)\n    return dt.to_fixed_offset()",
        "orig_context": "```python\n## src/juv/_stamp.py\nimport os\n\nfrom whenever import Date, OffsetDateTime, SystemDateTime, ZonedDateTime\n\ndef parse_date(date_str: str) -> OffsetDateTime:\n    \"\"\"Parse a common ISO 8601 date string (using the system's local timezone).\n\n    Defaults to midnight in the local timezone.\n    \"\"\"\n    try:\n        date = Date.parse_common_iso(date_str).add(days=1)\n    except ValueError as err:\n        msg = f\"'{date_str}' could not be parsed as a valid date.\"\n        raise ValueError(msg) from err\n\n    if \"JUV_TZ\" in os.environ:\n        # used in tests\n        dt = ZonedDateTime(date.year, date.month, date.day, tz=os.environ[\"JUV_TZ\"])\n    else:\n        dt = SystemDateTime(date.year, date.month, date.day)\n\n    return dt.to_fixed_offset()\n\n```\n\n\n",
        "eval_script": "# Mock implementations of the `whenever` module classes\n\nimport os  # Required for checking the environment variable\n\n# `Date` class mock\nclass Date:\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    @staticmethod\n    def parse_common_iso(date_str):\n        # Simple ISO date parsing mock, parsing \"YYYY-MM-DD\"\n        year, month, day = map(int, date_str.split('-'))\n        return Date(year, month, day)\n\n    def add(self, days=0):\n        # Naive increment by 1 day mock\n        return Date(self.year, self.month, self.day + days)\n\n# `OffsetDateTime` mock\nclass OffsetDateTime:\n    def __init__(self, year, month, day, offset=\"+00:00\"):\n        self.year = year\n        self.month = month\n        self.day = day\n        self.offset = offset\n\n    def to_fixed_offset(self):\n        return self\n\n# `SystemDateTime` mock\nclass SystemDateTime(OffsetDateTime):\n    def __init__(self, year, month, day):\n        super().__init__(year, month, day)\n        # Simulate system timezone as UTC\n        self.offset = \"+00:00\"\n\n# `ZonedDateTime` mock\nclass ZonedDateTime(OffsetDateTime):\n    def __init__(self, year, month, day, tz):\n        super().__init__(year, month, day)\n        # Store timezone, but for simple mock, use UTC offset\n        self.offset = \"+00:00\"\n\n# The original code with no changes\ndef parse_date(date_str: str) -> OffsetDateTime:\n    \"\"\"Parse a common ISO 8601 date string (using the system's local timezone).\n\n    Defaults to midnight in the local timezone.\n    \"\"\"\n    try:\n        date = Date.parse_common_iso(date_str).add(days=1)\n    except ValueError as err:\n        msg = f\"'{date_str}' could not be parsed as a valid date.\"\n        raise ValueError(msg) from err\n\n    if \"JUV_TZ\" in os.environ:\n        # used in tests\n        dt = ZonedDateTime(date.year, date.month, date.day, tz=os.environ[\"JUV_TZ\"])\n    else:\n        dt = SystemDateTime(date.year, date.month, date.day)\n\n    return dt.to_fixed_offset()\n\n# Test function to check if both parse_date and parse_date_new_implementation behave the same\ndef test_parse_date():\n    # Define a mock parse_date_new_implementation function for testing purposes.\n\n\n    # Test cases\n    test_case_1 = \"2023-10-05\"\n    test_case_2 = \"2020-02-29\"\n    test_case_3 = \"2000-01-01\"\n\n    # Actual testing\n    assert parse_date(test_case_1).__dict__ == parse_date_new_implementation(test_case_1).__dict__\n    assert parse_date(test_case_2).__dict__ == parse_date_new_implementation(test_case_2).__dict__\n    assert parse_date(test_case_3).__dict__ == parse_date_new_implementation(test_case_3).__dict__\n\n# __main__ function\nif __name__ == \"__main__\":\n    test_parse_date()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "get_git_timestamp",
        "idx": "118",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_stamp.py",
        "orig_func": "def get_git_timestamp(rev: str) -> OffsetDateTime:\n    \"\"\"Get the ISO 8601 timestamp of a Git revision.\"\"\"\n    ts = subprocess.check_output(['git', 'show', '-s', '--format=%cI', rev], text=True)\n    return OffsetDateTime.parse_rfc3339(ts.strip())",
        "orig_context": "```python\n## src/juv/_stamp.py\nimport subprocess\n\nfrom whenever import Date, OffsetDateTime, SystemDateTime, ZonedDateTime\n\ndef get_git_timestamp(rev: str) -> OffsetDateTime:\n    \"\"\"Get the ISO 8601 timestamp of a Git revision.\"\"\"\n    ts = subprocess.check_output(  # noqa: S603\n        [\"git\", \"show\", \"-s\", \"--format=%cI\", rev],  # noqa: S607\n        text=True,\n    )\n    return OffsetDateTime.parse_rfc3339(ts.strip())\n\n```\n\n\n",
        "eval_script": "import subprocess\nfrom unittest.mock import patch\nfrom whenever import OffsetDateTime  # Assuming there's a parse_rfc3339 method\n\ndef get_git_timestamp(rev: str) -> OffsetDateTime:\n    \"\"\"Get the ISO 8601 timestamp of a Git revision.\"\"\"\n    ts = subprocess.check_output(  # noqa: S603\n        [\"git\", \"show\", \"-s\", \"--format=%cI\", rev],  # noqa: S607\n        text=True,\n    )\n    return OffsetDateTime.parse_rfc3339(ts.strip())\n\n\n\ndef test_get_git_timestamp():\n    with patch('subprocess.check_output', return_value=\"2023-10-01T12:30:00+00:00\"):\n        timestamp_original = get_git_timestamp('mock-rev')\n        timestamp_new = get_git_timestamp_new_implementation('mock-rev')\n        assert timestamp_original == timestamp_new, \"Mismatch for revision 'mock-rev'\"\n    \n    with patch('subprocess.check_output', return_value=\"2023-11-15T09:45:00+00:00\"):\n        timestamp_original = get_git_timestamp('mock-rev-1')\n        timestamp_new = get_git_timestamp_new_implementation('mock-rev-1')\n        assert timestamp_original == timestamp_new, \"Mismatch for revision 'mock-rev-1'\"\n    \n    with patch('subprocess.check_output', return_value=\"2023-09-10T23:59:59+00:00\"):\n        timestamp_original = get_git_timestamp('mock-rev-2')\n        timestamp_new = get_git_timestamp_new_implementation('mock-rev-2')\n        assert timestamp_original == timestamp_new, \"Mismatch for revision 'mock-rev-2'\"\n\nif __name__ == \"__main__\":\n    test_get_git_timestamp()"
    },
    {
        "func_name": "uv",
        "idx": "119",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_uv.py",
        "orig_func": "def uv(args: list[str], *, check: bool) -> subprocess.CompletedProcess:\n    \"\"\"Invoke a uv subprocess and return the result.\n\n    Parameters\n    ----------\n    args : list[str]\n        The arguments to pass to the subprocess.\n\n    check : bool\n        Whether to raise an exception if the subprocess returns a non-zero exit code.\n\n    Returns\n    -------\n    subprocess.CompletedProcess\n        The result of the subprocess.\n\n    \"\"\"\n    uv = os.fsdecode(find_uv_bin())\n    return subprocess.run([uv, *args], capture_output=True, check=check, env=os.environ)",
        "orig_context": "```python\n## src/juv/_uv.py\nimport os\n\nimport subprocess\n\nfrom uv import find_uv_bin\n\ndef uv(args: list[str], *, check: bool) -> subprocess.CompletedProcess:\n    \"\"\"Invoke a uv subprocess and return the result.\n\n    Parameters\n    ----------\n    args : list[str]\n        The arguments to pass to the subprocess.\n\n    check : bool\n        Whether to raise an exception if the subprocess returns a non-zero exit code.\n\n    Returns\n    -------\n    subprocess.CompletedProcess\n        The result of the subprocess.\n\n    \"\"\"\n    uv = os.fsdecode(find_uv_bin())\n    return subprocess.run([uv, *args], capture_output=True, check=check, env=os.environ)\n\n```\n\n\n",
        "eval_script": "import os\nimport subprocess\nimport sys\n\ndef find_uv_bin():\n    uv_mock_path = '/home/user/tmp/mock_uv'\n    os.makedirs('/home/user/tmp', exist_ok=True)\n    with open(uv_mock_path, 'w') as f:\n        f.write('#!/bin/bash\\n')\n        f.write('echo \"Mock uv command executed with arguments: $@\"\\n')\n    os.chmod(uv_mock_path, 0o755)\n    return uv_mock_path\n\ndef uv(args: list[str], *, check: bool) -> subprocess.CompletedProcess:\n    uv = os.fsdecode(find_uv_bin())\n    return subprocess.run([uv, *args], capture_output=True, check=check, env=os.environ)\n\n\n\ndef test_uv():\n    # Test case 1: Basic argument list\n    result_1 = uv(['arg1', 'arg2'], check=True)\n    result_2 = uv_new_implementation(['arg1', 'arg2'], check=True)\n    assert result_1.stdout == result_2.stdout, \"Mismatch in test 1 stdout\"\n    assert result_1.returncode == result_2.returncode, \"Mismatch in test 1 returncode\"\n\n    # Test case 2: Empty argument list\n    result_1 = uv([], check=True)\n    result_2 = uv_new_implementation([], check=True)\n    assert result_1.stdout == result_2.stdout, \"Mismatch in test 2 stdout\"\n    assert result_1.returncode == result_2.returncode, \"Mismatch in test 2 returncode\"\n\n    # Test case 3: With different environment variable\n    env = os.environ.copy()\n    env[\"TEST_VAR\"] = \"1\"\n    result_1 = subprocess.run([os.fsdecode(find_uv_bin())], capture_output=True, check=True, env=env)\n    result_2 = subprocess.run([os.fsdecode(find_uv_bin())], capture_output=True, check=True, env=env)\n    assert result_1.stdout == result_2.stdout, \"Mismatch in test 3 stdout\"\n    assert result_1.returncode == result_2.returncode, \"Mismatch in test 3 returncode\"\n\nif __name__ == '__main__':\n    test_uv()"
    },
    {
        "func_name": "notebook_contents",
        "idx": "120",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_cat.py",
        "orig_func": "def notebook_contents(nb: Path | dict, *, script: bool) -> str:\n    fmt = 'py:percent' if script else 'md'\n    notebook = nb if isinstance(nb, dict) else jupytext.read(nb)\n    contents = jupytext.writes(notebook, fmt=fmt)\n    if script:\n        (_, contents) = strip_python_frontmatter_comment(contents)\n    else:\n        (_, contents) = strip_markdown_header(contents)\n    return contents.lstrip()",
        "orig_context": "```python\n## src/juv/_cat.py\nimport re\n\nimport jupytext\n\nfrom pathlib import Path\n\ndef strip_markdown_header(content: str) -> tuple[str, str]:\n    # Match content between first set of --- markers\n    match = re.match(r\"^---\\n.*?\\n---\\n(.*)$\", content, re.DOTALL)\n    if match:\n        header = content[: content.find(match.group(1))]\n        return header, match.group(1)\n    return \"\", content\n\ndef strip_python_frontmatter_comment(content: str) -> tuple[str, str]:\n    \"\"\"Remove frontmatter comment block from beginning of Python script.\n\n    Looks for content between # --- markers at start of file.\n\n    Args:\n        content: Full content of Python file\n\n    Returns:\n        tuple[str, str]: (frontmatter, remaining_content)\n\n    \"\"\"\n    lines = content.splitlines(keepends=True)\n    if not lines or lines[0].strip() != \"# ---\":\n        return \"\", content\n\n    for i, line in enumerate(lines[1:], 1):\n        if line.strip() == \"# ---\":\n            return \"\".join(lines[: i + 1]), \"\".join(lines[i + 1 :])\n\n    return \"\", content\n\ndef notebook_contents(nb: Path | dict, *, script: bool) -> str:\n    fmt = \"py:percent\" if script else \"md\"\n    notebook = nb if isinstance(nb, dict) else jupytext.read(nb)\n    contents = jupytext.writes(notebook, fmt=fmt)\n    if script:\n        _, contents = strip_python_frontmatter_comment(contents)\n    else:\n        _, contents = strip_markdown_header(contents)\n    return contents.lstrip()\n\n```\n\n\n",
        "eval_script": "import re\nfrom pathlib import Path\n\n# Mock import for jupytext as we're focusing on internal logic without I/O\nimport jupytext\n\n# Mock jupytext read and write functions to avoid file I/O.\ndef mock_jupytext_read(nb):\n    return nb  # Directly return the notebook (dict) as it's user-provided in test\n\ndef mock_jupytext_writes(notebook, fmt=None):\n    # A simple mock that converts the notebook dict to a string based on format\n    if fmt == \"py:percent\":\n        return \"# ---\\n# This is a script comment\\n# ---\\nprint('Hello, World!')\"\n    else:\n        return \"---\\n# This is markdown\\n---\\n# Hello, World!\"\n\n# Patch the jupytext functions\njupytext.read = mock_jupytext_read\njupytext.writes = mock_jupytext_writes\n\ndef strip_markdown_header(content: str) -> tuple[str, str]:\n    match = re.match(r\"^---\\n.*?\\n---\\n(.*)$\", content, re.DOTALL)\n    if match:\n        header = content[: content.find(match.group(1))]\n        return header, match.group(1)\n    return \"\", content\n\ndef strip_python_frontmatter_comment(content: str) -> tuple[str, str]:\n    lines = content.splitlines(keepends=True)\n    if not lines or lines[0].strip() != \"# ---\":\n        return \"\", content\n\n    for i, line in enumerate(lines[1:], 1):\n        if line.strip() == \"# ---\":\n            return \"\".join(lines[: i + 1]), \"\".join(lines[i + 1 :])\n\n    return \"\", content\n\ndef notebook_contents(nb: Path | dict, *, script: bool) -> str:\n    fmt = \"py:percent\" if script else \"md\"\n    notebook = nb if isinstance(nb, dict) else jupytext.read(nb)\n    contents = jupytext.writes(notebook, fmt=fmt)\n    if script:\n        _, contents = strip_python_frontmatter_comment(contents)\n    else:\n        _, contents = strip_markdown_header(contents)\n    return contents.lstrip()\n\n\n\ndef test_notebook_contents():\n    mock_notebook = {\"cells\": [{\"cell_type\": \"code\", \"source\": \"print('Hello, World!')\"}]}\n\n    # Test script output\n    result_original_script = notebook_contents(mock_notebook, script=True)\n    result_new_script = notebook_contents_new_implementation(mock_notebook, script=True)\n    assert result_original_script == result_new_script, \"Script format outputs do not match\"\n\n    # Test markdown output\n    result_original_markdown = notebook_contents(mock_notebook, script=False)\n    result_new_markdown = notebook_contents_new_implementation(mock_notebook, script=False)\n    assert result_original_markdown == result_new_markdown, \"Markdown format outputs do not match\"\n\n    # Test with a different mock notebook structure\n    mock_notebook_with_comment = {\n        \"cells\": [\n            {\"cell_type\": \"markdown\", \"source\": \"---\\nThis is markdown\\n---\\nHello, World!\"}\n        ]\n    }\n\n    result_original_markdown2 = notebook_contents(mock_notebook_with_comment, script=False)\n    result_new_markdown2 = notebook_contents_new_implementation(mock_notebook_with_comment, script=False)\n    assert result_original_markdown2 == result_new_markdown2, \"Different markdown inputs do not match\"\n\nif __name__ == \"__main__\":\n    test_notebook_contents()"
    },
    {
        "func_name": "strip_markdown_header",
        "idx": "121",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_cat.py",
        "orig_func": "def strip_markdown_header(content: str) -> tuple[str, str]:\n    match = re.match('^---\\\\n.*?\\\\n---\\\\n(.*)$', content, re.DOTALL)\n    if match:\n        header = content[:content.find(match.group(1))]\n        return (header, match.group(1))\n    return ('', content)",
        "orig_context": "```python\n## src/juv/_cat.py\nimport re\n\ndef strip_markdown_header(content: str) -> tuple[str, str]:\n    # Match content between first set of --- markers\n    match = re.match(r\"^---\\n.*?\\n---\\n(.*)$\", content, re.DOTALL)\n    if match:\n        header = content[: content.find(match.group(1))]\n        return header, match.group(1)\n    return \"\", content\n\n```\n\n\n",
        "eval_script": "## src/juv/_cat.py\nimport re\n\ndef strip_markdown_header(content: str) -> tuple[str, str]:\n    # Match content between first set of --- markers\n    match = re.match(r\"^---\\n.*?\\n---\\n(.*)$\", content, re.DOTALL)\n    if match:\n        header = content[: content.find(match.group(1))]\n        return header, match.group(1)\n    return \"\", content\n\n# Placeholder for the new implementation. This needs to be defined elsewhere or later.\n\n\ndef test_strip_markdown_header():\n    assert strip_markdown_header(\"---\\nheader: value\\n---\\nContent here\") == strip_markdown_header_new_implementation(\"---\\nheader: value\\n---\\nContent here\")\n    assert strip_markdown_header(\"---\\nkey: val\\n---\\nAnother content block\") == strip_markdown_header_new_implementation(\"---\\nkey: val\\n---\\nAnother content block\")\n    assert strip_markdown_header(\"---\\nSome header\\n---\\nYet another block\") == strip_markdown_header_new_implementation(\"---\\nSome header\\n---\\nYet another block\")\n\nif __name__ == \"__main__\":\n    test_strip_markdown_header()"
    },
    {
        "func_name": "prepare_run_script_and_uv_run_args",
        "idx": "122",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_run_template.py",
        "orig_func": "def prepare_run_script_and_uv_run_args(*, runtime: Runtime, meta: str, target: pathlib.Path, python: str | None, with_args: typing.Sequence[str], jupyter_args: typing.Sequence[str], no_project: bool, mode: str) -> tuple[str, list[str]]:\n    script = runtime.script_template().format(meta=meta, notebook=target, args=jupyter_args, SETUP_JUPYTER_DATA_DIR=SETUP_JUPYTER_DATA_DIR, is_managed=mode == 'managed')\n    args = ['run', *(['--no-project'] if no_project else []), *([f'--python={python}'] if python else []), f'--with={runtime.as_with_arg()}', *(['--with=' + ','.join(with_args)] if with_args else []), '-']\n    return (script, args)",
        "orig_context": "```python\n## src/juv/_run_template.py\nimport typing\n\nfrom dataclasses import dataclass\n\nimport pathlib\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nclass Runtime:\n    name: RuntimeName\n    version: str | None = None\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> Runtime:\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):  # noqa: PLR2004\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        # lab is actually jupyterlab\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        # append version if present\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        # notebook v6 requires setuptools\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\nSETUP_JUPYTER_DATA_DIR = \"\"\"\nimport tempfile\nimport signal\nfrom pathlib import Path\nimport os\nimport sys\n\nfrom platformdirs import user_data_dir\n\njuv_data_dir = Path(user_data_dir(\"juv\"))\njuv_data_dir.mkdir(parents=True, exist_ok=True)\n\ntemp_dir = tempfile.TemporaryDirectory(dir=juv_data_dir)\nmerged_dir = Path(temp_dir.name)\n\ndef handle_termination(signum, frame):\n    temp_dir.cleanup()\n    sys.exit(0)\n\nsignal.signal(signal.SIGTERM, handle_termination)\nsignal.signal(signal.SIGINT, handle_termination)\n\nconfig_paths = []\nroot_data_dir = Path(sys.prefix) / \"share\" / \"jupyter\"\njupyter_paths = [root_data_dir]\nfor path in map(Path, sys.path):\n    if not path.name == \"site-packages\":\n        continue\n    venv_path = path.parent.parent.parent\n    config_paths.append(venv_path / \"etc\" / \"jupyter\")\n    data_dir = venv_path / \"share\" / \"jupyter\"\n    if not data_dir.exists() or str(data_dir) == str(root_data_dir):\n        continue\n\n    jupyter_paths.append(data_dir)\n\n\nfor path in reversed(jupyter_paths):\n    for item in path.rglob('*'):\n        if item.is_file():\n            dest = merged_dir / item.relative_to(path)\n            dest.parent.mkdir(parents=True, exist_ok=True)\n            try:\n                os.link(item, dest)\n            except FileExistsError:\n                pass\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nos.environ[\"JUPYTER_CONFIG_PATH\"] = os.pathsep.join(map(str, config_paths))\n\"\"\"\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\ndef prepare_run_script_and_uv_run_args(  # noqa: PLR0913\n    *,\n    runtime: Runtime,\n    meta: str,\n    target: pathlib.Path,\n    python: str | None,\n    with_args: typing.Sequence[str],\n    jupyter_args: typing.Sequence[str],\n    no_project: bool,\n    mode: str,\n) -> tuple[str, list[str]]:\n    script = runtime.script_template().format(\n        meta=meta,\n        notebook=target,\n        args=jupyter_args,\n        SETUP_JUPYTER_DATA_DIR=SETUP_JUPYTER_DATA_DIR,\n        is_managed=mode == \"managed\",\n    )\n    args = [\n        \"run\",\n        *([\"--no-project\"] if no_project else []),\n        *([f\"--python={python}\"] if python else []),\n        f\"--with={runtime.as_with_arg()}\",\n        *([\"--with=\" + \",\".join(with_args)] if with_args else []),\n        \"-\",\n    ]\n    return script, args\n\n```\n\n\n",
        "eval_script": "## src/juv/_run_template.py\nimport typing\n\nfrom dataclasses import dataclass\n\nimport pathlib\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nclass Runtime:\n    name: RuntimeName\n    version: str | None = None\n\n    def __init__(self, name: RuntimeName, version: str | None = None):\n        self.name = name\n        self.version = version\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> 'Runtime':\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):  # noqa: PLR2004\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        # lab is actually jupyterlab\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        # append version if present\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        # notebook v6 requires setuptools\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\nSETUP_JUPYTER_DATA_DIR = \"\"\"\nimport tempfile\nimport signal\nfrom pathlib import Path\nimport os\nimport sys\n\nfrom platformdirs import user_data_dir\n\njuv_data_dir = Path(user_data_dir(\"juv\"))\njuv_data_dir.mkdir(parents=True, exist_ok=True)\n\ntemp_dir = tempfile.TemporaryDirectory(dir=juv_data_dir)\nmerged_dir = Path(temp_dir.name)\n\ndef handle_termination(signum, frame):\n    temp_dir.cleanup()\n    sys.exit(0)\n\nsignal.signal(signal.SIGTERM, handle_termination)\nsignal.signal(signal.SIGINT, handle_termination)\n\nconfig_paths = []\nroot_data_dir = Path(sys.prefix) / \"share\" / \"jupyter\"\njupyter_paths = [root_data_dir]\nfor path in map(Path, sys.path):\n    if not path.name == \"site-packages\":\n        continue\n    venv_path = path.parent.parent.parent\n    config_paths.append(venv_path / \"etc\" / \"jupyter\")\n    data_dir = venv_path / \"share\" / \"jupyter\"\n    if not data_dir.exists() or str(data_dir) == str(root_data_dir):\n        continue\n\n    jupyter_paths.append(data_dir)\n\n\nfor path in reversed(jupyter_paths):\n    for item in path.rglob('*'):\n        if item.is_file():\n            dest = merged_dir / item.relative_to(path)\n            dest.parent.mkdir(parents=True, exist_ok=True)\n            try:\n                os.link(item, dest)\n            except FileExistsError:\n                pass\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nos.environ[\"JUPYTER_CONFIG_PATH\"] = os.pathsep.join(map(str, config_paths))\n\"\"\"\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\ndef prepare_run_script_and_uv_run_args(  # noqa: PLR0913\n    *,\n    runtime: Runtime,\n    meta: str,\n    target: pathlib.Path,\n    python: str | None,\n    with_args: typing.Sequence[str],\n    jupyter_args: typing.Sequence[str],\n    no_project: bool,\n    mode: str,\n) -> tuple[str, list[str]]:\n    script = runtime.script_template().format(\n        meta=meta,\n        notebook=target,\n        args=jupyter_args,\n        SETUP_JUPYTER_DATA_DIR=SETUP_JUPYTER_DATA_DIR,\n        is_managed=mode == \"managed\",\n    )\n    args = [\n        \"run\",\n        *([\"--no-project\"] if no_project else []),\n        *([f\"--python={python}\"] if python else []),\n        f\"--with={runtime.as_with_arg()}\",\n        *([\"--with=\" + \",\".join(with_args)] if with_args else []),\n        \"-\",\n    ]\n    return script, args\n\n\n\ndef test_prepare_run_script_and_uv_run_args():\n    runtime1 = Runtime(\"notebook\", \"6.1.0\")\n    runtime2 = Runtime(\"lab\")\n    runtime3 = Runtime(\"nbclassic\", \"1.2.3\")\n    \n    meta = \"metadata\"\n    target = pathlib.Path(\"/home/user/tmp/notebook.ipynb\")\n    python_version = \"3.8\"\n    \n    # Test Case 1\n    script1, args1 = prepare_run_script_and_uv_run_args(\n        runtime=runtime1,\n        meta=meta,\n        target=target,\n        python=python_version,\n        with_args=[\"arg1\", \"arg2\"],\n        jupyter_args=[\"--test-mode\"],\n        no_project=False,\n        mode=\"managed\",\n    )\n    script1_new, args1_new = prepare_run_script_and_uv_run_args_new_implementation(\n        runtime=runtime1,\n        meta=meta,\n        target=target,\n        python=python_version,\n        with_args=[\"arg1\", \"arg2\"],\n        jupyter_args=[\"--test-mode\"],\n        no_project=False,\n        mode=\"managed\",\n    )\n    assert script1 == script1_new, \"Test Case 1: Script mismatch\"\n    assert args1 == args1_new, \"Test Case 1: Args mismatch\"\n\n    # Test Case 2\n    script2, args2 = prepare_run_script_and_uv_run_args(\n        runtime=runtime2,\n        meta=meta,\n        target=target,\n        python=None,\n        with_args=[],\n        jupyter_args=[],\n        no_project=True,\n        mode=\"unmanaged\",\n    )\n    script2_new, args2_new = prepare_run_script_and_uv_run_args_new_implementation(\n        runtime=runtime2,\n        meta=meta,\n        target=target,\n        python=None,\n        with_args=[],\n        jupyter_args=[],\n        no_project=True,\n        mode=\"unmanaged\",\n    )\n    assert script2 == script2_new, \"Test Case 2: Script mismatch\"\n    assert args2 == args2_new, \"Test Case 2: Args mismatch\"\n\n    # Test Case 3\n    script3, args3 = prepare_run_script_and_uv_run_args(\n        runtime=runtime3,\n        meta=meta,\n        target=target,\n        python=python_version,\n        with_args=[\"arg3\"],\n        jupyter_args=[\"--test\", \"--config\"],\n        no_project=True,\n        mode=\"managed\",\n    )\n    script3_new, args3_new = prepare_run_script_and_uv_run_args_new_implementation(\n        runtime=runtime3,\n        meta=meta,\n        target=target,\n        python=python_version,\n        with_args=[\"arg3\"],\n        jupyter_args=[\"--test\", \"--config\"],\n        no_project=True,\n        mode=\"managed\",\n    )\n    assert script3 == script3_new, \"Test Case 3: Script mismatch\"\n    assert args3 == args3_new, \"Test Case 3: Args mismatch\"\n\nif __name__ == \"__main__\":\n    test_prepare_run_script_and_uv_run_args()"
    },
    {
        "func_name": "Runtime.try_from_specifier",
        "idx": "123",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_run_template.py",
        "orig_func": "@classmethod\ndef try_from_specifier(cls, value: str) -> Runtime:\n    if '@' in value:\n        parts = value.split('@')\n    elif '==' in value:\n        parts = value.split('==')\n    else:\n        parts = [value]\n    if len(parts) == 2 and is_notebook_kind(parts[0]):\n        return Runtime(parts[0], parts[1])\n    if len(parts) == 1 and is_notebook_kind(parts[0]):\n        return Runtime(parts[0])\n    msg = f'Invalid runtime specifier: {value}'\n    raise ValueError(msg)",
        "orig_context": "```python\n## src/juv/_run_template.py\nimport typing\n\nfrom dataclasses import dataclass\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nclass Runtime:\n    name: RuntimeName\n    version: str | None = None\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> Runtime:\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):  # noqa: PLR2004\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        # lab is actually jupyterlab\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        # append version if present\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        # notebook v6 requires setuptools\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\n```\n\n\n",
        "eval_script": "from typing import Optional, TypeGuard, Literal\nfrom dataclasses import dataclass, field\n\nRuntimeName = Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\n@dataclass\nclass Runtime:\n    name: RuntimeName\n    version: Optional[str] = None\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> 'Runtime':\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):\n            return cls(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return cls(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n\n    @classmethod\n\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\ndef test_try_from_specifier():\n    # Test cases\n    assert Runtime.try_from_specifier(\"notebook@7.0.0\") == Runtime.try_from_specifier_new_implementation(\"notebook@7.0.0\")\n    assert Runtime.try_from_specifier(\"lab\") == Runtime.try_from_specifier_new_implementation(\"lab\")\n    assert Runtime.try_from_specifier(\"nbclassic==1.2.3\") == Runtime.try_from_specifier_new_implementation(\"nbclassic==1.2.3\")\n    \nif __name__ == \"__main__\":\n    test_try_from_specifier()"
    },
    {
        "func_name": "Runtime.script_template",
        "idx": "124",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_run_template.py",
        "orig_func": "def script_template(self) -> str:\n    if self.name == 'lab':\n        return LAB\n    if self.name == 'notebook':\n        if self.version and self.version.startswith('6'):\n            return NOTEBOOK_6\n        return NOTEBOOK\n    if self.name == 'nbclassic':\n        return NBCLASSIC\n    msg = f'Invalid self: {self.name}'\n    raise ValueError(msg)",
        "orig_context": "```python\n## src/juv/_run_template.py\nimport typing\n\nfrom dataclasses import dataclass\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nclass Runtime:\n    name: RuntimeName\n    version: str | None = None\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> Runtime:\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):  # noqa: PLR2004\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        # lab is actually jupyterlab\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        # append version if present\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        # notebook v6 requires setuptools\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\n```\n\n\n",
        "eval_script": "# Revised Code\n\nimport typing\nfrom dataclasses import dataclass\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nclass Runtime:\n    name: RuntimeName\n    version: str | None = None\n\n    def __init__(self, name: RuntimeName, version: str | None = None):\n        self.name = name\n        self.version = version\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> 'Runtime':\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n\n\n    def as_with_arg(self) -> str:\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n        if self.version:\n            with_ += f\"=={self.version}\"\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n        return with_\n\ndef test_script_template():\n    runtime_lab = Runtime(\"lab\")\n    runtime_notebook = Runtime(\"notebook\")\n    runtime_notebook_6 = Runtime(\"notebook\", \"6.0\")\n    runtime_nbclassic = Runtime(\"nbclassic\")\n\n    assert runtime_lab.script_template() == runtime_lab.script_template_new_implementation()\n    assert runtime_notebook.script_template() == runtime_notebook.script_template_new_implementation()\n    assert runtime_notebook_6.script_template() == runtime_notebook_6.script_template_new_implementation()\n    assert runtime_nbclassic.script_template() == runtime_nbclassic.script_template_new_implementation()\n\nif __name__ == \"__main__\":\n    test_script_template()"
    },
    {
        "func_name": "Runtime.as_with_arg",
        "idx": "125",
        "repo_name": "manzt___juv",
        "func_path": "src/juv/_run_template.py",
        "orig_func": "def as_with_arg(self) -> str:\n    with_ = 'jupyterlab' if self.name == 'lab' else self.name\n    if self.version:\n        with_ += f'=={self.version}'\n    if self.name == 'notebook' and self.version:\n        with_ += ',setuptools'\n    return with_",
        "orig_context": "```python\n## src/juv/_run_template.py\nimport typing\n\nfrom dataclasses import dataclass\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nclass Runtime:\n    name: RuntimeName\n    version: str | None = None\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> Runtime:\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):  # noqa: PLR2004\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        # lab is actually jupyterlab\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        # append version if present\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        # notebook v6 requires setuptools\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\n```\n\n\n",
        "eval_script": "import typing\nfrom dataclasses import dataclass\n\nRuntimeName = typing.Literal[\"notebook\", \"lab\", \"nbclassic\"]\n\ndef is_notebook_kind(kind: str) -> typing.TypeGuard[RuntimeName]:\n    return kind in {\"notebook\", \"lab\", \"nbclassic\"}\n\nLAB = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom jupyterlab.labapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"jupyterlab\")\n    print(\"JUV_MANGED=\" + \"jupyterlab\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-lab\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.app import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNOTEBOOK_6 = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom notebook.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"notebook\")\n    print(\"JUV_MANGED=\" + \"notebook\" + \",\" + version, file=sys.stderr)\n\nsys.argv = [\"jupyter-notebook\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nNBCLASSIC = \"\"\"\n{meta}\nimport os\nimport sys\n\nfrom nbclassic.notebookapp import main\n\n{SETUP_JUPYTER_DATA_DIR}\n\nif {is_managed}:\n    import importlib.metadata\n\n    version = importlib.metadata.version(\"nbclassic\")\n    print(\"JUV_MANGED=\" + \"nbclassic\" + \",\" + version, file=sys.stderr)\n\nos.environ[\"JUPYTER_DATA_DIR\"] = str(merged_dir)\nsys.argv = [\"jupyter-nbclassic\", \"{notebook}\", *{args}]\nmain()\n\"\"\"\n\nclass Runtime:\n    def __init__(self, name: RuntimeName, version: str | None = None):\n        self.name = name\n        self.version = version\n\n    @classmethod\n    def try_from_specifier(cls, value: str) -> 'Runtime':\n        if \"@\" in value:\n            parts = value.split(\"@\")\n        elif \"==\" in value:\n            parts = value.split(\"==\")\n        else:\n            parts = [value]\n\n        if len(parts) == 2 and is_notebook_kind(parts[0]):  # noqa: PLR2004\n            return Runtime(parts[0], parts[1])\n\n        if len(parts) == 1 and is_notebook_kind(parts[0]):\n            return Runtime(parts[0])\n\n        msg = f\"Invalid runtime specifier: {value}\"\n        raise ValueError(msg)\n\n    def script_template(self) -> str:\n        if self.name == \"lab\":\n            return LAB\n        if self.name == \"notebook\":\n            if self.version and self.version.startswith(\"6\"):\n                return NOTEBOOK_6\n            return NOTEBOOK\n        if self.name == \"nbclassic\":\n            return NBCLASSIC\n        msg = f\"Invalid self: {self.name}\"\n        raise ValueError(msg)\n\n    def as_with_arg(self) -> str:\n        with_ = \"jupyterlab\" if self.name == \"lab\" else self.name\n\n        if self.version:\n            with_ += f\"=={self.version}\"\n\n        if self.name == \"notebook\" and self.version:\n            with_ += \",setuptools\"\n\n        return with_\n\n\n\ndef test_as_with_arg():\n    # Test 1: Runtime with lab\n    runtime_instance = Runtime.try_from_specifier(\"lab==3.0.0\")\n    assert runtime_instance.as_with_arg() == runtime_instance.as_with_arg_new_implementation()\n    \n    # Test 2: Runtime with notebook with version\n    runtime_instance = Runtime.try_from_specifier(\"notebook==6.0.0\")\n    assert runtime_instance.as_with_arg() == runtime_instance.as_with_arg_new_implementation()\n    \n    # Test 3: Runtime with nbclassic no version\n    runtime_instance = Runtime.try_from_specifier(\"nbclassic\")\n    assert runtime_instance.as_with_arg() == runtime_instance.as_with_arg_new_implementation()\n\nif __name__ == \"__main__\":\n    test_as_with_arg()"
    },
    {
        "func_name": "is_primitive",
        "idx": "126",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/custom_logger.py",
        "orig_func": "def is_primitive(obj: object) -> bool:\n    \"\"\"Check if an object is an instance of a primitive type.\n\n    Args:\n        obj (Any): Standard Python object.\n\n    Returns:\n        bool: True if the object is a primitive type, False otherwise.\n\n    \"\"\"\n    primitives = (bool, str, int, float, type(None))\n    return isinstance(obj, primitives)",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\ndef is_primitive(obj: object) -> bool:\n    \"\"\"Check if an object is an instance of a primitive type.\n\n    Args:\n        obj (Any): Standard Python object.\n\n    Returns:\n        bool: True if the object is a primitive type, False otherwise.\n\n    \"\"\"\n    primitives = (bool, str, int, float, type(None))\n    return isinstance(obj, primitives)\n\n```\n\n\n",
        "eval_script": "## src/monzo_api_wrapper/utils/custom_logger.py\ndef is_primitive(obj: object) -> bool:\n    \"\"\"Check if an object is an instance of a primitive type.\n\n    Args:\n        obj (Any): Standard Python object.\n\n    Returns:\n        bool: True if the object is a primitive type, False otherwise.\n\n    \"\"\"\n    primitives = (bool, str, int, float, type(None))\n    return isinstance(obj, primitives)\n\n\n\ndef test_is_primitive():\n    # Test with an integer\n    assert is_primitive(5) == is_primitive_new_implementation(5), \"Test with integer failed\"\n    \n    # Test with a string\n    assert is_primitive(\"Hello\") == is_primitive_new_implementation(\"Hello\"), \"Test with string failed\"\n    \n    # Test with a custom object (Should not be primitive)\n    class CustomObject:\n        pass\n    assert is_primitive(CustomObject()) == is_primitive_new_implementation(CustomObject()), \"Test with custom object failed\"\n\n    # Add more test cases as needed\n    # Test with a float\n    assert is_primitive(5.5) == is_primitive_new_implementation(5.5), \"Test with float failed\"\n    \n    # Test with None\n    assert is_primitive(None) == is_primitive_new_implementation(None), \"Test with None failed\"\n\nif __name__ == \"__main__\":\n    test_is_primitive()"
    },
    {
        "func_name": "get_func_signature",
        "idx": "127",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/custom_logger.py",
        "orig_func": "def get_func_signature(list_of_args: list[tuple[str, Any]]) -> str:\n    \"\"\"Generate a string representation of the function's signature, with parameters\n    filtered and masked as needed.\n\n    Args:\n        list_of_args (list[tuple[str, Any]]): List of arguments passed to the function.\n\n    Returns:\n        str: String representation of the function's signature.\n\n    \"\"\"\n    args_repr = [f'{a[0]}={a[1]!r}' for a in list_of_args]\n    return ', '.join(args_repr)",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\nfrom typing import Any, Callable\n\ndef get_func_signature(list_of_args: list[tuple[str, Any]]) -> str:\n    \"\"\"Generate a string representation of the function's signature, with parameters\n    filtered and masked as needed.\n\n    Args:\n        list_of_args (list[tuple[str, Any]]): List of arguments passed to the function.\n\n    Returns:\n        str: String representation of the function's signature.\n\n    \"\"\"\n    args_repr = [f\"{a[0]}={a[1]!r}\" for a in list_of_args]\n    return \", \".join(args_repr)\n\n```\n\n\n",
        "eval_script": "## src/monzo_api_wrapper/utils/custom_logger.py\nfrom typing import Any, Callable\n\ndef get_func_signature(list_of_args: list[tuple[str, Any]]) -> str:\n    \"\"\"Generate a string representation of the function's signature, with parameters\n    filtered and masked as needed.\n\n    Args:\n        list_of_args (list[tuple[str, Any]]): List of arguments passed to the function.\n\n    Returns:\n        str: String representation of the function's signature.\n\n    \"\"\"\n    args_repr = [f\"{a[0]}={a[1]!r}\" for a in list_of_args]\n    return \", \".join(args_repr)\n\n\n\ndef test_get_func_signature():\n    # Test case 1\n    args1 = [(\"arg1\", 1), (\"arg2\", \"value\"), (\"arg3\", True)]\n    assert get_func_signature(args1) == get_func_signature_new_implementation(args1)\n\n    # Test case 2\n    args2 = [(\"param1\", None), (\"param2\", 3.14), (\"param3\", [1, 2, 3])]\n    assert get_func_signature(args2) == get_func_signature_new_implementation(args2)\n\n    # Test case 3\n    args3 = [(\"x\", \"abc\"), (\"y\", 123), (\"z\", (\"tuple\", \"in\", \"list\"))]\n    assert get_func_signature(args3) == get_func_signature_new_implementation(args3)\n\nif __name__ == \"__main__\":\n    test_get_func_signature()"
    },
    {
        "func_name": "Db.create_db_engine",
        "idx": "128",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/db.py",
        "orig_func": "def create_db_engine(self) -> Engine:\n    \"\"\"Create an SQLAlchemy engine for the database connection.\n\n        Returns:\n            Engine: An SQLAlchemy Engine instance configured with database connection details.\n\n        \"\"\"\n    username = os.getenv('DB_USER')\n    password = os.getenv('DB_PASS')\n    host = os.getenv('DB_HOST')\n    database_type = os.getenv('DB_TYPE')\n    database_name = os.getenv('DB_NAME')\n    port = os.getenv('DB_PORT')\n    sql_string = f'{database_type}://{username}:{password}@{host}:{port}/{database_name}'\n    return create_engine(sql_string)",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\nimport json\n\nimport os\n\nimport sys\n\nimport loguru\n\nfrom loguru import logger as loguru_base_logger\n\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\n\n    This class should not be instantiated directly. Use the `get_logger` class method\n    to retrieve the logger instance.\n\n    Raises:\n        RuntimeError: Raised if the class constructor is called directly.\n\n    \"\"\"\n\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\n\n        Raises:\n            RuntimeError: Raised if the class constructor is called directly.\n\n        \"\"\"\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru.Logger:\n        \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record: loguru.Record) -> None:\n        \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n```\n\n\n```python\n## src/monzo_api_wrapper/utils/db.py\nimport os\n\nfrom typing import Optional\n\nimport pandas as pd\n\nimport sqlalchemy\n\nfrom sqlalchemy import create_engine\n\nfrom sqlalchemy.engine import Engine\n\nfrom monzo_api_wrapper.utils import sql_templates\n\nfrom monzo_api_wrapper.utils.custom_logger import CustomLogger\n\nlogger = CustomLogger.get_logger()\n\nclass InsertArgumentError(ValueError):\n    \"\"\"Custom exception raised when both DataFrame and SQL string arguments are missing\n    for an insert operation.\n\n    This exception is used to ensure that at least one valid data source (a\n    DataFrame or an SQL string) is provided when attempting to insert data into\n    the database.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the InsertArgumentError exception with a specific error message.\n\n        This constructor sets the error message that indicates that either a\n        DataFrame or an SQL string must be provided for an insert operation.\n\n        The error message is passed to the base ValueError class.\n\n        \"\"\"\n        super().__init__(\n            \"Either a DataFrame or SQL string must be provided for the insert operation.\"\n        )\n\nclass Db:\n    \"\"\"Class to manage connection to a database and perform SQL operations.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the Db class and set up a database engine.\n\n        Initializes the SQLAlchemy engine and logs the connection status.\n\n        \"\"\"\n        self.engine = self.create_db_engine()\n        logger.debug(f\"Connected to database: {os.getenv('DB_NAME')}\")\n\n    def create_db_engine(self) -> Engine:\n        \"\"\"Create an SQLAlchemy engine for the database connection.\n\n        Returns:\n            Engine: An SQLAlchemy Engine instance configured with database connection details.\n\n        \"\"\"\n        username = os.getenv(\"DB_USER\")\n        password = os.getenv(\"DB_PASS\")\n        host = os.getenv(\"DB_HOST\")\n        database_type = os.getenv(\"DB_TYPE\")\n        database_name = os.getenv(\"DB_NAME\")\n        port = os.getenv(\"DB_PORT\")\n        sql_string = f\"{database_type}://{username}:{password}@{host}:{port}/{database_name}\"\n        return create_engine(sql_string)\n\n    def query(self, sql: str, return_data: bool = True) -> Optional[pd.DataFrame]:\n        \"\"\"Execute an SQL query against the database.\n\n        Args:\n            sql (str): The SQL query to be executed.\n            return_data (bool, optional): If True, returns query data as a DataFrame. Defaults to True.\n\n        Returns:\n            Optional[pd.DataFrame]: DataFrame with query results if return_data=True, else None.\n\n        \"\"\"\n        if return_data:\n            return pd.read_sql_query(sql, self.engine)\n        with self.engine.begin() as conn:\n            conn.execute(sqlalchemy.text(sql))\n        return None\n\n    def insert(\n        self, table: str, df: Optional[pd.DataFrame] = None, sql: Optional[str] = None\n    ) -> None:\n        \"\"\"Insert data into a table in the database.\n\n        Args:\n            table (str): The name of the target database table.\n            df (Optional[pd.DataFrame]): DataFrame containing data to insert. Defaults to None.\n            sql (Optional[str]): Custom SQL insert statement. Defaults to None.\n\n        Raises:\n            InsertArgumentError: If both `df` and `sql` are None.\n\n        \"\"\"\n        if df is None and not sql:\n            raise InsertArgumentError()\n\n        if sql:\n            insert_sql = f\"INSERT INTO {table} (\\n{sql}\\n);\"\n            self.query(insert_sql, return_data=False)\n            logger.debug(f\"Data inserted into {table}\")\n        else:\n            if df is not None:\n                rows = len(df)\n                chunksize = 20000 if rows > 20000 else None\n                schema, table_name = table.split(\".\")\n                with self.engine.begin() as conn:\n                    df.to_sql(\n                        schema=schema,\n                        name=table_name,\n                        index=False,\n                        con=conn,\n                        if_exists=\"append\",\n                        method=\"multi\",\n                        chunksize=chunksize,\n                    )\n                logger.debug(f\"{rows} rows inserted into {schema}.{table_name}\")\n\n    def delete(self, table: str, data: str) -> None:\n        \"\"\"Delete data from a table in the database.\n\n        Args:\n            table (str): The name of the database table from which data will be deleted.\n            data (str): Condition to specify which rows to delete, formatted as a SQL condition.\n\n        \"\"\"\n        sql_delete = sql_templates.delete.format(table=table, data=data)\n        logger.info(f\"Running delete statement: {sql_delete}\")\n        self.query(sql=sql_delete, return_data=False)\n\n```\n\n\n",
        "eval_script": "# Mock environment variables for database connection\nimport os\n\nos.environ[\"DB_USER\"] = \"mock_user\"\nos.environ[\"DB_PASS\"] = \"mock_pass\"\nos.environ[\"DB_HOST\"] = \"mock_host\"\nos.environ[\"DB_TYPE\"] = \"sqlite\"  # Changed from mock_dbtype to sqlite\nos.environ[\"DB_NAME\"] = \"/home/user/tmp/mock_db.sqlite\"  # Using a path for sqlite\nos.environ[\"DB_PORT\"] = \"1234\"\nos.environ[\"LOG_LEVEL\"] = \"DEBUG\"\n\n# Import necessary packages\nfrom typing import Optional\nimport pandas as pd\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\nimport json\nimport sys\nimport loguru\nfrom loguru import logger as loguru_base_logger\n\n# Mock sql_templates\nclass sql_templates:\n    delete = \"DELETE FROM {table} WHERE {data};\"\n\n# Custom Logger Class\nclass CustomLogger:\n    logger = None\n\n    def __init__(self) -> None:\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru.logger:  # Change here\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(lambda record: cls.logger_patch(record))\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record) -> None:\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n# Logger initialization\nlogger = CustomLogger.get_logger()\n\n# Db Class Definition\nclass InsertArgumentError(ValueError):\n    def __init__(self) -> None:\n        super().__init__(\n            \"Either a DataFrame or SQL string must be provided for the insert operation.\"\n        )\n\nclass Db:\n    def __init__(self) -> None:\n        self.engine = self.create_db_engine()\n        logger.debug(f\"Connected to database: {os.getenv('DB_NAME')}\")\n\n    def create_db_engine(self) -> Engine:\n        username = os.getenv(\"DB_USER\")\n        password = os.getenv(\"DB_PASS\")\n        host = os.getenv(\"DB_HOST\")\n        database_type = os.getenv(\"DB_TYPE\")\n        database_name = os.getenv(\"DB_NAME\")\n        port = os.getenv(\"DB_PORT\")\n        sql_string = f\"{database_type}://{username}:{password}@{host}:{port}/{database_name}\" \\\n                     if database_type != \"sqlite\" else f\"sqlite:///{database_name}\"\n        return create_engine(sql_string)\n\n\n    def query(self, sql: str, return_data: bool = True) -> Optional[pd.DataFrame]:\n        if return_data:\n            return pd.read_sql_query(sql, self.engine)\n        with self.engine.begin() as conn:\n            conn.execute(sqlalchemy.text(sql))\n        return None\n\n    def insert(self, table: str, df: Optional[pd.DataFrame] = None, sql: Optional[str] = None) -> None:\n        if df is None and not sql:\n            raise InsertArgumentError()\n\n        if sql:\n            insert_sql = f\"INSERT INTO {table} (\\n{sql}\\n);\"\n            self.query(insert_sql, return_data=False)\n            logger.debug(f\"Data inserted into {table}\")\n        else:\n            if df is not None:\n                rows = len(df)\n                chunksize = 20000 if rows > 20000 else None\n                schema, table_name = table.split(\".\")\n                with self.engine.begin() as conn:\n                    df.to_sql(\n                        schema=schema,\n                        name=table_name,\n                        index=False,\n                        con=conn,\n                        if_exists=\"append\",\n                        method=\"multi\",\n                        chunksize=chunksize,\n                    )\n                logger.debug(f\"{rows} rows inserted into {schema}.{table_name}\")\n\n    def delete(self, table: str, data: str) -> None:\n        sql_delete = sql_templates.delete.format(table=table, data=data)\n        logger.info(f\"Running delete statement: {sql_delete}\")\n        self.query(sql=sql_delete, return_data=False)\n\n# Test function\ndef test_create_db_engine():\n    db_instance = Db()\n    old_engine = db_instance.create_db_engine()\n    new_engine = db_instance.create_db_engine_new_implementation()\n\n    assert isinstance(old_engine, Engine), \"Old engine is not an Engine instance\"\n    assert isinstance(new_engine, Engine), \"New engine is not an Engine instance\"\n    assert str(old_engine.url) == str(new_engine.url), \"Engine URLs do not match\"\n\n# Main function\nif __name__ == \"__main__\":\n    test_create_db_engine()\n    print(\"All tests passed.\")",
        "eval_script_sh": "# Bash script to ensure the directory exists\nmkdir -p /home/user/tmp"
    },
    {
        "func_name": "Db.insert",
        "idx": "129",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/db.py",
        "orig_func": "def insert(self, table: str, df: Optional[pd.DataFrame]=None, sql: Optional[str]=None) -> None:\n    \"\"\"Insert data into a table in the database.\n\n        Args:\n            table (str): The name of the target database table.\n            df (Optional[pd.DataFrame]): DataFrame containing data to insert. Defaults to None.\n            sql (Optional[str]): Custom SQL insert statement. Defaults to None.\n\n        Raises:\n            InsertArgumentError: If both `df` and `sql` are None.\n\n        \"\"\"\n    if df is None and (not sql):\n        raise InsertArgumentError()\n    if sql:\n        insert_sql = f'INSERT INTO {table} (\\n{sql}\\n);'\n        self.query(insert_sql, return_data=False)\n        logger.debug(f'Data inserted into {table}')\n    elif df is not None:\n        rows = len(df)\n        chunksize = 20000 if rows > 20000 else None\n        (schema, table_name) = table.split('.')\n        with self.engine.begin() as conn:\n            df.to_sql(schema=schema, name=table_name, index=False, con=conn, if_exists='append', method='multi', chunksize=chunksize)\n        logger.debug(f'{rows} rows inserted into {schema}.{table_name}')",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\nimport json\n\nimport os\n\nimport sys\n\nimport loguru\n\nfrom loguru import logger as loguru_base_logger\n\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\n\n    This class should not be instantiated directly. Use the `get_logger` class method\n    to retrieve the logger instance.\n\n    Raises:\n        RuntimeError: Raised if the class constructor is called directly.\n\n    \"\"\"\n\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\n\n        Raises:\n            RuntimeError: Raised if the class constructor is called directly.\n\n        \"\"\"\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru.Logger:\n        \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record: loguru.Record) -> None:\n        \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n```\n\n\n```python\n## src/monzo_api_wrapper/utils/db.py\nimport os\n\nfrom typing import Optional\n\nimport pandas as pd\n\nimport sqlalchemy\n\nfrom sqlalchemy import create_engine\n\nfrom sqlalchemy.engine import Engine\n\nfrom monzo_api_wrapper.utils import sql_templates\n\nfrom monzo_api_wrapper.utils.custom_logger import CustomLogger\n\nlogger = CustomLogger.get_logger()\n\nclass InsertArgumentError(ValueError):\n    \"\"\"Custom exception raised when both DataFrame and SQL string arguments are missing\n    for an insert operation.\n\n    This exception is used to ensure that at least one valid data source (a\n    DataFrame or an SQL string) is provided when attempting to insert data into\n    the database.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the InsertArgumentError exception with a specific error message.\n\n        This constructor sets the error message that indicates that either a\n        DataFrame or an SQL string must be provided for an insert operation.\n\n        The error message is passed to the base ValueError class.\n\n        \"\"\"\n        super().__init__(\n            \"Either a DataFrame or SQL string must be provided for the insert operation.\"\n        )\n\nclass Db:\n    \"\"\"Class to manage connection to a database and perform SQL operations.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the Db class and set up a database engine.\n\n        Initializes the SQLAlchemy engine and logs the connection status.\n\n        \"\"\"\n        self.engine = self.create_db_engine()\n        logger.debug(f\"Connected to database: {os.getenv('DB_NAME')}\")\n\n    def create_db_engine(self) -> Engine:\n        \"\"\"Create an SQLAlchemy engine for the database connection.\n\n        Returns:\n            Engine: An SQLAlchemy Engine instance configured with database connection details.\n\n        \"\"\"\n        username = os.getenv(\"DB_USER\")\n        password = os.getenv(\"DB_PASS\")\n        host = os.getenv(\"DB_HOST\")\n        database_type = os.getenv(\"DB_TYPE\")\n        database_name = os.getenv(\"DB_NAME\")\n        port = os.getenv(\"DB_PORT\")\n        sql_string = f\"{database_type}://{username}:{password}@{host}:{port}/{database_name}\"\n        return create_engine(sql_string)\n\n    def query(self, sql: str, return_data: bool = True) -> Optional[pd.DataFrame]:\n        \"\"\"Execute an SQL query against the database.\n\n        Args:\n            sql (str): The SQL query to be executed.\n            return_data (bool, optional): If True, returns query data as a DataFrame. Defaults to True.\n\n        Returns:\n            Optional[pd.DataFrame]: DataFrame with query results if return_data=True, else None.\n\n        \"\"\"\n        if return_data:\n            return pd.read_sql_query(sql, self.engine)\n        with self.engine.begin() as conn:\n            conn.execute(sqlalchemy.text(sql))\n        return None\n\n    def insert(\n        self, table: str, df: Optional[pd.DataFrame] = None, sql: Optional[str] = None\n    ) -> None:\n        \"\"\"Insert data into a table in the database.\n\n        Args:\n            table (str): The name of the target database table.\n            df (Optional[pd.DataFrame]): DataFrame containing data to insert. Defaults to None.\n            sql (Optional[str]): Custom SQL insert statement. Defaults to None.\n\n        Raises:\n            InsertArgumentError: If both `df` and `sql` are None.\n\n        \"\"\"\n        if df is None and not sql:\n            raise InsertArgumentError()\n\n        if sql:\n            insert_sql = f\"INSERT INTO {table} (\\n{sql}\\n);\"\n            self.query(insert_sql, return_data=False)\n            logger.debug(f\"Data inserted into {table}\")\n        else:\n            if df is not None:\n                rows = len(df)\n                chunksize = 20000 if rows > 20000 else None\n                schema, table_name = table.split(\".\")\n                with self.engine.begin() as conn:\n                    df.to_sql(\n                        schema=schema,\n                        name=table_name,\n                        index=False,\n                        con=conn,\n                        if_exists=\"append\",\n                        method=\"multi\",\n                        chunksize=chunksize,\n                    )\n                logger.debug(f\"{rows} rows inserted into {schema}.{table_name}\")\n\n    def delete(self, table: str, data: str) -> None:\n        \"\"\"Delete data from a table in the database.\n\n        Args:\n            table (str): The name of the database table from which data will be deleted.\n            data (str): Condition to specify which rows to delete, formatted as a SQL condition.\n\n        \"\"\"\n        sql_delete = sql_templates.delete.format(table=table, data=data)\n        logger.info(f\"Running delete statement: {sql_delete}\")\n        self.query(sql=sql_delete, return_data=False)\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\nimport os\nimport pandas as pd\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\nfrom loguru import logger as loguru_base_logger\nimport json\nimport sys\nfrom typing import Optional  # Add this import to fix the NameError\n\n# Mock environment variables\nos.environ[\"DB_USER\"] = \"user\"\nos.environ[\"DB_PASS\"] = \"pass\"\nos.environ[\"DB_HOST\"] = \"localhost\"\nos.environ[\"DB_TYPE\"] = \"sqlite\"  # Using SQLite for a simple mock\nos.environ[\"DB_NAME\"] = \":memory:\"  # Use in-memory database for testing\nos.environ[\"DB_PORT\"] = \"5432\"\nos.environ[\"LOG_LEVEL\"] = \"DEBUG\"\n\n# Mocking the sql_templates module\nclass sql_templates:\n    delete = \"DELETE FROM {table} WHERE {data}\"\n\n# Custom Logger definition from CONTEXT\nclass CustomLogger:\n    logger = None\n\n    def __init__(self) -> None:\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls):\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr,\n                format=\"{extra[serialized]}\",\n                level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record):\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\nlogger = CustomLogger.get_logger()\n\n# Database class definition from PYTHON CODE\nclass InsertArgumentError(ValueError):\n    def __init__(self) -> None:\n        super().__init__(\"Either a DataFrame or SQL string must be provided for the insert operation.\")\n\nclass Db:\n    def __init__(self) -> None:\n        self.engine = self.create_db_engine()\n        logger.debug(f\"Connected to database: {os.getenv('DB_NAME')}\")\n\n    def create_db_engine(self) -> Engine:\n        # Creating a mock engine that connects to an SQLite memory DB for testing\n        sql_string = \"sqlite:///:memory:\"\n        return create_engine(sql_string)\n\n    def query(self, sql: str, return_data: bool = True) -> Optional[pd.DataFrame]:\n        if return_data:\n            return pd.read_sql_query(sql, self.engine)\n        with self.engine.begin() as conn:\n            conn.execute(sqlalchemy.text(sql))\n        return None\n\n    def insert(self, table: str, df: Optional[pd.DataFrame] = None, sql: Optional[str] = None) -> None:\n        if df is None and not sql:\n            raise InsertArgumentError()\n        \n        if sql:\n            # Adjusted to simulate insertion using mock values within testing context - valid SQL syntax\n            insert_sql = f\"INSERT INTO {table} (id, name) VALUES (1, 'test');\"\n            self.query(insert_sql, return_data=False)\n            logger.debug(f\"Data inserted into {table}\")\n        else:\n            if df is not None:\n                rows = len(df)\n                chunksize = 20000 if rows > 20000 else None\n                # SQLite does not support schema separate namespace inherently\n                table_name = table  # No split usage, single selectable table name excercise in-memory eval\n                with self.engine.begin() as conn:\n                    df.to_sql(\n                        name=table_name,\n                        index=False,\n                        con=conn,\n                        if_exists=\"append\",\n                        method=\"multi\",\n                        chunksize=chunksize,\n                    )\n                logger.debug(f\"{rows} rows inserted into {table_name}\")\n\n\n    def delete(self, table: str, data: str) -> None:\n        sql_delete = sql_templates.delete.format(table=table, data=data)\n        logger.info(f\"Running delete statement: {sql_delete}\")\n        self.query(sql=sql_delete, return_data=False)\n\ndef test_insert():\n    db = Db()\n\n    # Test case 1: Insert using SQL string\n    # Introduce table creation valid stand-in to align tests without result rows failing simple insert consistency\n    sql = \"SELECT 1 as id, 'test' as name\"  # Mock select statement for later instance use\n    db.query(\"CREATE TABLE mock_table (id INTEGER, name TEXT)\", return_data=False)\n    db.insert(\"mock_table\", sql=sql)\n    db.insert_new_implementation(\"mock_table\", sql=sql)\n    result = db.query(\"SELECT COUNT(*) FROM mock_table\")\n    assert result.iloc[0, 0] == 2  # Check if two rows have been inserted\n\n    # Test case 2: Insert using DataFrame\n    df = pd.DataFrame({'id': [2], 'name': ['test2']})\n    db.insert(\"mock_table\", df=df)\n    db.insert_new_implementation(\"mock_table\", df=df)\n    result = db.query(\"SELECT COUNT(*) FROM mock_table\")\n    assert result.iloc[0, 0] == 4  # Check if four rows have been inserted\n\n    # Test case 3: Insert without DataFrame or SQL raises exception\n    try:\n        db.insert(\"mock_table\")\n    except InsertArgumentError:\n        exception_raised = True\n    else:\n        exception_raised = False\n    assert exception_raised\n\n    try:\n        db.insert_new_implementation(\"mock_table\")\n    except InsertArgumentError:\n        new_exception_raised = True\n    else:\n        new_exception_raised = False\n    assert new_exception_raised\n\nif __name__ == \"__main__\":\n    test_insert()"
    },
    {
        "func_name": "Db.delete",
        "idx": "130",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/db.py",
        "orig_func": "def delete(self, table: str, data: str) -> None:\n    \"\"\"Delete data from a table in the database.\n\n        Args:\n            table (str): The name of the database table from which data will be deleted.\n            data (str): Condition to specify which rows to delete, formatted as a SQL condition.\n\n        \"\"\"\n    sql_delete = sql_templates.delete.format(table=table, data=data)\n    logger.info(f'Running delete statement: {sql_delete}')\n    self.query(sql=sql_delete, return_data=False)",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\nimport json\n\nimport os\n\nimport sys\n\nimport loguru\n\nfrom loguru import logger as loguru_base_logger\n\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\n\n    This class should not be instantiated directly. Use the `get_logger` class method\n    to retrieve the logger instance.\n\n    Raises:\n        RuntimeError: Raised if the class constructor is called directly.\n\n    \"\"\"\n\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\n\n        Raises:\n            RuntimeError: Raised if the class constructor is called directly.\n\n        \"\"\"\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru.Logger:\n        \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record: loguru.Record) -> None:\n        \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n```\n\n\n```python\n## src/monzo_api_wrapper/utils/db.py\nimport os\n\nfrom typing import Optional\n\nimport pandas as pd\n\nimport sqlalchemy\n\nfrom sqlalchemy import create_engine\n\nfrom sqlalchemy.engine import Engine\n\nfrom monzo_api_wrapper.utils import sql_templates\n\nfrom monzo_api_wrapper.utils.custom_logger import CustomLogger\n\nlogger = CustomLogger.get_logger()\n\nclass InsertArgumentError(ValueError):\n    \"\"\"Custom exception raised when both DataFrame and SQL string arguments are missing\n    for an insert operation.\n\n    This exception is used to ensure that at least one valid data source (a\n    DataFrame or an SQL string) is provided when attempting to insert data into\n    the database.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the InsertArgumentError exception with a specific error message.\n\n        This constructor sets the error message that indicates that either a\n        DataFrame or an SQL string must be provided for an insert operation.\n\n        The error message is passed to the base ValueError class.\n\n        \"\"\"\n        super().__init__(\n            \"Either a DataFrame or SQL string must be provided for the insert operation.\"\n        )\n\nclass Db:\n    \"\"\"Class to manage connection to a database and perform SQL operations.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the Db class and set up a database engine.\n\n        Initializes the SQLAlchemy engine and logs the connection status.\n\n        \"\"\"\n        self.engine = self.create_db_engine()\n        logger.debug(f\"Connected to database: {os.getenv('DB_NAME')}\")\n\n    def create_db_engine(self) -> Engine:\n        \"\"\"Create an SQLAlchemy engine for the database connection.\n\n        Returns:\n            Engine: An SQLAlchemy Engine instance configured with database connection details.\n\n        \"\"\"\n        username = os.getenv(\"DB_USER\")\n        password = os.getenv(\"DB_PASS\")\n        host = os.getenv(\"DB_HOST\")\n        database_type = os.getenv(\"DB_TYPE\")\n        database_name = os.getenv(\"DB_NAME\")\n        port = os.getenv(\"DB_PORT\")\n        sql_string = f\"{database_type}://{username}:{password}@{host}:{port}/{database_name}\"\n        return create_engine(sql_string)\n\n    def query(self, sql: str, return_data: bool = True) -> Optional[pd.DataFrame]:\n        \"\"\"Execute an SQL query against the database.\n\n        Args:\n            sql (str): The SQL query to be executed.\n            return_data (bool, optional): If True, returns query data as a DataFrame. Defaults to True.\n\n        Returns:\n            Optional[pd.DataFrame]: DataFrame with query results if return_data=True, else None.\n\n        \"\"\"\n        if return_data:\n            return pd.read_sql_query(sql, self.engine)\n        with self.engine.begin() as conn:\n            conn.execute(sqlalchemy.text(sql))\n        return None\n\n    def insert(\n        self, table: str, df: Optional[pd.DataFrame] = None, sql: Optional[str] = None\n    ) -> None:\n        \"\"\"Insert data into a table in the database.\n\n        Args:\n            table (str): The name of the target database table.\n            df (Optional[pd.DataFrame]): DataFrame containing data to insert. Defaults to None.\n            sql (Optional[str]): Custom SQL insert statement. Defaults to None.\n\n        Raises:\n            InsertArgumentError: If both `df` and `sql` are None.\n\n        \"\"\"\n        if df is None and not sql:\n            raise InsertArgumentError()\n\n        if sql:\n            insert_sql = f\"INSERT INTO {table} (\\n{sql}\\n);\"\n            self.query(insert_sql, return_data=False)\n            logger.debug(f\"Data inserted into {table}\")\n        else:\n            if df is not None:\n                rows = len(df)\n                chunksize = 20000 if rows > 20000 else None\n                schema, table_name = table.split(\".\")\n                with self.engine.begin() as conn:\n                    df.to_sql(\n                        schema=schema,\n                        name=table_name,\n                        index=False,\n                        con=conn,\n                        if_exists=\"append\",\n                        method=\"multi\",\n                        chunksize=chunksize,\n                    )\n                logger.debug(f\"{rows} rows inserted into {schema}.{table_name}\")\n\n    def delete(self, table: str, data: str) -> None:\n        \"\"\"Delete data from a table in the database.\n\n        Args:\n            table (str): The name of the database table from which data will be deleted.\n            data (str): Condition to specify which rows to delete, formatted as a SQL condition.\n\n        \"\"\"\n        sql_delete = sql_templates.delete.format(table=table, data=data)\n        logger.info(f\"Running delete statement: {sql_delete}\")\n        self.query(sql=sql_delete, return_data=False)\n\n```\n\n\n",
        "eval_script": "import os\nimport json\nimport sys\nimport pandas as pd\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\nfrom loguru import logger as loguru_base_logger\nfrom typing import Optional\n\n# Placeholder for sql_templates providing an SQL DELETE format template\nclass sql_templates:\n    delete = \"DELETE FROM {table} WHERE {data};\"\n\n# CustomLogger for structured logging\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\"\"\"\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\"\"\"\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru_base_logger:\n        \"\"\"Retrieve an instance of the logger with customized settings.\"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record) -> None:  # Removed loguru.Record type hint\n        \"\"\"Customize the log record format for the Loguru logger.\"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n\n# Mocking environment variables for database connection\nos.environ[\"DB_USER\"] = \"mock_user\"\nos.environ[\"DB_PASS\"] = \"mock_pass\"\nos.environ[\"DB_HOST\"] = \"mock_host\"\nos.environ[\"DB_TYPE\"] = \"sqlite\"  # Using SQLite for simplicity\nos.environ[\"DB_NAME\"] = \"mock_db\"\nos.environ[\"DB_PORT\"] = \"\"  # Port is not required for SQLite\n\n# Raise custom exception for insert argument error\nclass InsertArgumentError(ValueError):\n    \"\"\"Custom exception raised when both DataFrame and SQL string arguments are missing for an insert operation.\"\"\"\n    def __init__(self) -> None:\n        super().__init__(\"Either a DataFrame or SQL string must be provided for the insert operation.\")\n\n# Db class originally provided\nclass Db:\n    \"\"\"Class to manage connection to a database and perform SQL operations.\"\"\"\n    def __init__(self) -> None:\n        self.engine = self.create_db_engine()\n        logger.debug(f\"Connected to database: {os.getenv('DB_NAME')}\")\n\n    def create_db_engine(self) -> Engine:\n        \"\"\"Create an SQLAlchemy engine for the database connection.\"\"\"\n        database_type = os.getenv(\"DB_TYPE\")\n        database_name = os.getenv(\"DB_NAME\")\n        sql_string = f\"{database_type}:///{database_name}\"\n        return create_engine(sql_string)\n\n    def query(self, sql: str, return_data: bool = True) -> Optional[pd.DataFrame]:\n        \"\"\"Execute an SQL query against the database.\"\"\"\n        if return_data:\n            return pd.read_sql_query(sql, self.engine)\n        with self.engine.begin() as conn:\n            conn.execute(sqlalchemy.text(sql))\n        return None\n\n    def insert(self, table: str, df: Optional[pd.DataFrame] = None, sql: Optional[str] = None) -> None:\n        \"\"\"Insert data into a table in the database.\"\"\"\n        if df is None and not sql:\n            raise InsertArgumentError()\n\n        if sql:\n            insert_sql = f\"INSERT INTO {table} (\\n{sql}\\n);\"\n            self.query(insert_sql, return_data=False)\n            logger.debug(f\"Data inserted into {table}\")\n        else:\n            if df is not None:\n                rows = len(df)\n                chunksize = 20000 if rows > 20000 else None\n                schema, table_name = table.split(\".\")\n                with self.engine.begin() as conn:\n                    df.to_sql(\n                        schema=schema,\n                        name=table_name,\n                        index=False,\n                        con=conn,\n                        if_exists=\"append\",\n                        method=\"multi\",\n                        chunksize=chunksize,\n                    )\n                logger.debug(f\"{rows} rows inserted into {schema}.{table_name}\")\n\n    def delete(self, table: str, data: str) -> str:  # Return the SQL for testing purposes\n        \"\"\"Delete data from a table in the database.\"\"\"\n        sql_delete = sql_templates.delete.format(table=table, data=data)\n        logger.info(f\"Running delete statement: {sql_delete}\")\n        self.query(sql=sql_delete, return_data=False)\n        return sql_delete  # Return SQL\n\n\n# Initialize the logger\nlogger = CustomLogger.get_logger()\n\ndef test_delete():\n    db_instance = Db()\n    \n    # Create the test_table to ensure it exists\n    create_table_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS test_table (\n        id INTEGER PRIMARY KEY,\n        name TEXT\n    );\n    \"\"\"\n    db_instance.query(create_table_sql, return_data=False)\n    \n    table = \"test_table\"\n    condition = \"id = 123\"\n    \n    # We will use the sql to test equivalency of methods by generating them\n    expected_sql = sql_templates.delete.format(table=table, data=condition)\n    \n    # Capturing SQL generated by the delete functions\n    sql_generated_delete = db_instance.delete(table, condition)\n    sql_generated_delete_new = db_instance.delete_new_implementation(table, condition)\n    \n    # Here, assertions are used to check the consistency\n    assert sql_generated_delete == expected_sql, \"SQL generated mismatch in delete method\"\n    assert sql_generated_delete_new == expected_sql, \"SQL generated mismatch in new delete method\"\n    assert expected_sql == sql_templates.delete.format(table=table, data=condition), \"Template mismatch\"\n\nif __name__ == \"__main__\":\n    test_delete()"
    },
    {
        "func_name": "CustomLogger.get_logger",
        "idx": "131",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/custom_logger.py",
        "orig_func": "@classmethod\ndef get_logger(cls) -> loguru.Logger:\n    \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n    if not cls.logger:\n        cls.logger = loguru_base_logger\n        cls.logger.remove()\n        cls.logger = cls.logger.patch(cls.logger_patch)\n        cls.logger.add(sys.stderr, format='{extra[serialized]}', level=os.getenv('LOG_LEVEL', 'INFO'))\n    return cls.logger",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\nimport json\n\nimport os\n\nimport sys\n\nimport loguru\n\nfrom loguru import logger as loguru_base_logger\n\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\n\n    This class should not be instantiated directly. Use the `get_logger` class method\n    to retrieve the logger instance.\n\n    Raises:\n        RuntimeError: Raised if the class constructor is called directly.\n\n    \"\"\"\n\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\n\n        Raises:\n            RuntimeError: Raised if the class constructor is called directly.\n\n        \"\"\"\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru.Logger:\n        \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record: loguru.Record) -> None:\n        \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\nimport os\nimport sys\nimport json\nfrom loguru import logger as loguru_base_logger\nfrom loguru._logger import Logger  # Correct import for type hint\nfrom typing import Any  # Import Any for type hint\n\nclass CustomLogger:\n    logger = None\n    logger_new = None  # Added for new implementation\n\n    def __init__(self) -> None:\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> Logger:  # Updated type hint\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n        return cls.logger\n\n\n    @classmethod\n    def logger_patch(cls, record: Any) -> None:  # Use Any for the type of record\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\ndef test_get_logger():\n    logger_old = CustomLogger.get_logger()\n    logger_new = CustomLogger.get_logger_new_implementation()\n\n    # Test 1: Check that both loggers are instances of loguru.Logger\n    assert isinstance(logger_old, Logger), \"Old logger is not an instance of loguru.Logger\"\n    assert isinstance(logger_new, Logger), \"New logger is not an instance of loguru.Logger\"\n\n    # Test 2: Verify both loggers produce the same formatted output\n    log_output_old = []\n    log_output_new = []\n\n    def capture_old(message):\n        log_output_old.append(message)\n\n    def capture_new(message):\n        log_output_new.append(message)\n\n    logger_old.add(capture_old, format=\"{extra[serialized]}\", level=\"INFO\")\n    logger_new.add(capture_new, format=\"{extra[serialized]}\", level=\"INFO\")\n\n    logger_old.info(\"Test message\")\n    logger_new.info(\"Test message\")\n\n    assert log_output_old == log_output_new, \"Log outputs differ between implementations\"\n\n    # Test 3: Check configuration settings, assume some attribute or setting to verify\n    # (typically would check handlers or logging levels)\n    assert logger_old._core.handlers == logger_new._core.handlers, \"Handlers configuration differs\"\n\nif __name__ == \"__main__\":\n    test_get_logger()"
    },
    {
        "func_name": "CustomLogger.logger_patch",
        "idx": "132",
        "repo_name": "eliasbenaddou___monzo-api-wrapper",
        "func_path": "src/monzo_api_wrapper/utils/custom_logger.py",
        "orig_func": "@classmethod\ndef logger_patch(cls, record: loguru.Record) -> None:\n    \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n    record['extra']['serialized'] = json.dumps({'timestamp': str(record['time']), 'module': record['name'], 'function': record['function'], 'line_number': record['line'], 'level': record['level'].name, 'message': record['message'], 'extra': record['extra']})",
        "orig_context": "```python\n## src/monzo_api_wrapper/utils/custom_logger.py\nimport json\n\nimport os\n\nimport sys\n\nimport loguru\n\nfrom loguru import logger as loguru_base_logger\n\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\n\n    This class should not be instantiated directly. Use the `get_logger` class method\n    to retrieve the logger instance.\n\n    Raises:\n        RuntimeError: Raised if the class constructor is called directly.\n\n    \"\"\"\n\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\n\n        Raises:\n            RuntimeError: Raised if the class constructor is called directly.\n\n        \"\"\"\n        raise RuntimeError\n\n    @classmethod\n    def get_logger(cls) -> loguru.Logger:\n        \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record: loguru.Record) -> None:\n        \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n```\n\n\n",
        "eval_script": "## src/monzo_api_wrapper/utils/custom_logger.py\nimport json\n\nimport os\n\nimport sys\n\nfrom loguru import logger as loguru_base_logger\n\nfrom datetime import datetime  # Import Python's datetime module\n\nclass CustomLogger:\n    \"\"\"Class to create custom loggers using the Loguru package.\n\n    This class should not be instantiated directly. Use the `get_logger` class method\n    to retrieve the logger instance.\n\n    Raises:\n        RuntimeError: Raised if the class constructor is called directly.\n\n    \"\"\"\n\n    logger = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the custom logger instance.\"\"\"\n        pass\n\n    @classmethod\n    def get_logger(cls):\n        \"\"\"Retrieve an instance of the logger with customised settings.\n\n        The logger is configured to output logs to stderr with a specified format and\n        log level set by the environment variable `LOG_LEVEL`, defaults to INFO.\n\n        Returns:\n            Logger: An instance of the customised Loguru logger.\n\n        \"\"\"\n        if not cls.logger:\n            cls.logger = loguru_base_logger\n            cls.logger.remove()\n            cls.logger = cls.logger.patch(cls.logger_patch)\n            cls.logger.add(\n                sys.stderr, format=\"{extra[serialized]}\", level=os.getenv(\"LOG_LEVEL\", \"INFO\")\n            )\n\n        return cls.logger\n\n    @classmethod\n    def logger_patch(cls, record: dict) -> None:\n        \"\"\"Customises the log record format for the Loguru logger.\n\n        This method is used to patch the logger and serialize the log record data into JSON format.\n\n        Args:\n            record (dict[str, Any]): Dictionary containing log record data.\n\n        \"\"\"\n        record[\"extra\"][\"serialized\"] = json.dumps({\n            \"timestamp\": str(record[\"time\"]),\n            \"module\": record[\"name\"],\n            \"function\": record[\"function\"],\n            \"line_number\": record[\"line\"],\n            \"level\": record[\"level\"].name,\n            \"message\": record[\"message\"],\n            \"extra\": record[\"extra\"],\n        })\n\n\ndef test_logger_patch():\n    \"\"\"Test that logger_patch_new_implementation is equivalent to logger_patch.\"\"\"\n    record = {\n        \"time\": datetime.now(),  # Use Python's datetime for the time field\n        \"name\": \"test_module\",\n        \"function\": \"test_function\",\n        \"line\": 10,\n        \"level\": loguru_base_logger.level(\"INFO\"),\n        \"message\": \"Test message\",\n        \"extra\": {}\n    }\n\n    # Use a copy of the record for independent testing\n    record_original = record.copy()\n    record_new_impl = record.copy()\n\n    # Applying original patch\n    CustomLogger.logger_patch(record_original)\n    # Applying new implementation patch\n    CustomLogger.logger_patch_new_implementation(record_new_impl)\n\n    # Compare the serialized JSON outputs\n    serialized_original = record_original[\"extra\"][\"serialized\"]\n    serialized_new = record_new_impl[\"extra\"][\"serialized\"]\n\n    assert serialized_original == serialized_new, \"Serialized outputs do not match\"\n    assert json.loads(serialized_original) == json.loads(serialized_new), \"Deserialized JSONs do not match\"\n    assert record_original[\"extra\"][\"serialized\"] == record_new_impl[\"extra\"][\"serialized\"], \"Extra fields mismatch\"\n\nif __name__ == \"__main__\":\n    test_logger_patch()"
    },
    {
        "func_name": "softmax",
        "idx": "135",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "examples/arnnpv.py",
        "orig_func": "def softmax(inp):\n    max_vals = inp.max(axis=1)\n    max_vals = max_vals.reshape((max_vals.size(), 1))\n    u = (inp - max_vals).exp()\n    v = u.sum(axis=1)\n    v = v.reshape((v.size(), 1))\n    u = u / v\n    return u",
        "orig_context": "```python\n## examples/arnnpv.py\ndef softmax(inp):\n\tmax_vals = inp.max(axis=1)\n\tmax_vals = max_vals.reshape((max_vals.size(), 1))\n\tu = (inp - max_vals).exp()\n\tv = u.sum(axis=1)\n\tv = v.reshape((v.size(), 1))\n\tu = u / v\n\treturn u\n\n```\n\n\n",
        "eval_script": "import numpy as np\n\ndef softmax(inp):\n    max_vals = np.max(inp, axis=1)\n    max_vals = max_vals.reshape((max_vals.size, 1))\n    u = np.exp(inp - max_vals)\n    v = u.sum(axis=1)\n    v = v.reshape((v.size, 1))\n    u = u / v\n    return u\n\n\n\ndef test_softmax():\n    test_input_1 = np.array([[1, 2, 3], [1, 2, 3]])\n    assert np.allclose(softmax(test_input_1), softmax_new_implementation(test_input_1)), \"Test Case 1 Failed\"\n    \n    test_input_2 = np.array([[0, 0, 0], [1, 1, 1], [-1, -1, -1]])\n    assert np.allclose(softmax(test_input_2), softmax_new_implementation(test_input_2)), \"Test Case 2 Failed\"\n    \n    test_input_3 = np.array([[1000, 1000, 1000], [-1000, -1000, -1000]])\n    assert np.allclose(softmax(test_input_3), softmax_new_implementation(test_input_3)), \"Test Case 3 Failed\"\n\nif __name__ == \"__main__\":\n    test_softmax()"
    },
    {
        "func_name": "softmax_orig",
        "idx": "136",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "examples/arnnp_lpvip.py",
        "orig_func": "def softmax_orig(inp):\n    max_vals = np.max(inp, axis=1)\n    max_vals = np.reshape(max_vals, (max_vals.size, 1))\n    u = np.exp(inp - max_vals)\n    v = np.sum(u, axis=1)\n    v.shape = (v.size, 1)\n    u = u / v\n    return u",
        "orig_context": "```python\n## examples/arnnp_lpvip.py\nimport numpy as np\n\ndef softmax_orig(inp):\n        max_vals = np.max(inp, axis=1)\n        max_vals = np.reshape(max_vals, (max_vals.size, 1))\n        u = np.exp(inp - max_vals)\n        v = np.sum(u, axis=1)\n        v.shape = v.size, 1\n        u = u / v\n        return u\n\n```\n\n\n",
        "eval_script": "## examples/arnnp_lpvip.py\nimport numpy as np\n\ndef softmax_orig(inp):\n        max_vals = np.max(inp, axis=1)\n        max_vals = np.reshape(max_vals, (max_vals.size, 1))\n        u = np.exp(inp - max_vals)\n        v = np.sum(u, axis=1)\n        v.shape = v.size, 1\n        u = u / v\n        return u\n\n\n\ndef test_softmax_orig():\n    # Test case 1: Simple 1D array\n    data1 = np.array([[1, 2, 3]])\n    expected1 = softmax_orig(data1)\n    result1 = softmax_orig_new_implementation(data1)\n    assert np.allclose(expected1, result1), \"Test case 1 failed\"\n\n    # Test case 2: 2D array with different ranges\n    data2 = np.array([[1, 2, 3], [1, 5, 1]])\n    expected2 = softmax_orig(data2)\n    result2 = softmax_orig_new_implementation(data2)\n    assert np.allclose(expected2, result2), \"Test case 2 failed\"\n\n    # Test case 3: Array with all the same elements\n    data3 = np.array([[10, 10, 10], [5, 5, 5]])\n    expected3 = softmax_orig(data3)\n    result3 = softmax_orig_new_implementation(data3)\n    assert np.allclose(expected3, result3), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_softmax_orig()"
    },
    {
        "func_name": "XXXxlnsbcopy",
        "idx": "137",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "src/xlns.py",
        "orig_func": "def XXXxlnsbcopy(x, setB=None):\n    if setB == None:\n        copyB = xlnsB\n    else:\n        copyB = setB\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            r += [XXXxlnsbcopy(y, copyB)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlns) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n            r += [xlnsb(y, copyB)]\n        else:\n            r += [y]\n    return r",
        "orig_context": "```python\n# src/xlns.py\n\nimport numpy as np\nimport math\n\ndef sbdb_ufunc_ideal(z, s, B=None, F=None):\n    \"\"\"default ufunc for ideal Gaussian log for all add,sub; may substitute user supplied sbdb_ufunc\"\"\"\n    global xlnsB\n    if B == None:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + xlnsB ** np.minimum(-s, z))) / math.log(xlnsB)))\n    else:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + B ** np.minimum(-s, z))) / math.log(B)))\nsbdb_ufunc = sbdb_ufunc_ideal\n\nclass xlnsb:\n    \"\"\"scalar non-redundant LNS whose precision on each instance defined by arbitrary real B>1\"\"\"\n\n    def __init__(self, v, setB):\n        self.B = setB\n        if isinstance(v, int) or isinstance(v, float):\n            if abs(v) != 0:\n                self.x = int(round(math.log(abs(v)) / math.log(self.B)))\n            else:\n                self.x = -1e309\n            self.s = False if v >= 0.0 else True\n        elif isinstance(v, xlnsb):\n            cv = np.log(v.B) / np.log(self.B)\n            self.x = v.x if v.x == -1e309 else int(cv * v.x + 0.5)\n            self.s = v.s\n        elif isinstance(v, str):\n            if v.replace('.', '', 1).replace('+', '', 2).replace('-', '', 2).replace('e', '', 1).isdigit():\n                temp = xlnsb(float(v), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                self.x = v\n                self.s = False\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpv) or isinstance(v, xlnsnpb):\n            temp = v.xlns()\n            if v.size() == 1:\n                temp = xlnsb(float(temp[0]), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsb')\n                return NotImplemented\n        elif isinstance(v, list):\n            print('cannot cast list as xlnsb')\n            return NotImplemented\n        else:\n            temp = xlnsb(float(v), self.B)\n            self.x = temp.x\n            self.s = temp.s\n            self.B = temp.B\n\n    def __float__(self):\n        return (-1 if self.s else 1) * float(self.B ** self.x)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        if self.x == -1e309:\n            return '0'\n        log10 = self.x / (math.log(10) / math.log(self.B))\n        if abs(log10) > 10:\n            return ('-' if self.s else '') + str(10.0 ** (log10 % 1)) + 'E' + str(math.floor(log10))\n        else:\n            return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsb(' + str(self) + ' B=' + str(self.B) + ')'\n\n    def conjugate(self):\n        return self\n\n    def __abs__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False\n        return t\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.exp(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.exp(float(x)), xlnsB)\n        return xlnsb(math.exp(float(x)), x.B)\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.log(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.log(float(x)), xlnsB)\n        return xlnsb(math.log(float(x)), x.B)\n\n    def __mul__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self * xlnsb(v, self.B)\n        t.x = self.x + v.x\n        t.s = self.s != v.s\n        return t\n\n    def __pow__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int):\n            t.x = self.x * v\n            t.s = self.s if v % 2 == 1 else False\n        elif isinstance(v, float):\n            if self.s:\n                return float(self) ** v\n            t.x = self.x * v\n            t.s = False\n        else:\n            if self.s:\n                return float(self) ** float(v)\n            t.x = self.x * float(v)\n            t.s = False\n        return t\n\n    def __truediv__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self / xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self / xlnsb(v, self.B)\n        t.x = self.x - v.x\n        t.s = self.s != v.s\n        return t\n\n    def __add__(self, v):\n\n        def sbprevious(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbprevious(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sbexact(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbexact(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sb(z):\n            if z <= 0:\n                return int(sbdb_ufunc(z, 0, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 0, B=self.B) // 2 + z)\n\n        def db(z):\n            if z == 0:\n                return -1e309\n            elif z < 0:\n                return int(sbdb_ufunc(z, 1, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 1, B=self.B) // 2 + z)\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self + xlnsb(v, self.B)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        elif not isinstance(v, xlnsb) or self.B != v.B:\n            return self + xlnsb(v, self.B)\n        if v.x == -1e309:\n            return self\n        if self.x == -1e309:\n            return v\n        z = self.x - v.x\n        if self.s == v.s:\n            t.x = v.x + sb(z)\n            t.s = v.s\n        else:\n            t.x = v.x + db(z)\n            t.s = v.s if v.x > self.x else self.s\n        return t\n\n    def __neg__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False if self.s else True\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __rtruediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) / self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsb(v, self.B) / self\n        return v / self\n\n    def __rpow__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) ** self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsv(v, self.B) ** self\n        return v ** self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __idiv__(self, v):\n        self = self.__div__(v)\n        return self\n\n    def __ipow__(self, v):\n        self = self.__pow__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self == xlnsb(v, self.B)\n        return self.x == v.x and self.s == v.s\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self != xlnsb(v, self.B)\n        return self.x != v.x or self.s != v.s\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self < xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x > v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x < v.x\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self > xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x < v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x > v.x\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self <= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x >= v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x <= v.x\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self >= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x <= v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x >= v.x\n\nclass xlnsr:\n    \"\"\"scalar redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, xlnsr):\n            self.p = v.p\n            self.n = v.n\n        elif isinstance(v, int) or isinstance(v, float):\n            if xlns(v) >= 0:\n                self.p = xlns(v)\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = -xlns(v)\n        elif isinstance(v, xlns):\n            if v >= 0:\n                self.p = v\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = abs(v)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr):\n            temp = v.xlns()\n            if v.size() == 1:\n                if temp[0] >= 0:\n                    self.p = temp[0]\n                    self.n = xlns(0)\n                else:\n                    self.p = xlns(0)\n                    self.n = -temp[0]\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsr')\n                return NotImplemented\n        elif xlns(v) >= 0:\n            self.p = xlns(v)\n            self.n = xlns(0)\n        else:\n            self.p = xlns(0)\n            self.n = -xlns(v)\n\n    def __float__(self):\n        return float(self.p) - float(self.n)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsr(' + str(self) + ' = ' + str(float(self.p)) + '-' + str(float(self.n)) + ')'\n\n    def __neg__(self):\n        t = xlnsr(self)\n        t.p = self.n\n        t.n = self.p\n        return t\n\n    def __add__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        t = xlnsr(v)\n        t.p += self.p\n        t.n += self.n\n        return t\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __sub__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return -v + self\n        t = xlnsr(v)\n        t2 = self.p + t.n\n        t.n = self.n + t.p\n        t.p = t2\n        return t\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n * t.p\n                t.p = self.p * t.p\n            else:\n                t.p = self.n * t.n\n                t.n = self.p * t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) * float(v))\n            print('xlnsr*xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        else:\n            return self * xlnsr(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __truediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n / t.p\n                t.p = self.p / t.p\n            else:\n                t.p = self.n / t.n\n                t.n = self.p / t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) / float(v))\n            print('xlnsr/xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        else:\n            return self / xlnsr(v)\n\n    def __rtruediv__(self, v):\n        t = xlnsr(float(v) / float(self))\n        print('any/xlnsr: result computed in float')\n        return t\n\n    def __abs__(self):\n        t = xlnsr(self)\n        if self.p < self.n:\n            t.p = self.n\n            t.n = self.p\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsr):\n            return self == xlnsr(v)\n        t = self - v\n        return t.p == t.n\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsr):\n            return self != xlnsr(v)\n        t = self - v\n        return t.p != t.n\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self < xlnsr(v)\n        t = self - v\n        return t.p < t.n\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self > xlnsr(v)\n        t = self - v\n        return t.p > t.n\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsr):\n            return self <= xlnsr(v)\n        t = self - v\n        return t.p <= t.n\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsr):\n            return self >= xlnsr(v)\n        t = self - v\n        return t.p >= t.n\n\ndef xlnscopy(x, xlnstype=xlns, setFB=None):\n    \"\"\"deep copy of list converting to xlns (or type and precision specified by 2nd and 3rd args)\"\"\"\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            if setFB == None:\n                r += [xlnscopy(y, xlnstype)]\n            else:\n                r += [xlnscopy(y, xlnstype, setFB)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlns) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n            if setFB == None:\n                r += [xlnstype(y)]\n            else:\n                r += [xlnstype(y, setFB)]\n        else:\n            r += [y]\n    return r\n\nclass xlnsnp:\n    \"\"\"numpy-like array of non-redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, int) or isinstance(v, float):\n            if v == 0:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * xlns(v).x + xlns(v).s], dtype='i8')\n        elif isinstance(v, xlns):\n            if v.x == -1e309:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * v.x + v.s], dtype='i8')\n        elif isinstance(v, xlnsnp):\n            self.nd = v.nd\n        elif isinstance(v, xlnsr):\n            if v.p == v.n:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            elif v.p > v.n:\n                self.nd = np.array([2 * xlns(v).x], dtype='i8')\n            else:\n                self.nd = np.array([2 * xlns(v).x + 1], dtype='i8')\n        elif isinstance(v, xlnsb) or isinstance(v, xlnsv):\n            self.nd = xlnsnp(float(v)).nd\n        elif isinstance(v, xlnsnpb) or isinstance(v, xlnsnpv) or isinstance(v, xlnsnpr):\n            self.nd = xlnsnp(np.float64(v.xlns())).nd\n        elif isinstance(v, np.ndarray):\n            if isinstance(np.ravel(v)[0], xlns):\n                t = []\n                for y in np.ravel(v):\n                    if y.x == -1e309:\n                        t += [-9223372036854775807 - 1 + y.s]\n                    else:\n                        t += [2 * y.x + y.s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n            elif isinstance(np.ravel(v)[0], xlnsv):\n                self.nd = xlnsnp(np.float64(v)).nd\n            else:\n                t = []\n                for y in np.ravel(xlnscopy(v)):\n                    if isinstance(y, xlns):\n                        if y.x == -1e309:\n                            t += [-9223372036854775807 - 1 + y.s]\n                        else:\n                            t += [2 * y.x + y.s]\n                    elif y == 0:\n                        t += [-9223372036854775807 - 1]\n                    else:\n                        t += [2 * xlns(y).x + xlns(y).s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        elif isinstance(v, list):\n            if isinstance(v[0], xlnsv):\n                self.nd = xlnsnp(np.float64(v)).nd\n            else:\n                t = []\n                for y in xlnscopy(v):\n                    if y.x == -1e309:\n                        t += [-9223372036854775807 - 1 + y.s]\n                    else:\n                        t += [2 * y.x + y.s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        else:\n            self.nd = np.array([])\n\n    def add(self, v):\n        return self + v\n\n    def __str__(self):\n        return str(self.xlns())\n\n    def __repr__(self):\n        return 'xlnsnp(' + str(self.xlns()) + ')'\n\n    def __bool__(self):\n        return bool(self.nd == 0)\n\n    def xlns(self):\n        \"\"\"convert xlnsnp to similar-shaped list of xlns\"\"\"\n        t = []\n        for y in np.ravel(self.nd):\n            txlns = xlns(0)\n            if y | 1 == -9223372036854775807:\n                txlns.x = -1e309\n            else:\n                txlns.x = y // 2\n            txlns.s = True if y & 1 else False\n            t += [txlns]\n        return np.reshape(t, np.shape(self.nd))\n\n    def ovf(x):\n        \"\"\"return copy except for values that underflow/overflow, which are clipped according to xlnssetovf\"\"\"\n        global ovfmin, ovfmax\n        return xlnsnp.where((abs(x) >= ovfmin) * (abs(x) < ovfmax), x, xlnsnp.zeros(x.shape()) + (abs(x) >= ovfmax) * ovfmax * x.sign())\n\n    def conjugate(self):\n        return self\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnp(math.exp(x))\n        if isinstance(x, xlnsnp):\n            return math.exp(1) ** x\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnp(math.log(x))\n        if isinstance(x, xlnsnp):\n            return xlnsnp(np.log(x.xlns()))\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsnp(v)\n        if isinstance(v, xlnsnp):\n            t = xlnsnp('')\n            t.nd = np.where((self.nd | 1 == -9223372036854775807) | (v.nd | 1 == -9223372036854775807), -9223372036854775807 - 1 | (self.nd ^ v.nd) & 1, self.nd + v.nd - (v.nd & 1) ^ v.nd & 1)\n            return t\n        else:\n            return self * xlnsnp(v)\n\n# ...\n\ndef XXXxlnsbcopy(x, setB=None):\n    if setB == None:\n        copyB = xlnsB\n    else:\n        copyB = setB\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            r += [XXXxlnsbcopy(y, copyB)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlns) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n            r += [xlnsb(y, copyB)]\n        else:\n            r += [y]\n    return r\n\n```\n",
        "eval_script": "# src/xlns.py\n\nimport numpy as np\nimport math\n\n# Define the global variable xlnsB\nxlnsB = 2.0  \n\ndef sbdb_ufunc_ideal(z, s, B=None, F=None):\n    \"\"\"default ufunc for ideal Gaussian log for all add,sub; may substitute user supplied sbdb_ufunc\"\"\"\n    global xlnsB\n    if B == None:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + xlnsB ** np.minimum(-s, z))) / math.log(xlnsB)))\n    else:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + B ** np.minimum(-s, z))) / math.log(B)))\nsbdb_ufunc = sbdb_ufunc_ideal\n\nclass xlnsb:\n    \"\"\"scalar non-redundant LNS whose precision on each instance defined by arbitrary real B>1\"\"\"\n\n    def __init__(self, v, setB):\n        self.B = setB\n        if isinstance(v, int) or isinstance(v, float):\n            if abs(v) != 0:\n                self.x = int(round(math.log(abs(v)) / math.log(self.B)))\n            else:\n                self.x = -1e309\n            self.s = False if v >= 0.0 else True\n        elif isinstance(v, xlnsb):\n            cv = np.log(v.B) / np.log(self.B)\n            self.x = v.x if v.x == -1e309 else int(cv * v.x + 0.5)\n            self.s = v.s\n        elif isinstance(v, str):\n            if v.replace('.', '', 1).replace('+', '', 2).replace('-', '', 2).replace('e', '', 1).isdigit():\n                temp = xlnsb(float(v), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                self.x = v\n                self.s = False\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpv) or isinstance(v, xlnsnpb):\n            temp = v.xlns()\n            if v.size() == 1:\n                temp = xlnsb(float(temp[0]), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsb')\n                return NotImplemented\n        elif isinstance(v, list):\n            print('cannot cast list as xlnsb')\n            return NotImplemented\n        else:\n            temp = xlnsb(float(v), self.B)\n            self.x = temp.x\n            self.s = temp.s\n            self.B = temp.B\n\n    def __float__(self):\n        return (-1 if self.s else 1) * float(self.B ** self.x)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        if self.x == -1e309:\n            return '0'\n        log10 = self.x / (math.log(10) / math.log(self.B))\n        if abs(log10) > 10:\n            return ('-' if self.s else '') + str(10.0 ** (log10 % 1)) + 'E' + str(math.floor(log10))\n        else:\n            return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsb(' + str(self) + ' B=' + str(self.B) + ')'\n\n    def conjugate(self):\n        return self\n\n    def __abs__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False\n        return t\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.exp(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.exp(float(x)), xlnsB)\n        return xlnsb(math.exp(float(x)), x.B)\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.log(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.log(float(x)), xlnsB)\n        return xlnsb(math.log(float(x)), x.B)\n\n    def __mul__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self * xlnsb(v, self.B)\n        t.x = self.x + v.x\n        t.s = self.s != v.s\n        return t\n\n    def __pow__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int):\n            t.x = self.x * v\n            t.s = self.s if v % 2 == 1 else False\n        elif isinstance(v, float):\n            if self.s:\n                return float(self) ** v\n            t.x = self.x * v\n            t.s = False\n        else:\n            if self.s:\n                return float(self) ** float(v)\n            t.x = self.x * float(v)\n            t.s = False\n        return t\n\n    def __truediv__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self / xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self / xlnsb(v, self.B)\n        t.x = self.x - v.x\n        t.s = self.s != v.s\n        return t\n\n    def __add__(self, v):\n\n        def sbprevious(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbprevious(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sbexact(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbexact(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sb(z):\n            if z <= 0:\n                return int(sbdb_ufunc(z, 0, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 0, B=self.B) // 2 + z)\n\n        def db(z):\n            if z == 0:\n                return -1e309\n            elif z < 0:\n                return int(sbdb_ufunc(z, 1, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 1, B=self.B) // 2 + z)\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self + xlnsb(v, self.B)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        elif not isinstance(v, xlnsb) or self.B != v.B:\n            return self + xlnsb(v, self.B)\n        if v.x == -1e309:\n            return self\n        if self.x == -1e309:\n            return v\n        z = self.x - v.x\n        if self.s == v.s:\n            t.x = v.x + sb(z)\n            t.s = v.s\n        else:\n            t.x = v.x + db(z)\n            t.s = v.s if v.x > self.x else self.s\n        return t\n\n    def __neg__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False if self.s else True\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __rtruediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) / self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsb(v, self.B) / self\n        return v / self\n\n    def __rpow__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) ** self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsv(v, self.B) ** self\n        return v ** self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __idiv__(self, v):\n        self = self.__div__(v)\n        return self\n\n    def __ipow__(self, v):\n        self = self.__pow__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self == xlnsb(v, self.B)\n        return self.x == v.x and self.s == v.s\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self != xlnsb(v, self.B)\n        return self.x != v.x or self.s != v.s\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self < xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x > v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x < v.x\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self > xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x < v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x > v.x\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self <= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x >= v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x <= v.x\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self >= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x <= v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x >= v.x\n\nclass xlnsr:\n    \"\"\"scalar redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, xlnsr):\n            self.p = v.p\n            self.n = v.n\n        elif isinstance(v, int) or isinstance(v, float):\n            if xlns(v) >= 0:\n                self.p = xlns(v)\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = -xlns(v)\n        elif isinstance(v, xlns):\n            if v >= 0:\n                self.p = v\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = abs(v)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr):\n            temp = v.xlns()\n            if v.size() == 1:\n                if temp[0] >= 0:\n                    self.p = temp[0]\n                    self.n = xlns(0)\n                else:\n                    self.p = xlns(0)\n                    self.n = -temp[0]\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsr')\n                return NotImplemented\n        elif xlns(v) >= 0:\n            self.p = xlns(v)\n            self.n = xlns(0)\n        else:\n            self.p = xlns(0)\n            self.n = -xlns(v)\n\n    def __float__(self):\n        return float(self.p) - float(self.n)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsr(' + str(self) + ' = ' + str(float(self.p)) + '-' + str(float(self.n)) + ')'\n\n    def __neg__(self):\n        t = xlnsr(self)\n        t.p = self.n\n        t.n = self.p\n        return t\n\n    def __add__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        t = xlnsr(v)\n        t.p += self.p\n        t.n += self.n\n        return t\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __sub__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return -v + self\n        t = xlnsr(v)\n        t2 = self.p + t.n\n        t.n = self.n + t.p\n        t.p = t2\n        return t\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n * t.p\n                t.p = self.p * t.p\n            else:\n                t.p = self.n * t.n\n                t.n = self.p * t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) * float(v))\n            print('xlnsr*xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        else:\n            return self * xlnsr(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __truediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n / t.p\n                t.p = self.p / t.p\n            else:\n                t.p = self.n / t.n\n                t.n = self.p / t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) / float(v))\n            print('xlnsr/xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        else:\n            return self / xlnsr(v)\n\n    def __rtruediv__(self, v):\n        t = xlnsr(float(v) / float(self))\n        print('any/xlnsr: result computed in float')\n        return t\n\n    def __abs__(self):\n        t = xlnsr(self)\n        if self.p < self.n:\n            t.p = self.n\n            t.n = self.p\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsr):\n            return self == xlnsr(v)\n        t = self - v\n        return t.p == t.n\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsr):\n            return self != xlnsr(v)\n        t = self - v\n        return t.p != t.n\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self < xlnsr(v)\n        t = self - v\n        return t.p < t.n\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self > xlnsr(v)\n        t = self - v\n        return t.p > t.n\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsr):\n            return self <= xlnsr(v)\n        t = self - v\n        return t.p <= t.n\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsr):\n            return self >= xlnsr(v)\n        t = self - v\n        return t.p >= t.n\n\ndef xlnscopy(x, xlnstype=xlnsb, setFB=None):\n    \"\"\"deep copy of list converting to xlns (or type and precision specified by 2nd and 3rd args)\"\"\"\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            if setFB == None:\n                r += [xlnscopy(y, xlnstype)]\n            else:\n                r += [xlnscopy(y, xlnstype, setFB)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlnsb, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv):\n            if setFB == None:\n                r += [xlnstype(y)]\n            else:\n                r += [xlnstype(y, setFB)]\n        else:\n            r += [y]\n    return r\n\nclass xlnsnp:\n    \"\"\"numpy-like array of non-redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, int) or isinstance(v, float):\n            if v == 0:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * xlns(v).x + xlns(v).s], dtype='i8')\n        elif isinstance(v, xlns):\n            if v.x == -1e309:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * v.x + v.s], dtype='i8')\n        elif isinstance(v, xlnsnp):\n            self.nd = v.nd\n        elif isinstance(v, xlnsr):\n            if v.p == v.n:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            elif v.p > v.n:\n                self.nd = np.array([2 * xlns(v).x], dtype='i8')\n            else:\n                self.nd = np.array([2 * xlns(v).x + 1], dtype='i8')\n        elif isinstance(v, xlnsb) or isinstance(v, xlnsv):\n            self.nd = xlnsnp(float(v)).nd\n        elif isinstance(v, xlnsnpb) or isinstance(v, xlnsnpv) or isinstance(v, xlnsnpr):\n            self.nd = xlnsnp(np.float64(v.xlns())).nd\n        elif isinstance(v, np.ndarray):\n            if isinstance(np.ravel(v)[0], xlns):\n                t = []\n                for y in np.ravel(v):\n                    if y.x == -1e309:\n                        t += [-9223372036854775807 - 1 + y.s]\n                    else:\n                        t += [2 * y.x + y.s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n            elif isinstance(np.ravel(v)[0], xlnsv):\n                self.nd = xlnsnp(np.float64(v)).nd\n            else:\n                t = []\n                for y in np.ravel(xlnscopy(v)):\n                    if isinstance(y, xlns):\n                        if y.x == -1e309:\n                            t += [-9223372036854775807 - 1 + y.s]\n                        else:\n                            t += [2 * y.x + y.s]\n                    elif y == 0:\n                        t += [-9223372036854775807 - 1]\n                    else:\n                        t += [2 * xlns(y).x + xlns(y).s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        elif isinstance(v, list):\n            if isinstance(v[0], xlnsv):\n                self.nd = xlnsnp(np.float64(v)).nd\n            else:\n                t = []\n                for y in xlnscopy(v):\n                    if y.x == -1e309:\n                        t += [-9223372036854775807 - 1 + y.s]\n                    else:\n                        t += [2 * y.x + y.s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        else:\n            self.nd = np.array([])\n\n    def add(self, v):\n        return self + v\n\n    def __str__(self):\n        return str(self.xlns())\n\n    def __repr__(self):\n        return 'xlnsnp(' + str(self.xlns()) + ')'\n\n    def __bool__(self):\n        return bool(self.nd == 0)\n\n    def xlns(self):\n        \"\"\"convert xlnsnp to similar-shaped list of xlns\"\"\"\n        t = []\n        for y in np.ravel(self.nd):\n            txlns = xlns(0)\n            if y | 1 == -9223372036854775807:\n                txlns.x = -1e309\n            else:\n                txlns.x = y // 2\n            txlns.s = True if y & 1 else False\n            t += [txlns]\n        return np.reshape(t, np.shape(self.nd))\n\n    def ovf(x):\n        \"\"\"return copy except for values that underflow/overflow, which are clipped according to xlnssetovf\"\"\"\n        global ovfmin, ovfmax\n        return xlnsnp.where((abs(x) >= ovfmin) * (abs(x) < ovfmax), x, xlnsnp.zeros(x.shape()) + (abs(x) >= ovfmax) * ovfmax * x.sign())\n\n    def conjugate(self):\n        return self\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnp(math.exp(x))\n        if isinstance(x, xlnsnp):\n            return math.exp(1) ** x\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnp(math.log(x))\n        if isinstance(x, xlnsnp):\n            return xlnsnp(np.log(x.xlns()))\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsnp(v)\n        if isinstance(v, xlnsnp):\n            t = xlnsnp('')\n            t.nd = np.where((self.nd | 1 == -9223372036854775807) | (v.nd | 1 == -9223372036854775807), -9223372036854775807 - 1 | (self.nd ^ v.nd) & 1, self.nd + v.nd - (v.nd & 1) ^ v.nd & 1)\n            return t\n        else:\n            return self * xlnsnp(v)\n\n# ...\n\ndef XXXxlnsbcopy(x, setB=None):\n    if setB == None:\n        copyB = xlnsB\n    else:\n        copyB = setB\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            r += [XXXxlnsbcopy(y, copyB)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlnsb) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv):\n            r += [xlnsb(y, copyB)]\n        else:\n            r += [y]\n    return r\n\n\ndef test_XXXxlnsbcopy():\n    # Test 1: Check with simple list of integers\n    result_original = XXXxlnsbcopy([1, 2, 3])\n    result_new = XXXxlnsbcopy_new_implementation([1, 2, 3])\n    assert result_original == result_new, \"Test 1 failed\"\n\n    # Test 2: Check with simple list of floats\n    result_original = XXXxlnsbcopy([1.0, 2.5, -3.2])\n    result_new = XXXxlnsbcopy_new_implementation([1.0, 2.5, -3.2])\n    assert result_original == result_new, \"Test 2 failed\"\n\n    # Test 3: Check with nested list\n    result_original = XXXxlnsbcopy([[1, 2], [3, 4]])\n    result_new = XXXxlnsbcopy_new_implementation([[1, 2], [3, 4]])\n    assert result_original == result_new, \"Test 3 failed\"\n\nif __name__ == '__main__':\n    test_XXXxlnsbcopy()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "sb_ufunc_premit",
        "idx": "138",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "examples/arnnpr_lpvip.py",
        "orig_func": "def sb_ufunc_premit(zi, s):\n    global xlnsF\n    postcond = np.where(zi <= -(3 << xl.xlnsF), 0, np.where(zi >= -(3 << xl.xlnsF - 2), -1, +1))\n    z = (zi << 3) + (zi & 4095 ^ 4095) + 16 >> 3\n    return np.where(zi == 0, 1 << xl.xlnsF, ((1 << xl.xlnsF) + (z & (1 << xl.xlnsF) - 1) >> -(z >> xl.xlnsF)) + postcond) << 1",
        "orig_context": "```python\n## examples/arnnpr_lpvip.py\nimport numpy as np\n\nimport xlns as xl\n\ndef sb_ufunc_premit(zi,s):   #was called premitchnpi(zi):\n  global xlnsF\n  #if s==1:\n  #   print(\"sb_ufunc_ideal cannot do db\")\n  #if isinstance(s,np.ndarray):\n  #   if not(s.any()):\n  #       print(\"sb_ufunc_ideal cannot do db\")\n  postcond = np.where(zi <= -(3<<xl.xlnsF), 0,\n             np.where(zi >= -(3<<(xl.xlnsF-2)), -1, +1))\n  z = ((zi<<3) + (zi&0xfff^0xfff) + 16)>>3\n  return np.where(zi==0,1<<xl.xlnsF, (((1<<xl.xlnsF)+(z&((1<<xl.xlnsF)-1)))>>(-(z>>xl.xlnsF)))+postcond )<<1\n\n```\n\n\n",
        "eval_script": "## examples/arnnpr_lpvip.py\nimport numpy as np\n\nimport xlns as xl\n\ndef sb_ufunc_premit(zi,s):   #was called premitchnpi(zi):\n  global xlnsF\n  #if s==1:\n  #   print(\"sb_ufunc_ideal cannot do db\")\n  #if isinstance(s,np.ndarray):\n  #   if not(s.any()):\n  #       print(\"sb_ufunc_ideal cannot do db\")\n  postcond = np.where(zi <= -(3<<xl.xlnsF), 0,\n             np.where(zi >= -(3<<(xl.xlnsF-2)), -1, +1))\n  z = ((zi<<3) + (zi&0xfff^0xfff) + 16)>>3\n  return np.where(zi==0,1<<xl.xlnsF, (((1<<xl.xlnsF)+(z&((1<<xl.xlnsF)-1)))>>(-(z>>xl.xlnsF)))+postcond )<<1\n\n\n\ndef test_sb_ufunc_premit():\n    # Test case 1\n    zi = np.array([0, -8, 8], dtype=np.int64)\n    s = 0\n    result_old = sb_ufunc_premit(zi, s)\n    result_new = sb_ufunc_premit_new_implementation(zi, s)\n    assert np.array_equal(result_old, result_new), f\"Test case 1 failed: {result_old} != {result_new}\"\n\n    # Test case 2\n    zi = np.array([-1_000_000, 0, 1_000_000], dtype=np.int64)\n    s = 0\n    result_old = sb_ufunc_premit(zi, s)\n    result_new = sb_ufunc_premit_new_implementation(zi, s)\n    assert np.array_equal(result_old, result_new), f\"Test case 2 failed: {result_old} != {result_new}\"\n\n    # Test case 3\n    zi = np.array([-1, 1, 1024], dtype=np.int64)\n    s = 0\n    result_old = sb_ufunc_premit(zi, s)\n    result_new = sb_ufunc_premit_new_implementation(zi, s)\n    assert np.array_equal(result_old, result_new), f\"Test case 3 failed: {result_old} != {result_new}\"\n\nif __name__ == \"__main__\":\n    test_sb_ufunc_premit()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "XXXxlnsudcopy",
        "idx": "139",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "src/xlns.py",
        "orig_func": "def XXXxlnsudcopy(x):\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            r += [XXXxlnsudcopy(y)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlns) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n            r += [xlnsud(y)]\n        else:\n            r += [y]\n    return r",
        "orig_context": "```python\n# src/xlns.py\n\nimport math\nimport numpy as np\n\ndef sbdb_ufunc_ideal(z, s, B=None, F=None):\n    \"\"\"default ufunc for ideal Gaussian log for all add,sub; may substitute user supplied sbdb_ufunc\"\"\"\n    global xlnsB\n    if B == None:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + xlnsB ** np.minimum(-s, z))) / math.log(xlnsB)))\n    else:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + B ** np.minimum(-s, z))) / math.log(B)))\nsbdb_ufunc = sbdb_ufunc_ideal\n\nclass xlnsb:\n    \"\"\"scalar non-redundant LNS whose precision on each instance defined by arbitrary real B>1\"\"\"\n\n    def __init__(self, v, setB):\n        self.B = setB\n        if isinstance(v, int) or isinstance(v, float):\n            if abs(v) != 0:\n                self.x = int(round(math.log(abs(v)) / math.log(self.B)))\n            else:\n                self.x = -1e309\n            self.s = False if v >= 0.0 else True\n        elif isinstance(v, xlnsb):\n            cv = np.log(v.B) / np.log(self.B)\n            self.x = v.x if v.x == -1e309 else int(cv * v.x + 0.5)\n            self.s = v.s\n        elif isinstance(v, str):\n            if v.replace('.', '', 1).replace('+', '', 2).replace('-', '', 2).replace('e', '', 1).isdigit():\n                temp = xlnsb(float(v), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                self.x = v\n                self.s = False\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpv) or isinstance(v, xlnsnpb):\n            temp = v.xlns()\n            if v.size() == 1:\n                temp = xlnsb(float(temp[0]), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsb')\n                return NotImplemented\n        elif isinstance(v, list):\n            print('cannot cast list as xlnsb')\n            return NotImplemented\n        else:\n            temp = xlnsb(float(v), self.B)\n            self.x = temp.x\n            self.s = temp.s\n            self.B = temp.B\n\n    def __float__(self):\n        return (-1 if self.s else 1) * float(self.B ** self.x)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        if self.x == -1e309:\n            return '0'\n        log10 = self.x / (math.log(10) / math.log(self.B))\n        if abs(log10) > 10:\n            return ('-' if self.s else '') + str(10.0 ** (log10 % 1)) + 'E' + str(math.floor(log10))\n        else:\n            return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsb(' + str(self) + ' B=' + str(self.B) + ')'\n\n    def conjugate(self):\n        return self\n\n    def __abs__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False\n        return t\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.exp(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.exp(float(x)), xlnsB)\n        return xlnsb(math.exp(float(x)), x.B)\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.log(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.log(float(x)), xlnsB)\n        return xlnsb(math.log(float(x)), x.B)\n\n    def __mul__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self * xlnsb(v, self.B)\n        t.x = self.x + v.x\n        t.s = self.s != v.s\n        return t\n\n    def __pow__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int):\n            t.x = self.x * v\n            t.s = self.s if v % 2 == 1 else False\n        elif isinstance(v, float):\n            if self.s:\n                return float(self) ** v\n            t.x = self.x * v\n            t.s = False\n        else:\n            if self.s:\n                return float(self) ** float(v)\n            t.x = self.x * float(v)\n            t.s = False\n        return t\n\n    def __truediv__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self / xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self / xlnsb(v, self.B)\n        t.x = self.x - v.x\n        t.s = self.s != v.s\n        return t\n\n    def __add__(self, v):\n\n        def sbprevious(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbprevious(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sbexact(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbexact(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sb(z):\n            if z <= 0:\n                return int(sbdb_ufunc(z, 0, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 0, B=self.B) // 2 + z)\n\n        def db(z):\n            if z == 0:\n                return -1e309\n            elif z < 0:\n                return int(sbdb_ufunc(z, 1, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 1, B=self.B) // 2 + z)\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self + xlnsb(v, self.B)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        elif not isinstance(v, xlnsb) or self.B != v.B:\n            return self + xlnsb(v, self.B)\n        if v.x == -1e309:\n            return self\n        if self.x == -1e309:\n            return v\n        z = self.x - v.x\n        if self.s == v.s:\n            t.x = v.x + sb(z)\n            t.s = v.s\n        else:\n            t.x = v.x + db(z)\n            t.s = v.s if v.x > self.x else self.s\n        return t\n\n    def __neg__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False if self.s else True\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __rtruediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) / self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsb(v, self.B) / self\n        return v / self\n\n    def __rpow__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) ** self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsv(v, self.B) ** self\n        return v ** self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __idiv__(self, v):\n        self = self.__div__(v)\n        return self\n\n    def __ipow__(self, v):\n        self = self.__pow__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self == xlnsb(v, self.B)\n        return self.x == v.x and self.s == v.s\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self != xlnsb(v, self.B)\n        return self.x != v.x or self.s != v.s\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self < xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x > v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x < v.x\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self > xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x < v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x > v.x\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self <= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x >= v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x <= v.x\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self >= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x <= v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x >= v.x\n\nclass xlnsr:\n    \"\"\"scalar redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, xlnsr):\n            self.p = v.p\n            self.n = v.n\n        elif isinstance(v, int) or isinstance(v, float):\n            if xlns(v) >= 0:\n                self.p = xlns(v)\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = -xlns(v)\n        elif isinstance(v, xlns):\n            if v >= 0:\n                self.p = v\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = abs(v)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr):\n            temp = v.xlns()\n            if v.size() == 1:\n                if temp[0] >= 0:\n                    self.p = temp[0]\n                    self.n = xlns(0)\n                else:\n                    self.p = xlns(0)\n                    self.n = -temp[0]\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsr')\n                return NotImplemented\n        elif xlns(v) >= 0:\n            self.p = xlns(v)\n            self.n = xlns(0)\n        else:\n            self.p = xlns(0)\n            self.n = -xlns(v)\n\n    def __float__(self):\n        return float(self.p) - float(self.n)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsr(' + str(self) + ' = ' + str(float(self.p)) + '-' + str(float(self.n)) + ')'\n\n    def __neg__(self):\n        t = xlnsr(self)\n        t.p = self.n\n        t.n = self.p\n        return t\n\n    def __add__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        t = xlnsr(v)\n        t.p += self.p\n        t.n += self.n\n        return t\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __sub__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return -v + self\n        t = xlnsr(v)\n        t2 = self.p + t.n\n        t.n = self.n + t.p\n        t.p = t2\n        return t\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n * t.p\n                t.p = self.p * t.p\n            else:\n                t.p = self.n * t.n\n                t.n = self.p * t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) * float(v))\n            print('xlnsr*xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        else:\n            return self * xlnsr(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __truediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n / t.p\n                t.p = self.p / t.p\n            else:\n                t.p = self.n / t.n\n                t.n = self.p / t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) / float(v))\n            print('xlnsr/xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        else:\n            return self / xlnsr(v)\n\n    def __rtruediv__(self, v):\n        t = xlnsr(float(v) / float(self))\n        print('any/xlnsr: result computed in float')\n        return t\n\n    def __abs__(self):\n        t = xlnsr(self)\n        if self.p < self.n:\n            t.p = self.n\n            t.n = self.p\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsr):\n            return self == xlnsr(v)\n        t = self - v\n        return t.p == t.n\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsr):\n            return self != xlnsr(v)\n        t = self - v\n        return t.p != t.n\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self < xlnsr(v)\n        t = self - v\n        return t.p < t.n\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self > xlnsr(v)\n        t = self - v\n        return t.p > t.n\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsr):\n            return self <= xlnsr(v)\n        t = self - v\n        return t.p <= t.n\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsr):\n            return self >= xlnsr(v)\n        t = self - v\n        return t.p >= t.n\n\nclass xlnsnpb:\n    \"\"\"numpy-like array of non-redundant LNS whose precision on each instance defined by real B>1\"\"\"\n\n    def __init__(self, v, setB):\n        self.B = setB\n        if isinstance(v, xlnsv) or isinstance(v, xlnsb) or isinstance(v, xlnsnpb):\n            cv = np.log(v.B) / np.log(self.B)\n        else:\n            cv = np.log(xlnsB) / np.log(self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            if v == 0:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * xlnsb(v, setB).x + xlnsb(v, setB).s], dtype='i8')\n        elif isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            if v.x == -1e309:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * int(cv * v.x + 0.5) + v.s], dtype='i8')\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpb):\n            self.nd = np.where(v.nd | 1 == -9223372036854775807, v.nd // 2, np.int64(cv * (v.nd // 2) + 0.5)) * 2 | v.nd & 1\n        elif isinstance(v, xlnsr):\n            if v.p == v.n:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            elif v.p > v.n:\n                self.nd = np.array([2 * int(cv * xlns(v).x + 0.5)], dtype='i8')\n            else:\n                self.nd = np.array([2 * int(cv * xlns(v).x + 0.5) + 1], dtype='i8')\n        elif isinstance(v, xlnsnpr):\n            temp = xlnsnpb(v.xlns(), setB)\n            self.nd = temp.nd\n        elif isinstance(v, np.ndarray):\n            if isinstance(np.ravel(v)[0], xlns):\n                t = []\n                for y in np.ravel(v):\n                    if y.x == -1e309:\n                        t += [-9223372036854775807 - 1 + y.s]\n                    else:\n                        t += [2 * int(cv * y.x + 0.5) + y.s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n            elif isinstance(np.ravel(v)[0], xlnsv) or isinstance(np.ravel(v)[0], xlnsb):\n                self.nd = xlnsnpb(np.float64(v), setB).nd\n            else:\n                t = []\n                for y in np.ravel(xlnscopy(v)):\n                    if isinstance(y, xlns) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n                        if y.x == -1e309:\n                            t += [-9223372036854775807 - 1 + y.s]\n                        else:\n                            t += [2 * int(cv * y.x + 0.5) + y.s]\n                    elif y == 0:\n                        t += [-9223372036854775807 - 1]\n                    else:\n                        t += [2 * xlnsb(y, setB).x + xlnsb(y, setB).s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        elif isinstance(v, list):\n            t = []\n            for y in xlnscopy(v):\n                if y.x == -1e309:\n                    t += [-9223372036854775807 - 1 + y.s]\n                else:\n                    t += [2 * int(cv * y.x + 0.5) + y.s]\n            self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        else:\n            self.nd = np.array([])\n\n    def add(self, v):\n        return self + v\n\n    def __str__(self):\n        return str(self.xlns())\n\n    def __repr__(self):\n        return 'xlnsnpb(' + str(np.float64(self.xlns())) + ' B=' + str(self.B) + ')'\n\n    def __bool__(self):\n        return bool(self.nd == 0)\n\n    def xlns(self):\n        \"\"\"convert xlnsnpb to similar-shaped list of xlnsb\"\"\"\n        t = []\n        for y in np.ravel(self.nd):\n            txlns = xlnsb(0, self.B)\n            if y | 1 == -9223372036854775807:\n                txlns.x = -1e309\n            else:\n                txlns.x = y // 2\n            txlns.s = True if y & 1 else False\n            t += [txlns]\n        return np.reshape(t, np.shape(self.nd))\n\n    def ovf(x):\n        \"\"\"return copy except for values that underflow/overflow, which are clipped according to xlnssetovf\"\"\"\n        global ovfmin, ovfmax\n        return xlnsnpb.where((abs(x) >= ovfmin) * (abs(x) < ovfmax), x, x * xlnsnpb.zeros(x.shape()) + (abs(x) >= ovfmax) * ovfmax * x.sign())\n\n    def conjugate(self):\n        return self\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpb(math.exp(x), xlnsB)\n        if isinstance(x, xlnsnpb):\n            return xlnsnpb(np.exp(np.float64(x.xlns())), x.B)\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpb(math.log(x), xlnsB)\n        if isinstance(x, xlnsnpb):\n            return xlnsnpb(np.log(np.float64(x.xlns())), x.B)\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsnpb(v, self.B)\n        if isinstance(v, xlnsnpb):\n            t = xlnsnpb('', self.B)\n            if v.B == self.B:\n                t.nd = np.where((self.nd | 1 == -9223372036854775807) | (v.nd | 1 == -9223372036854775807), -9223372036854775807 - 1 | (self.nd ^ v.nd) & 1, self.nd + (v.nd - (v.nd & 1)) ^ v.nd & 1)\n            else:\n                cv = np.log(v.B) / np.log(self.B)\n                t.nd = np.where((self.nd | 1 == -9223372036854775807) | (v.nd | 1 == -9223372036854775807), -9223372036854775807 - 1 | (self.nd ^ v.nd) & 1, self.nd + np.int64(cv * (v.nd // 2) + 0.5) * 2 ^ v.nd & 1)\n            return t\n        else:\n            return self * xlnsnpb(v, self.B)\n\n# ...\n\ndef XXXxlnsudcopy(x):\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            r += [XXXxlnsudcopy(y)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlns) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n            r += [xlnsud(y)]\n        else:\n            r += [y]\n    return r\n\n```\n",
        "eval_script": "# src/xlns.py\n\nimport math\nimport numpy as np\n\ndef sbdb_ufunc_ideal(z, s, B=None, F=None):\n    \"\"\"default ufunc for ideal Gaussian log for all add,sub; may substitute user supplied sbdb_ufunc\"\"\"\n    global xlnsB\n    if B == None:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + xlnsB ** np.minimum(-s, z))) / math.log(xlnsB)))\n    else:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + B ** np.minimum(-s, z))) / math.log(B)))\nsbdb_ufunc = sbdb_ufunc_ideal\n\n# Define placeholder classes for undefined types\nclass xlns: pass\nclass xlnsr: pass\nclass xlnsnp: pass\nclass xlnsnpr: pass\nclass xlnsnpv: pass\nclass xlnsv: pass\n\nclass xlnsb:\n    \"\"\"scalar non-redundant LNS whose precision on each instance defined by arbitrary real B>1\"\"\"\n\n    def __init__(self, v, setB):\n        self.B = setB\n        if isinstance(v, int) or isinstance(v, float):\n            if abs(v) != 0:\n                self.x = int(round(math.log(abs(v)) / math.log(self.B)))\n            else:\n                self.x = -1e309\n            self.s = False if v >= 0.0 else True\n        elif isinstance(v, xlnsb):\n            cv = np.log(v.B) / np.log(self.B)\n            self.x = v.x if v.x == -1e309 else int(cv * v.x + 0.5)\n            self.s = v.s\n        elif isinstance(v, str):\n            if v.replace('.', '', 1).replace('+', '', 2).replace('-', '', 2).replace('e', '', 1).isdigit():\n                temp = xlnsb(float(v), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                self.x = v\n                self.s = False\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpv) or isinstance(v, xlnsnpb):\n            temp = v.xlns()\n            if v.size() == 1:\n                temp = xlnsb(float(temp[0]), self.B)\n                self.x = temp.x\n                self.s = temp.s\n                self.B = temp.B\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsb')\n                return NotImplemented\n        elif isinstance(v, list):\n            print('cannot cast list as xlnsb')\n            return NotImplemented\n        else:\n            temp = xlnsb(float(v), self.B)\n            self.x = temp.x\n            self.s = temp.s\n            self.B = temp.B\n\n    def __float__(self):\n        return (-1 if self.s else 1) * float(self.B ** self.x)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        if self.x == -1e309:\n            return '0'\n        log10 = self.x / (math.log(10) / math.log(self.B))\n        if abs(log10) > 10:\n            return ('-' if self.s else '') + str(10.0 ** (log10 % 1)) + 'E' + str(math.floor(log10))\n        else:\n            return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsb(' + str(self) + ' B=' + str(self.B) + ')'\n\n    def conjugate(self):\n        return self\n\n    def __abs__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False\n        return t\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.exp(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.exp(float(x)), xlnsB)\n        return xlnsb(math.exp(float(x)), x.B)\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsb(math.log(x), xlnsB)\n        if not isinstance(x, xlnsb):\n            return xlnsb(math.log(float(x)), xlnsB)\n        return xlnsb(math.log(float(x)), x.B)\n\n    def __mul__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self * xlnsb(v, self.B)\n        t.x = self.x + v.x\n        t.s = self.s != v.s\n        return t\n\n    def __pow__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int):\n            t.x = self.x * v\n            t.s = self.s if v % 2 == 1 else False\n        elif isinstance(v, float):\n            if self.s:\n                return float(self) ** v\n            t.x = self.x * v\n            t.s = False\n        else:\n            if self.s:\n                return float(self) ** float(v)\n            t.x = self.x * float(v)\n            t.s = False\n        return t\n\n    def __truediv__(self, v):\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self / xlnsb(v, self.B)\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self / xlnsb(v, self.B)\n        t.x = self.x - v.x\n        t.s = self.s != v.s\n        return t\n\n    def __add__(self, v):\n\n        def sbprevious(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbprevious(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sbexact(z):\n            return int(round(math.log(1.0 + self.B ** z) / math.log(self.B)))\n\n        def dbexact(z):\n            if z == 0:\n                return -1e309\n            else:\n                return int(round(math.log(abs(1.0 - self.B ** z)) / math.log(self.B)))\n\n        def sb(z):\n            if z <= 0:\n                return int(sbdb_ufunc(z, 0, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 0, B=self.B) // 2 + z)\n\n        def db(z):\n            if z == 0:\n                return -1e309\n            elif z < 0:\n                return int(sbdb_ufunc(z, 1, B=self.B) // 2)\n            else:\n                return int(sbdb_ufunc(-z, 1, B=self.B) // 2 + z)\n        t = xlnsb('', self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            return self + xlnsb(v, self.B)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        elif not isinstance(v, xlnsb) or self.B != v.B:\n            return self + xlnsb(v, self.B)\n        if v.x == -1e309:\n            return self\n        if self.x == -1e309:\n            return v\n        z = self.x - v.x\n        if self.s == v.s:\n            t.x = v.x + sb(z)\n            t.s = v.s\n        else:\n            t.x = v.x + db(z)\n            t.s = v.s if v.x > self.x else self.s\n        return t\n\n    def __neg__(self):\n        t = xlnsb('', self.B)\n        t.x = self.x\n        t.s = False if self.s else True\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __rtruediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) / self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsb(v, self.B) / self\n        return v / self\n\n    def __rpow__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return xlnsb(v, self.B) ** self\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return xlnsv(v, self.B) ** self\n        return v ** self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    # Corrected undefined __div__ to __truediv__ \n    def __idiv__(self, v):\n        self = self.__truediv__(v)\n        return self\n\n    def __ipow__(self, v):\n        self = self.__pow__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self == xlnsb(v, self.B)\n        return self.x == v.x and self.s == v.s\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self != xlnsb(v, self.B)\n        return self.x != v.x or self.s != v.s\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self < xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x > v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x < v.x\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self > xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x < v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x > v.x\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self <= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x >= v.x\n            else:\n                return True\n        elif v.s:\n            return False\n        else:\n            return self.x <= v.x\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsb) or self.B != v.B:\n            return self >= xlnsb(v, self.B)\n        if self.s:\n            if v.s:\n                return self.x <= v.x\n            else:\n                return False\n        elif v.s:\n            return True\n        else:\n            return self.x >= v.x\n\nclass xlnsr:\n    \"\"\"scalar redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, xlnsr):\n            self.p = v.p\n            self.n = v.n\n        elif isinstance(v, int) or isinstance(v, float):\n            if xlns(v) >= 0:\n                self.p = xlns(v)\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = -xlns(v)\n        elif isinstance(v, xlns):\n            if v >= 0:\n                self.p = v\n                self.n = xlns(0)\n            else:\n                self.p = xlns(0)\n                self.n = abs(v)\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr):\n            temp = v.xlns()\n            if v.size() == 1:\n                if temp[0] >= 0:\n                    self.p = temp[0]\n                    self.n = xlns(0)\n                else:\n                    self.p = xlns(0)\n                    self.n = -temp[0]\n            else:\n                print('cannot cast non-scalar xlnsnp[r] as xlnsr')\n                return NotImplemented\n        elif xlns(v) >= 0:\n            self.p = xlns(v)\n            self.n = xlns(0)\n        else:\n            self.p = xlns(0)\n            self.n = -xlns(v)\n\n    def __float__(self):\n        return float(self.p) - float(self.n)\n\n    def __int__(self):\n        return int(float(self))\n\n    def __str__(self):\n        return str(float(self))\n\n    def __repr__(self):\n        return 'xlnsr(' + str(self) + ' = ' + str(float(self.p)) + '-' + str(float(self.n)) + ')'\n\n    def __neg__(self):\n        t = xlnsr(self)\n        t.p = self.n\n        t.n = self.p\n        return t\n\n    def __add__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v + self\n        t = xlnsr(v)\n        t.p += self.p\n        t.n += self.n\n        return t\n\n    def __radd__(self, v):\n        return self.__add__(v)\n\n    def __sub__(self, v):\n        if isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return -v + self\n        t = xlnsr(v)\n        t2 = self.p + t.n\n        t.n = self.n + t.p\n        t.p = t2\n        return t\n\n    def __rsub__(self, v):\n        return (-self).__add__(v)\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n * t.p\n                t.p = self.p * t.p\n            else:\n                t.p = self.n * t.n\n                t.n = self.p * t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) * float(v))\n            print('xlnsr*xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return v * self\n        else:\n            return self * xlnsr(v)\n\n    def __rmul__(self, v):\n        return self.__mul__(v)\n\n    def __truediv__(self, v):\n        if isinstance(v, int) or isinstance(v, float) or isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            t = xlnsr(v)\n            if v >= 0:\n                t.n = self.n / t.p\n                t.p = self.p / t.p\n            else:\n                t.p = self.n / t.n\n                t.n = self.p / t.n\n            return t\n        elif isinstance(v, xlnsr):\n            t = xlnsr(float(self) / float(v))\n            print('xlnsr/xlnsr: result computed in float')\n            return t\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpr) or isinstance(v, xlnsnpb):\n            return 1 / v * self\n        else:\n            return self / xlnsr(v)\n\n    def __rtruediv__(self, v):\n        t = xlnsr(float(v) / float(self))\n        print('any/xlnsr: result computed in float')\n        return t\n\n    def __abs__(self):\n        t = xlnsr(self)\n        if self.p < self.n:\n            t.p = self.n\n            t.n = self.p\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __iadd__(self, v):\n        self = self.__add__(v)\n        return self\n\n    def __isub__(self, v):\n        self = self.__sub__(v)\n        return self\n\n    def __imul__(self, v):\n        self = self.__mul__(v)\n        return self\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsr):\n            return self == xlnsr(v)\n        t = self - v\n        return t.p == t.n\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsr):\n            return self != xlnsr(v)\n        t = self - v\n        return t.p != t.n\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self < xlnsr(v)\n        t = self - v\n        return t.p < t.n\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsr):\n            return self > xlnsr(v)\n        t = self - v\n        return t.p > t.n\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsr):\n            return self <= xlnsr(v)\n        t = self - v\n        return t.p <= t.n\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsr):\n            return self >= xlnsr(v)\n        t = self - v\n        return t.p >= t.n\n\nclass xlnsnpb:\n    \"\"\"numpy-like array of non-redundant LNS whose precision on each instance defined by real B>1\"\"\"\n\n    def __init__(self, v, setB):\n        self.B = setB\n        if isinstance(v, xlnsv) or isinstance(v, xlnsb) or isinstance(v, xlnsnpb):\n            cv = np.log(v.B) / np.log(self.B)\n        else:\n            cv = np.log(xlnsB) / np.log(self.B)\n        if isinstance(v, int) or isinstance(v, float):\n            if v == 0:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * xlnsb(v, setB).x + xlnsb(v, setB).s], dtype='i8')\n        elif isinstance(v, xlns) or isinstance(v, xlnsv) or isinstance(v, xlnsb):\n            if v.x == -1e309:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.nd = np.array([2 * int(cv * v.x + 0.5) + v.s], dtype='i8')\n        elif isinstance(v, xlnsnp) or isinstance(v, xlnsnpb):\n            self.nd = np.where(v.nd | 1 == -9223372036854775807, v.nd // 2, np.int64(cv * (v.nd // 2) + 0.5)) * 2 | v.nd & 1\n        elif isinstance(v, xlnsr):\n            if v.p == v.n:\n                self.nd = np.array([-9223372036854775807 - 1], dtype='i8')\n            elif v.p > v.n:\n                self.nd = np.array([2 * int(cv * xlns(v).x + 0.5)], dtype='i8')\n            else:\n                self.nd = np.array([2 * int(cv * xlns(v).x + 0.5) + 1], dtype='i8')\n        elif isinstance(v, xlnsnpr):\n            temp = xlnsnpb(v.xlns(), setB)\n            self.nd = temp.nd\n        elif isinstance(v, np.ndarray):\n            if isinstance(np.ravel(v)[0], xlns):\n                t = []\n                for y in np.ravel(v):\n                    if y.x == -1e309:\n                        t += [-9223372036854775807 - 1 + y.s]\n                    else:\n                        t += [2 * int(cv * y.x + 0.5) + y.s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n            elif isinstance(np.ravel(v)[0], xlnsv) or isinstance(np.ravel(v)[0], xlnsb):\n                self.nd = xlnsnpb(np.float64(v), setB).nd\n            else:\n                t = []\n                for y in np.ravel(xlnscopy(v)):\n                    if isinstance(y, xlns) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n                        if y.x == -1e309:\n                            t += [-9223372036854775807 - 1 + y.s]\n                        else:\n                            t += [2 * int(cv * y.x + 0.5) + y.s]\n                    elif y == 0:\n                        t += [-9223372036854775807 - 1]\n                    else:\n                        t += [2 * xlnsb(y, setB).x + xlnsb(y, setB).s]\n                self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        elif isinstance(v, list):\n            t = []\n            for y in xlnscopy(v):\n                if y.x == -1e309:\n                    t += [-9223372036854775807 - 1 + y.s]\n                else:\n                    t += [2 * int(cv * y.x + 0.5) + y.s]\n            self.nd = np.array(np.reshape(t, np.shape(v)), dtype='i8')\n        else:\n            self.nd = np.array([])\n\n    def add(self, v):\n        return self + v\n\n    def __str__(self):\n        return str(self.xlns())\n\n    def __repr__(self):\n        return 'xlnsnpb(' + str(np.float64(self.xlns())) + ' B=' + str(self.B) + ')'\n\n    def __bool__(self):\n        return bool(self.nd == 0)\n\n    def xlns(self):\n        \"\"\"convert xlnsnpb to similar-shaped list of xlnsb\"\"\"\n        t = []\n        for y in np.ravel(self.nd):\n            txlns = xlnsb(0, self.B)\n            if y | 1 == -9223372036854775807:\n                txlns.x = -1e309\n            else:\n                txlns.x = y // 2\n            txlns.s = True if y & 1 else False\n            t += [txlns]\n        return np.reshape(t, np.shape(self.nd))\n\n    def ovf(x):\n        \"\"\"return copy except for values that underflow/overflow, which are clipped according to xlnssetovf\"\"\"\n        global ovfmin, ovfmax\n        return xlnsnpb.where((abs(x) >= ovfmin) * (abs(x) < ovfmax), x, x * xlnsnpb.zeros(x.shape()) + (abs(x) >= ovfmax) * ovfmax * x.sign())\n\n    def conjugate(self):\n        return self\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpb(math.exp(x), xlnsB)\n        if isinstance(x, xlnsnpb):\n            return xlnsnpb(np.exp(np.float64(x.xlns())), x.B)\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpb(math.log(x), xlnsB)\n        if isinstance(x, xlnsnpb):\n            return xlnsnpb(np.log(np.float64(x.xlns())), x.B)\n\n    def __mul__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return self * xlnsnpb(v, self.B)\n        if isinstance(v, xlnsnpb):\n            t = xlnsnpb('', self.B)\n            if v.B == self.B:\n                t.nd = np.where((self.nd | 1 == -9223372036854775807) | (v.nd | 1 == -9223372036854775807), -9223372036854775807 - 1 | (self.nd ^ v.nd) & 1, self.nd + (v.nd - (v.nd & 1)) ^ v.nd & 1)\n            else:\n                cv = np.log(v.B) / np.log(self.B)\n                t.nd = np.where((self.nd | 1 == -9223372036854775807) | (v.nd | 1 == -9223372036854775807), -9223372036854775807 - 1 | (self.nd ^ v.nd) & 1, self.nd + np.int64(cv * (v.nd // 2) + 0.5) * 2 ^ v.nd & 1)\n            return t\n        else:\n            return self * xlnsnpb(v, self.B)\n\n# Define a placeholder function for xlnsud\ndef xlnsud(y):\n    # Assuming xlnsud is supposed to return the parameter unchanged or perform minimal operations\n    return y\n\ndef XXXxlnsudcopy(x):\n    r = []\n    for y in x:\n        if isinstance(y, list) or isinstance(y, np.ndarray):\n            r += [XXXxlnsudcopy(y)]\n        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.float64) or isinstance(y, np.float32) or isinstance(y, xlns) or isinstance(y, xlnsr) or isinstance(y, xlnsnp) or isinstance(y, xlnsnpr) or isinstance(y, xlnsnpv) or isinstance(y, xlnsv) or isinstance(y, xlnsb):\n            r += [xlnsud(y)]\n        else:\n            r += [y]\n    return r\n\n\n\ndef test_XXXxlnsudcopy():\n    # Test with simple scalar types\n    assert XXXxlnsudcopy([5]) == XXXxlnsudcopy_new_implementation([5])\n    # Test with nested lists\n    assert XXXxlnsudcopy([[1, 2], [3, 4]]) == XXXxlnsudcopy_new_implementation([[1, 2], [3, 4]])\n    # Test with numpy arrays\n    assert XXXxlnsudcopy(np.array([1, 2, 3])) == XXXxlnsudcopy_new_implementation(np.array([1, 2, 3]))\n\nif __name__ == \"__main__\":\n    test_XXXxlnsudcopy()"
    },
    {
        "func_name": "softmax_orig",
        "idx": "141",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "examples/arnnpb.py",
        "orig_func": "def softmax_orig(inp):\n    max_vals = np.max(inp, axis=1)\n    max_vals = np.reshape(max_vals, (max_vals.size, 1))\n    u = np.exp(inp - max_vals)\n    v = np.sum(u, axis=1)\n    v.shape = (v.size, 1)\n    u = u / v\n    return u",
        "orig_context": "```python\n## examples/arnnpb.py\nimport numpy as np\n\ndef softmax_orig(inp):\n        max_vals = np.max(inp, axis=1)\n        max_vals = np.reshape(max_vals, (max_vals.size, 1))\n        u = np.exp(inp - max_vals)\n        v = np.sum(u, axis=1)\n        v.shape = v.size, 1\n        u = u / v\n        return u\n\n```\n\n\n",
        "eval_script": "import numpy as np\n\ndef softmax_orig(inp):\n    max_vals = np.max(inp, axis=1)\n    max_vals = np.reshape(max_vals, (max_vals.size, 1))\n    u = np.exp(inp - max_vals)\n    v = np.sum(u, axis=1)\n    v.shape = v.size, 1\n    u = u / v\n    return u\n\n# Assuming this is the new implementation we need to test\n\n\ndef test_softmax_orig():\n    # Test case 1: symmetric input\n    inp1 = np.array([[1, 2, 3], [1, 2, 3]])\n    assert np.allclose(softmax_orig(inp1), softmax_orig_new_implementation(inp1)), \"Test case 1 failed\"\n\n    # Test case 2: zero input\n    inp2 = np.array([[0, 0, 0], [0, 0, 0]])\n    assert np.allclose(softmax_orig(inp2), softmax_orig_new_implementation(inp2)), \"Test case 2 failed\"\n    \n    # Test case 3: negative numbers\n    inp3 = np.array([[-1, -2, -3], [-1, -2, -3]])\n    assert np.allclose(softmax_orig(inp3), softmax_orig_new_implementation(inp3)), \"Test case 3 failed\"\n\n    print(\"All test cases passed.\")\n\nif __name__ == \"__main__\":\n    test_softmax_orig()"
    },
    {
        "func_name": "sb_ufunc_premit",
        "idx": "142",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "src/conf/lpvip_ufunc.py",
        "orig_func": "def sb_ufunc_premit(zi, F):\n    postcond = np.where(zi <= -(3 << F), 0, np.where(zi >= -(3 << F - 2), -1, +1))\n    z = (zi << 3) + (zi & 4095 ^ 4095) + 16 >> 3\n    return np.where(zi == 0, 1 << F, ((1 << F) + (z & (1 << F) - 1) >> -(z >> F)) + postcond) << 1",
        "orig_context": "```python\n## src/conf/lpvip_ufunc.py\nimport numpy as np\n\ndef sb_ufunc_premit(zi,F):   #was called premitchnpi(zi):\n  postcond = np.where(zi <= -(3<<F), 0,\n             np.where(zi >= -(3<<(F-2)), -1, +1))\n  z = ((zi<<3) + (zi&0xfff^0xfff) + 16)>>3\n  return np.where(zi==0,1<<F, (((1<<F)+(z&((1<<F)-1)))>>(-(z>>F)))+postcond )<<1\n\n```\n\n\n",
        "eval_script": "## src/conf/lpvip_ufunc.py\nimport numpy as np\n\ndef sb_ufunc_premit(zi,F):   #was called premitchnpi(zi):\n  postcond = np.where(zi <= -(3<<F), 0,\n             np.where(zi >= -(3<<(F-2)), -1, +1))\n  z = ((zi<<3) + (zi&0xfff^0xfff) + 16)>>3\n  return np.where(zi==0,1<<F, (((1<<F)+(z&((1<<F)-1)))>>max(0, -(z>>F)))+postcond )<<1\n\n\ndef test_sb_ufunc_premit():\n  assert np.array_equal(sb_ufunc_premit(0, 4), sb_ufunc_premit_new_implementation(0, 4))\n  assert np.array_equal(sb_ufunc_premit(-10, 3), sb_ufunc_premit_new_implementation(-10, 3))\n  assert np.array_equal(sb_ufunc_premit(100, 5), sb_ufunc_premit_new_implementation(100, 5))\n  \nif __name__ == \"__main__\":\n  test_sb_ufunc_premit()"
    },
    {
        "func_name": "xlnsnpr.xlns",
        "idx": "146",
        "repo_name": "xlnsresearch___xlns",
        "func_path": "src/xlns.py",
        "orig_func": "def xlns(self):\n    \"\"\"convert xlnsnpr to similar-shaped list of xlns\"\"\"\n    t = []\n    yp = np.ravel(self.ndp)\n    yn = np.ravel(self.ndn)\n    for i in range(len(yp)):\n        txlns = xlns(0)\n        if yp[i] == yn[i]:\n            txlns.x = -1e309\n        else:\n            txlns.x = np.maximum(yp[i], yn[i]) + sbdb_ufunc(-np.abs(yp[i] - yn[i]), 1) // 2\n        txlns.s = True if yn[i] > yp[i] else False\n        t += [txlns]\n    return np.reshape(t, np.shape(self.ndp))",
        "orig_context": "```python\n# src/xlns.py\n\nxlnsnpr.sum_default = xlnsnpr.sum\nimport math\nimport numpy as np\n\ndef sbdb_ufunc_ideal(z, s, B=None, F=None):\n    \"\"\"default ufunc for ideal Gaussian log for all add,sub; may substitute user supplied sbdb_ufunc\"\"\"\n    global xlnsB\n    if B == None:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + xlnsB ** np.minimum(-s, z))) / math.log(xlnsB)))\n    else:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + B ** np.minimum(-s, z))) / math.log(B)))\nsbdb_ufunc = sbdb_ufunc_ideal\n\nclass xlnsnpv(xlnsnp):\n    \"\"\"numpy-like array of non-redundant LNS using int precision f <= xlnsF on each instance\"\"\"\n\n    def __init__(self, v, setF=None):\n        global xlns1stCons\n        xlns1stCons = False\n        if setF == None:\n            setF = xlnsF\n        if setF < xlnsF:\n            self.f = setF\n        else:\n            self.f = xlnsF\n        if not isinstance(v, xlnsnp):\n            v = xlnsnp(v)\n        if self.f >= xlnsF or len(v.nd) == 0:\n            self.nd = v.nd\n        else:\n            self.nd = v.nd + (1 << xlnsF - self.f) & (-1 << xlnsF - self.f + 1 | 1)\n\n    def xlns(self):\n        \"\"\"convert xlnsnpv to similar-shaped list of xlnsv\"\"\"\n        t = []\n        for y in xlnsnp.xlns(self.ravel()):\n            t += [xlnsv(y, self.f)]\n        return np.reshape(t, np.shape(self.nd))\n\n    def ovf(x):\n        \"\"\"return copy except for values that underflow/overflow, which are clipped according to xlnssetovf\"\"\"\n        global ovfmin, ovfmax\n        return xlnsnpv.where((abs(x) >= ovfmin) * (abs(x) < ovfmax), x, x * xlnsnpv.zeros(x.shape()) + (abs(x) >= ovfmax) * ovfmax * x.sign())\n\n    def __repr__(self):\n        return 'xlnsnpv(' + str(np.float64(self.xlns())) + ' f=' + str(self.f) + ')'\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpv(math.exp(x))\n        if isinstance(x, xlnsnpv):\n            return xlnsnpv(math.exp(1) ** x, x.f)\n        if isinstance(x, xlnsnp):\n            return math.exp(1) ** x\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpv(math.log(x))\n        if isinstance(x, xlnsnpv):\n            return xlnsnpv(np.log(x.xlns()), x.f)\n        if isinstance(x, xlnsnp):\n            return xlnsnpv(np.log(x.xlns()))\n\n    def __mul__(self, v):\n        if isinstance(v, xlnsnp):\n            return xlnsnpv(xlnsnp.__mul__(self, v), self.f)\n        else:\n            return self * xlnsnpv(v, self.f)\n\n    def __truediv__(self, v):\n        if isinstance(v, xlnsnp):\n            return xlnsnpv(xlnsnp.__truediv__(self, v), self.f)\n        else:\n            return self / xlnsnpv(v, self.f)\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(xlnsnpv(v, self.f))\n\n    def __rsub__(self, v):\n        return (-self).__add__(xlnsnpv(v, self.f))\n\n    def __rmul__(self, v):\n        return self.__mul__(xlnsnpv(v, self.f))\n\n    def __rtruediv__(self, v):\n        return xlnsnpv(xlnsnp.__truediv__(xlnsnp(v), self), self.f)\n\n    def __pow__(self, v):\n        t = xlnsnpv('')\n        t.f = self.f\n        if isinstance(v, int):\n            if v & 1 == 0:\n                t.nd = (self.nd & -2) * v & -2\n            else:\n                t.nd = (self.nd & -2) * v & -2 | self.nd & 1\n            return t\n        if isinstance(v, float):\n            t.nd = np.int64((self.nd & -2) * v) & -2\n            return t\n        return xlnsnpv.exp(xlnsnpv.log(self) * v)\n\n    def __rpow__(self, v):\n        t = xlnsnpv('')\n        t.f = self.f\n        if isinstance(v, xlnsnp):\n            t.nd = (v ** self).nd\n        elif isinstance(v, float) or isinstance(v, int):\n            t.nd = (2 - 4 * (self.nd & 1)) * np.int64(xlnsB ** (self.nd // 2) * math.log(v, xlnsB))\n        else:\n            t.nd = (v ** self).nd\n        return t\n\n    def __getitem__(self, v):\n        t = xlnsnpv('')\n        t.f = self.f\n        t.nd = self.nd.__getitem__(v)\n        return t\n\n    def __setitem__(self, v1, v2):\n        self.nd.__setitem__(v1, xlnsnpv(v2, self.f).nd)\n\n    def concatenate(list1):\n        t = xlnsnpv('')\n        t.f = list1[0].f\n        list2 = []\n        for item in list1:\n            list2 += [item.nd]\n        t.nd = np.concatenate(list2)\n        return t\n\n    def vstack(list1):\n        t = xlnsnpv('')\n        t.f = list1[0].f\n        list2 = []\n        for item in list1:\n            list2 += [item.nd]\n        t.nd = np.vstack(list2)\n        return t\n\n    def hstack(list1):\n        t = xlnsnpv('')\n        t.f = list1[0].f\n        list2 = []\n        for item in list1:\n            list2 += [item.nd]\n        t.nd = np.hstack(list2)\n        return t\n\n# ...\n\nclass xlnsnpr:\n    \"\"\"numpy-like array of redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, int) or isinstance(v, float):\n            if v == 0:\n                self.ndp = np.array([0], dtype='i8')\n                self.ndn = np.array([0], dtype='i8')\n            elif v > 0:\n                self.ndp = np.array([xlns(v).x], dtype='i8')\n                self.ndn = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.ndp = np.array([-9223372036854775807 - 1], dtype='i8')\n                self.ndn = np.array([xlns(v).x], dtype='i8')\n        elif isinstance(v, xlns):\n            if v == 0:\n                self.ndp = np.array([0], dtype='i8')\n                self.ndn = np.array([0], dtype='i8')\n            elif v > 0:\n                self.ndp = np.array([v.x], dtype='i8')\n                self.ndn = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.ndp = np.array([-9223372036854775807 - 1], dtype='i8')\n                self.ndn = np.array([v.x], dtype='i8')\n        elif isinstance(v, xlnsr):\n            if v.p == v.n:\n                self.ndp = np.array([0], dtype='i8')\n                self.ndn = np.array([0], dtype='i8')\n            else:\n                if v.p.x != -1e309:\n                    self.ndp = np.array([v.p.x], dtype='i8')\n                else:\n                    self.ndp = np.array([-9223372036854775807 - 1], dtype='i8')\n                if v.n.x != -1e309:\n                    self.ndn = np.array([v.n.x], dtype='i8')\n                else:\n                    self.ndn = np.array([-9223372036854775807 - 1], dtype='i8')\n        elif isinstance(v, xlnsb) or isinstance(v, xlnsv):\n            temp = xlnsnpr(float(v))\n            self.ndp = temp.ndp\n            self.ndn = temp.ndn\n        elif isinstance(v, xlnsnpr):\n            self.ndp = v.ndp\n            self.ndn = v.ndn\n        elif isinstance(v, xlnsnp):\n            temp = xlnsnpr(v.xlns())\n            self.ndp = temp.ndp\n            self.ndn = temp.ndn\n        elif isinstance(v, xlnsnpb) or isinstance(v, xlnsnpv):\n            temp = xlnsnpr(np.float64(v.xlns()))\n            self.ndp = temp.ndp\n            self.ndn = temp.ndn\n        elif isinstance(v, np.ndarray):\n            if isinstance(np.ravel(v)[0], xlns):\n                tp = []\n                tn = []\n                for y in np.ravel(v):\n                    if y.x == -1e309:\n                        tp += [0]\n                        tn += [0]\n                    elif y > 0:\n                        tp += [y.x]\n                        tn += [-9223372036854775807 - 1]\n                    else:\n                        tp += [-9223372036854775807 - 1]\n                        tn += [y.x]\n                self.ndp = np.array(np.reshape(tp, np.shape(v)), dtype='i8')\n                self.ndn = np.array(np.reshape(tn, np.shape(v)), dtype='i8')\n            elif isinstance(np.ravel(v)[0], xlnsv):\n                temp = xlnsnpr(np.float64(v))\n                self.ndp = temp.ndp\n                self.ndn = temp.ndn\n            else:\n                tp = []\n                tn = []\n                for y in np.ravel(xlnscopy(v)):\n                    if y.x == -1e309:\n                        tp += [0]\n                        tn += [0]\n                    elif y > 0:\n                        tp += [y.x]\n                        tn += [-9223372036854775807 - 1]\n                    else:\n                        tp += [-9223372036854775807 - 1]\n                        tn += [y.x]\n                self.ndp = np.array(np.reshape(tp, np.shape(v)), dtype='i8')\n                self.ndn = np.array(np.reshape(tn, np.shape(v)), dtype='i8')\n        elif isinstance(v, list):\n            if isinstance(v[0], xlnsv):\n                temp = xlnsnpr(np.float64(v))\n                self.ndp = temp.ndp\n                self.ndn = temp.ndn\n            else:\n                tp = []\n                tn = []\n                for y in np.ravel(xlnscopy(v)):\n                    if y.x == -1e309:\n                        tp += [0]\n                        tn += [0]\n                    elif y > 0:\n                        tp += [y.x]\n                        tn += [-9223372036854775807 - 1]\n                    else:\n                        tp += [-9223372036854775807 - 1]\n                        tn += [y.x]\n                self.ndp = np.array(np.reshape(tp, np.shape(v)), dtype='i8')\n                self.ndn = np.array(np.reshape(tn, np.shape(v)), dtype='i8')\n        else:\n            self.nd = np.array([])\n\n    def add(self, v):\n        return self + v\n\n    def __str__(self):\n        return str(self.xlns())\n\n    def __repr__(self):\n        return 'xlnsnpr(' + str(self.xlns()) + ')'\n\n    def __bool__(self):\n        return bool(self.nd == 0)\n\n    def __mul__(self, v):\n        if isinstance(v, xlnsnpr) or isinstance(v, xlnsr):\n            print('should not xlnsnpr*xlns(np)r: converted to xlnsnp')\n            return self * xlnsnp(v)\n        elif isinstance(v, int) or isinstance(v, float):\n            return self * xlnsnp(v)\n        elif isinstance(v, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where((self.ndp == self.ndn) | (v.nd | 1 == -9223372036854775807), 0, np.where(v.nd & 1, np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn + v.nd // 2), np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp + v.nd // 2)))\n            t.ndn = np.where((self.ndp == self.ndn) | (v.nd | 1 == -9223372036854775807), 0, np.where(v.nd & 1, np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp + v.nd // 2), np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn + v.nd // 2)))\n            return t\n        else:\n            return self * xlnsnp(v)\n\n    def __truediv__(self, v):\n        if isinstance(v, xlnsnpr) or isinstance(v, xlnsr):\n            print('should not xlnsnpr/xlns(np)r: converted to xlnsnp')\n            return self / xlnsnp(v)\n        elif isinstance(v, int) or isinstance(v, float):\n            return self / xlnsnp(v)\n        elif isinstance(v, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where(v.nd & 1, np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn - v.nd // 2), np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp - v.nd // 2))\n            t.ndn = np.where(v.nd & 1, np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp - v.nd // 2), np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn - v.nd // 2))\n            return t\n        else:\n            return self / xlnsnp(v)\n\n    def __neg__(self):\n        t = self * -1\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(xlnsnpr(v))\n\n    def __rsub__(self, v):\n        return (-self).__add__(xlnsnpr(v))\n\n    def __rdiv__(self, v):\n        return self.__div__(xlnsnp(v))\n\n    def __rmul__(self, v):\n        return self.__mul__(xlnsnp(v))\n\n    def __rtruediv__(self, v):\n        return xlnsnp(v).__truediv__(self)\n\n    def __len__(self):\n        return self.ndp.__len__()\n\n    def __getitem__(self, v):\n        t = xlnsnpr('')\n        t.ndp = self.ndp.__getitem__(v)\n        t.ndn = self.ndn.__getitem__(v)\n        return t\n\n    def __setitem__(self, v1, v2):\n        if isinstance(v2, xlnsnpr):\n            self.ndp.__setitem__(v1, v2.ndp)\n            self.ndn.__setitem__(v1, v2.ndn)\n        else:\n            t = xlnsnpr(v2)\n            self.ndp.__setitem__(v1, t.ndp)\n            self.ndn.__setitem__(v1, t.ndn)\n\n    def where(self, vt, vf):\n        if isinstance(vt, xlnsnpr) and isinstance(vf, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where(self.ndp == self.npn, vt.ndp, vf.ndp)\n            t.ndn = np.where(self.ndp == self.npn, vt.ndn, vf.ndn)\n            return t\n        else:\n            print('xlnsnpr.where must match type')\n            return None\n\n    def sign(v):\n        if isinstance(v, xlnsnpr):\n            t = xlnsnp('')\n            t.nd = np.where(v.ndp == v.ndn, -9223372036854775807 - 1, np.where(v.ndp > v.ndn, 0, 1))\n            return t\n        else:\n            return np.sign(v)\n\n    def concatenate(list1):\n        t = xlnsnpr('')\n        list2p = []\n        list2n = []\n        for item in list1:\n            list2p += [item.ndp]\n            list2n += [item.ndn]\n        t.ndp = np.concatenate(list2p)\n        t.ndn = np.concatenate(list2n)\n        return t\n\n    def vstack(list1):\n        t = xlnsnpr('')\n        list2p = []\n        list2n = []\n        for item in list1:\n            list2p += [item.ndp]\n            list2n += [item.ndn]\n        t.ndp = np.vstack(list2p)\n        t.ndn = np.vstack(list2n)\n        return t\n\n    def hstack(list1):\n        t = xlnsnpr('')\n        list2p = []\n        list2n = []\n        for item in list1:\n            list2p += [item.ndp]\n            list2n += [item.ndn]\n        t.ndp = np.hstack(list2p)\n        t.ndn = np.hstack(list2n)\n        return t\n\n    def zeros(v):\n        t = xlnsnpr('')\n        t.ndp = np.zeros(v, dtype='i8')\n        t.ndn = np.zeros(v, dtype='i8')\n        return t\n\n    def sum(self, axis=None):\n        if axis == None:\n            a = 1.0 * xlnsnpr.ravel(self)\n        elif axis == 1:\n            a = 1.0 * xlnsnpr.transpose(self)\n        else:\n            a = 1.0 * self\n        if len(a) == 1:\n            return a\n        mid = 2 ** int(np.floor(math.log(len(a) - 1, 2)))\n        end = len(a)\n        while mid > 0:\n            a[0:end - mid] += a[mid:end]\n            end = mid\n            mid //= 2\n        return a[0]\n\n    def prod(self, axis=None):\n        if axis == None:\n            a = 1.0 * xlnsnp.ravel(self)\n        elif axis == 1:\n            a = 1.0 * xlnsnp.transpose(self)\n        else:\n            a = 1.0 * self\n        mid = 2 ** int(np.floor(math.log(len(a) - 1, 2)))\n        end = len(a)\n        while mid > 0:\n            a[0:end - mid] *= a[mid:end]\n            end = mid\n            mid //= 2\n        return a[0]\n\n    def ravel(v):\n        t = xlnsnpr('')\n        t.ndp = np.ravel(v.ndp)\n        t.ndn = np.ravel(v.ndn)\n        return t\n\n    def shape(v):\n        return np.shape(v.ndp)\n\n    def transpose(v):\n        t = xlnsnpr('')\n        t.ndp = np.transpose(v.ndp)\n        t.ndn = np.transpose(v.ndn)\n        return t\n\n    def size(v):\n        return np.size(v.ndp)\n\n    def max(self, axis=None):\n        return xlnsnpr(np.max(self.xlns(), axis=axis))\n\n    def min(self, axis=None):\n        return xlnsnpr(np.min(self.xlns(), axis=axis))\n\n    def argmax(self, axis=None):\n        return np.argmax(self.xlns(), axis=axis)\n\n    def reshape(self, shape):\n        t = xlnsnpr('')\n        t.ndp = np.reshape(self.ndp, shape)\n        t.ndn = np.reshape(self.ndn, shape)\n        return t\n\n    def __matmul__(self, v):\n        if not isinstance(v, xlnsnp):\n            print('converting v to xlnsnp for @')\n            v = xlnsnp(v)\n        t = xlnsnpr.zeros((xlnsnpr.shape(self)[0], xlnsnp.shape(v)[1]))\n        vtrans = xlnsnp.transpose(v)\n        for r in range(xlnsnpr.shape(self)[0]):\n            t[r] = (self[r] * vtrans).sum(axis=1)\n        return t\n\n    def __rmatmul__(self, v):\n        print('rmatmul')\n        if not isinstance(v, xlnsnp):\n            print('converting v to xlnsnp for @')\n            v = xlnsnp(v)\n        t = xlnsnpr.zeros((xlnsnp.shape(v)[0], xlnsnpr.shape(self)[1]))\n        selftrans = xlnsnpr.transpose(self)\n        for r in range(xlnsnp.shape(v)[0]):\n            t[r] = (selftrans * v[r]).sum(axis=1)\n        return t\n\n    def __abs__(self):\n        t = xlnsnpr('')\n        t.ndp = np.where(self.ndp > self.ndn, self.ndp, self.ndn)\n        t.ndn = np.where(self.ndp > self.ndn, self.ndn, self.ndp)\n        return t\n\n    def __add__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return self + xlnsnp(v)\n        if isinstance(v, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where(v.nd | 1 == -9223372036854775807, self.ndp, np.where(v.nd & 1, self.ndp, np.maximum(self.ndp, v.nd // 2) + sbdb_ufunc(-np.abs(v.nd // 2 - self.ndp), 0) // 2))\n            t.ndn = np.where(v.nd | 1 == -9223372036854775807, self.ndn, np.where(v.nd & 1, np.maximum(self.ndn, v.nd // 2) + sbdb_ufunc(-np.abs(v.nd // 2 - self.ndn), 0) // 2, self.ndn))\n            return t\n        if isinstance(v, xlnsnpr):\n            t = xlnsnpr('')\n            t.ndp = np.where(self.ndp == self.ndn, v.ndp, np.where(v.ndp == v.ndn, self.ndp, np.maximum(self.ndp, v.ndp) + sbdb_ufunc(-np.abs(v.ndp - self.ndp), 0) // 2))\n            t.ndn = np.where(self.ndp == self.ndn, v.ndn, np.where(v.ndp == v.ndn, self.ndn, np.maximum(self.ndn, v.ndn) + sbdb_ufunc(-np.abs(v.ndn - self.ndn), 0) // 2))\n            return t\n        else:\n            return self + xlnsnp(v)\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self < xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp < t.ndn)\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self > xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp > t.ndn)\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self >= xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp >= t.ndn)\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self <= xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp <= t.ndn)\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self == xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp == t.ndn)\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self != xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp != t.ndn)\n\n    def xlns(self):\n        \"\"\"convert xlnsnpr to similar-shaped list of xlns\"\"\"\n        t = []\n        yp = np.ravel(self.ndp)\n        yn = np.ravel(self.ndn)\n        for i in range(len(yp)):\n            txlns = xlns(0)\n            if yp[i] == yn[i]:\n                txlns.x = -1e309\n            else:\n                txlns.x = np.maximum(yp[i], yn[i]) + sbdb_ufunc(-np.abs(yp[i] - yn[i]), 1) // 2\n            txlns.s = True if yn[i] > yp[i] else False\n            t += [txlns]\n        return np.reshape(t, np.shape(self.ndp))\n\n```\n",
        "eval_script": "# Mock definitions for missing classes/functions.\nclass xlns:\n    def __init__(self, x):\n        self.x = x\n        self.s = False\n    def __eq__(self, other):\n        return isinstance(other, xlns) and self.x == other.x and self.s == other.s\n\nclass xlnsnp:\n    def __init__(self, x):\n        self.nd = x\n    def xlns(self):\n        return self.nd\n\nclass xlnsr:\n    def __init__(self, p, n):\n        self.p = xlns(p)\n        self.n = xlns(n)\n\nclass xlnsb:\n    def __init__(self, x):\n        self.x = x\n\nclass xlnsv:\n    def __init__(self, x):\n        self.x = x\n\nclass xlnsnpb:\n    def __init__(self, x):\n        self.nd = x\n    def xlns(self):\n        return self.nd\n\ndef xlnscopy(v):\n    return v\n\nxlnsB = 1.0  # Placeholder, needs proper value.\nxlnsF = 10  # Placeholder, needs proper value.\novfmin = -1e10  # Mock value for overflow minimum.\novfmax = 1e10   # Mock value for overflow maximum.\n\n# Mock function implementations for the unavailable ones in the code.\ndef sbdb_ufunc(z, s):\n    return 0\n\n# Original code remains the same, placed after mock definitions for execution\nimport numpy as np\nimport math\n\ndef sbdb_ufunc_ideal(z, s, B=None, F=None):\n    \"\"\"default ufunc for ideal Gaussian log for all add,sub; may substitute user supplied sbdb_ufunc\"\"\"\n    global xlnsB\n    if B is None:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + xlnsB ** np.minimum(-s, z))) / math.log(xlnsB)))\n    else:\n        return 2 * np.int64(np.round(np.log(np.abs(1.0 - 2.0 * s + B ** np.minimum(-s, z))) / math.log(B)))\nsbdb_ufunc = sbdb_ufunc_ideal\n\nclass xlnsnpv(xlnsnp):\n    \"\"\"numpy-like array of non-redundant LNS using int precision f <= xlnsF on each instance\"\"\"\n\n    def __init__(self, v, setF=None):\n        global xlns1stCons\n        xlns1stCons = False\n        if setF is None:\n            setF = xlnsF\n        if setF < xlnsF:\n            self.f = setF\n        else:\n            self.f = xlnsF\n        if not isinstance(v, xlnsnp):\n            v = xlnsnp(v)\n        if self.f >= xlnsF or len(v.nd) == 0:\n            self.nd = v.nd\n        else:\n            self.nd = v.nd + (1 << xlnsF - self.f) & (-1 << xlnsF - self.f + 1 | 1)\n\n    def xlns(self):\n        \"\"\"convert xlnsnpv to similar-shaped list of xlnsv\"\"\"\n        t = []\n        for y in xlnsnp.xlns(self.ravel()):\n            t += [xlnsv(y, self.f)]\n        return np.reshape(t, np.shape(self.nd))\n\n    def ovf(x):\n        \"\"\"return copy except for values that underflow/overflow, which are clipped according to xlnssetovf\"\"\"\n        global ovfmin, ovfmax\n        return xlnsnpv.where((abs(x) >= ovfmin) * (abs(x) < ovfmax), x, x * xlnsnpv.zeros(x.shape()) + (abs(x) >= ovfmax) * ovfmax * x.sign())\n\n    def __repr__(self):\n        return 'xlnsnpv(' + str(np.float64(self.xlns())) + ' f=' + str(self.f) + ')'\n\n    def exp(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpv(math.exp(x))\n        if isinstance(x, xlnsnpv):\n            return xlnsnpv(math.exp(1) ** x, x.f)\n        if isinstance(x, xlnsnp):\n            return math.exp(1) ** x\n\n    def log(x):\n        if isinstance(x, float) or isinstance(x, int):\n            return xlnsnpv(math.log(x))\n        if isinstance(x, xlnsnpv):\n            return xlnsnpv(np.log(x.xlns()), x.f)\n        if isinstance(x, xlnsnp):\n            return xlnsnpv(np.log(x.xlns()))\n\n    def __mul__(self, v):\n        if isinstance(v, xlnsnp):\n            return xlnsnpv(xlnsnp.__mul__(self, v), self.f)\n        else:\n            return self * xlnsnpv(v, self.f)\n\n    def __truediv__(self, v):\n        if isinstance(v, xlnsnp):\n            return xlnsnpv(xlnsnp.__truediv__(self, v), self.f)\n        else:\n            return self / xlnsnpv(v, self.f)\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(xlnsnpv(v, self.f))\n\n    def __rsub__(self, v):\n        return (-self).__add__(xlnsnpv(v, self.f))\n\n    def __rmul__(self, v):\n        return self.__mul__(xlnsnpv(v, self.f))\n\n    def __rtruediv__(self, v):\n        return xlnsnpv(xlnsnp.__truediv__(xlnsnp(v), self), self.f)\n\n    def __pow__(self, v):\n        t = xlnsnpv('')\n        t.f = self.f\n        if isinstance(v, int):\n            if v & 1 == 0:\n                t.nd = (self.nd & -2) * v & -2\n            else:\n                t.nd = (self.nd & -2) * v & -2 | self.nd & 1\n            return t\n        if isinstance(v, float):\n            t.nd = np.int64((self.nd & -2) * v) & -2\n            return t\n        return xlnsnpv.exp(xlnsnpv.log(self) * v)\n\n    def __rpow__(self, v):\n        t = xlnsnpv('')\n        t.f = self.f\n        if isinstance(v, xlnsnp):\n            t.nd = (v ** self).nd\n        elif isinstance(v, float) or isinstance(v, int):\n            t.nd = (2 - 4 * (self.nd & 1)) * np.int64(xlnsB ** (self.nd // 2) * math.log(v, xlnsB))\n        else:\n            t.nd = (v ** self).nd\n        return t\n\n    def __getitem__(self, v):\n        t = xlnsnpv('')\n        t.f = self.f\n        t.nd = self.nd.__getitem__(v)\n        return t\n\n    def __setitem__(self, v1, v2):\n        self.nd.__setitem__(v1, xlnsnpv(v2, self.f).nd)\n\n    def concatenate(list1):\n        t = xlnsnpv('')\n        t.f = list1[0].f\n        list2 = []\n        for item in list1:\n            list2 += [item.nd]\n        t.nd = np.concatenate(list2)\n        return t\n\n    def vstack(list1):\n        t = xlnsnpv('')\n        t.f = list1[0].f\n        list2 = []\n        for item in list1:\n            list2 += [item.nd]\n        t.nd = np.vstack(list2)\n        return t\n\n    def hstack(list1):\n        t = xlnsnpv('')\n        t.f = list1[0].f\n        list2 = []\n        for item in list1:\n            list2 += [item.nd]\n        t.nd = np.hstack(list2)\n        return t\n\nclass xlnsnpr:\n    \"\"\"numpy-like array of redundant LNS using global base xlnsB\"\"\"\n\n    def __init__(self, v):\n        global xlns1stCons\n        xlns1stCons = False\n        if isinstance(v, int) or isinstance(v, float):\n            if v == 0:\n                self.ndp = np.array([0], dtype='i8')\n                self.ndn = np.array([0], dtype='i8')\n            elif v > 0:\n                self.ndp = np.array([xlns(v).x], dtype='i8')\n                self.ndn = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.ndp = np.array([-9223372036854775807 - 1], dtype='i8')\n                self.ndn = np.array([xlns(v).x], dtype='i8')\n        elif isinstance(v, xlns):\n            if v == 0:\n                self.ndp = np.array([0], dtype='i8')\n                self.ndn = np.array([0], dtype='i8')\n            elif v > 0:\n                self.ndp = np.array([v.x], dtype='i8')\n                self.ndn = np.array([-9223372036854775807 - 1], dtype='i8')\n            else:\n                self.ndp = np.array([-9223372036854775807 - 1], dtype='i8')\n                self.ndn = np.array([v.x], dtype='i8')\n        elif isinstance(v, xlnsr):\n            if v.p == v.n:\n                self.ndp = np.array([0], dtype='i8')\n                self.ndn = np.array([0], dtype='i8')\n            else:\n                if v.p.x != -1e309:\n                    self.ndp = np.array([v.p.x], dtype='i8')\n                else:\n                    self.ndp = np.array([-9223372036854775807 - 1], dtype='i8')\n                if v.n.x != -1e309:\n                    self.ndn = np.array([v.n.x], dtype='i8')\n                else:\n                    self.ndn = np.array([-9223372036854775807 - 1], dtype='i8')\n        elif isinstance(v, xlnsb) or isinstance(v, xlnsv):\n            temp = xlnsnpr(float(v))\n            self.ndp = temp.ndp\n            self.ndn = temp.ndn\n        elif isinstance(v, xlnsnpr):\n            self.ndp = v.ndp\n            self.ndn = v.ndn\n        elif isinstance(v, xlnsnp):\n            temp = xlnsnpr(v.xlns())\n            self.ndp = temp.ndp\n            self.ndn = temp.ndn\n        elif isinstance(v, xlnsnpb) or isinstance(v, xlnsnpv):\n            temp = xlnsnpr(np.float64(v.xlns()))\n            self.ndp = temp.ndp\n            self.ndn = temp.ndn\n        elif isinstance(v, np.ndarray):\n            if isinstance(np.ravel(v)[0], xlns):\n                tp = []\n                tn = []\n                for y in np.ravel(v):\n                    if y.x == -1e309:\n                        tp += [0]\n                        tn += [0]\n                    elif y > 0:\n                        tp += [y.x]\n                        tn += [-9223372036854775807 - 1]\n                    else:\n                        tp += [-9223372036854775807 - 1]\n                        tn += [y.x]\n                self.ndp = np.array(np.reshape(tp, np.shape(v)), dtype='i8')\n                self.ndn = np.array(np.reshape(tn, np.shape(v)), dtype='i8')\n            elif isinstance(np.ravel(v)[0], xlnsv):\n                temp = xlnsnpr(np.float64(v))\n                self.ndp = temp.ndp\n                self.ndn = temp.ndn\n            else:\n                tp = []\n                tn = []\n                for y in np.ravel(xlnscopy(v)):\n                    if y.x == -1e309:\n                        tp += [0]\n                        tn += [0]\n                    elif y > 0:\n                        tp += [y.x]\n                        tn += [-9223372036854775807 - 1]\n                    else:\n                        tp += [-9223372036854775807 - 1]\n                        tn += [y.x]\n                self.ndp = np.array(np.reshape(tp, np.shape(v)), dtype='i8')\n                self.ndn = np.array(np.reshape(tn, np.shape(v)), dtype='i8')\n        elif isinstance(v, list):\n            if isinstance(v[0], xlnsv):\n                temp = xlnsnpr(np.float64(v))\n                self.ndp = temp.ndp\n                self.ndn = temp.ndn\n            else:\n                tp = []\n                tn = []\n                for y in np.ravel(xlnscopy(v)):\n                    if y.x == -1e309:\n                        tp += [0]\n                        tn += [0]\n                    elif y > 0:\n                        tp += [y.x]\n                        tn += [-9223372036854775807 - 1]\n                    else:\n                        tp += [-9223372036854775807 - 1]\n                        tn += [y.x]\n                self.ndp = np.array(np.reshape(tp, np.shape(v)), dtype='i8')\n                self.ndn = np.array(np.reshape(tn, np.shape(v)), dtype='i8')\n        else:\n            self.nd = np.array([])\n\n    def add(self, v):\n        return self + v\n\n    def __str__(self):\n        return str(self.xlns())\n\n    def __repr__(self):\n        return 'xlnsnpr(' + str(self.xlns()) + ')'\n\n    def __bool__(self):\n        return bool(self.nd == 0)\n\n    def __mul__(self, v):\n        if isinstance(v, xlnsnpr) or isinstance(v, xlnsr):\n            print('should not xlnsnpr*xlns(np)r: converted to xlnsnp')\n            return self * xlnsnp(v)\n        elif isinstance(v, int) or isinstance(v, float):\n            return self * xlnsnp(v)\n        elif isinstance(v, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where((self.ndp == self.ndn) | (v.nd | 1 == -9223372036854775807), 0, np.where(v.nd & 1, np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn + v.nd // 2), np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp + v.nd // 2)))\n            t.ndn = np.where((self.ndp == self.ndn) | (v.nd | 1 == -9223372036854775807), 0, np.where(v.nd & 1, np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp + v.nd // 2), np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn + v.nd // 2)))\n            return t\n        else:\n            return self * xlnsnp(v)\n\n    def __truediv__(self, v):\n        if isinstance(v, xlnsnpr) or isinstance(v, xlnsr):\n            print('should not xlnsnpr/xlns(np)r: converted to xlnsnp')\n            return self / xlnsnp(v)\n        elif isinstance(v, int) or isinstance(v, float):\n            return self / xlnsnp(v)\n        elif isinstance(v, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where(v.nd & 1, np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn - v.nd // 2), np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp - v.nd // 2))\n            t.ndn = np.where(v.nd & 1, np.where(self.ndp | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndp - v.nd // 2), np.where(self.ndn | 1 == -9223372036854775807, -9223372036854775807 - 1, self.ndn - v.nd // 2))\n            return t\n        else:\n            return self / xlnsnp(v)\n\n    def __neg__(self):\n        t = self * -1\n        return t\n\n    def __pos__(self):\n        return self\n\n    def __sub__(self, v):\n        return self.__add__(-v)\n\n    def __radd__(self, v):\n        return self.__add__(xlnsnpr(v))\n\n    def __rsub__(self, v):\n        return (-self).__add__(xlnsnpr(v))\n\n    def __rdiv__(self, v):\n        return self.__div__(xlnsnp(v))\n\n    def __rmul__(self, v):\n        return self.__mul__(xlnsnp(v))\n\n    def __rtruediv__(self, v):\n        return xlnsnp(v).__truediv__(self)\n\n    def __len__(self):\n        return len(self.ndp)\n\n    def __getitem__(self, v):\n        t = xlnsnpr('')\n        t.ndp = self.ndp[v]\n        t.ndn = self.ndn[v]\n        return t\n\n    def __setitem__(self, v1, v2):\n        if isinstance(v2, xlnsnpr):\n            self.ndp[v1] = v2.ndp\n            self.ndn[v1] = v2.ndn\n        else:\n            t = xlnsnpr(v2)\n            self.ndp[v1] = t.ndp\n            self.ndn[v1] = t.ndn\n\n    def where(self, vt, vf):\n        if isinstance(vt, xlnsnpr) and isinstance(vf, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where(self.ndp == self.npn, vt.ndp, vf.ndp)\n            t.ndn = np.where(self.ndp == self.npn, vt.ndn, vf.ndn)\n            return t\n        else:\n            print('xlnsnpr.where must match type')\n            return None\n\n    def sign(v):\n        if isinstance(v, xlnsnpr):\n            t = xlnsnp('')\n            t.nd = np.where(v.ndp == v.ndn, -9223372036854775807 - 1, np.where(v.ndp > v.ndn, 0, 1))\n            return t\n        else:\n            return np.sign(v)\n\n    def concatenate(list1):\n        t = xlnsnpr('')\n        list2p = []\n        list2n = []\n        for item in list1:\n            list2p += [item.ndp]\n            list2n += [item.ndn]\n        t.ndp = np.concatenate(list2p)\n        t.ndn = np.concatenate(list2n)\n        return t\n\n    def vstack(list1):\n        t = xlnsnpr('')\n        list2p = []\n        list2n = []\n        for item in list1:\n            list2p += [item.ndp]\n            list2n += [item.ndn]\n        t.ndp = np.vstack(list2p)\n        t.ndn = np.vstack(list2n)\n        return t\n\n    def hstack(list1):\n        t = xlnsnpr('')\n        list2p = []\n        list2n = []\n        for item in list1:\n            list2p += [item.ndp]\n            list2n += [item.ndn]\n        t.ndp = np.hstack(list2p)\n        t.ndn = np.hstack(list2n)\n        return t\n\n    def zeros(v):\n        t = xlnsnpr('')\n        t.ndp = np.zeros(v, dtype='i8')\n        t.ndn = np.zeros(v, dtype='i8')\n        return t\n\n    def sum(self, axis=None):\n        if axis == None:\n            a = 1.0 * self.ravel()\n        elif axis == 1:\n            a = 1.0 * self.transpose()\n        else:\n            a = 1.0 * self\n        if len(a) == 1:\n            return a\n        mid = 2 ** int(np.floor(math.log(len(a) - 1, 2)))\n        end = len(a)\n        while mid > 0:\n            a[0:end - mid] += a[mid:end]\n            end = mid\n            mid //= 2\n        return a[0]\n\n    def prod(self, axis=None):\n        if axis == None:\n            a = 1.0 * xlnsnp.ravel(self)\n        elif axis == 1:\n            a = 1.0 * xlnsnp.transpose(self)\n        else:\n            a = 1.0 * self\n        mid = 2 ** int(np.floor(math.log(len(a) - 1, 2)))\n        end = len(a)\n        while mid > 0:\n            a[0:end - mid] *= a[mid:end]\n            end = mid\n            mid //= 2\n        return a[0]\n\n    def ravel(v):\n        t = xlnsnpr('')\n        t.ndp = np.ravel(v.ndp)\n        t.ndn = np.ravel(v.ndn)\n        return t\n\n    def shape(v):\n        return np.shape(v.ndp)\n\n    def transpose(v):\n        t = xlnsnpr('')\n        t.ndp = np.transpose(v.ndp)\n        t.ndn = np.transpose(v.ndn)\n        return t\n\n    def size(v):\n        return np.size(v.ndp)\n\n    def max(self, axis=None):\n        return xlnsnpr(np.max(self.xlns(), axis=axis))\n\n    def min(self, axis=None):\n        return xlnsnpr(np.min(self.xlns(), axis=axis))\n\n    def argmax(self, axis=None):\n        return np.argmax(self.xlns(), axis=axis)\n\n    def reshape(self, shape):\n        t = xlnsnpr('')\n        t.ndp = np.reshape(self.ndp, shape)\n        t.ndn = np.reshape(self.ndn, shape)\n        return t\n\n    def __matmul__(self, v):\n        if not isinstance(v, xlnsnp):\n            print('converting v to xlnsnp for @')\n            v = xlnsnp(v)\n        t = xlnsnpr.zeros((self.shape()[0], xlnsnp.shape(v)[1]))\n        vtrans = xlnsnp.transpose(v)\n        for r in range(self.shape()[0]):\n            t[r] = (self[r] * vtrans).sum(axis=1)\n        return t\n\n    def __rmatmul__(self, v):\n        print('rmatmul')\n        if not isinstance(v, xlnsnp):\n            print('converting v to xlnsnp for @')\n            v = xlnsnp(v)\n        t = xlnsnpr.zeros((xlnsnp.shape(v)[0], self.shape()[1]))\n        selftrans = self.transpose()\n        for r in range(xlnsnp.shape(v)[0]):\n            t[r] = (selftrans * v[r]).sum(axis=1)\n        return t\n\n    def __abs__(self):\n        t = xlnsnpr('')\n        t.ndp = np.where(self.ndp > self.ndn, self.ndp, self.ndn)\n        t.ndn = np.where(self.ndp > self.ndn, self.ndn, self.ndp)\n        return t\n\n    def __add__(self, v):\n        if isinstance(v, int) or isinstance(v, float):\n            return self + xlnsnp(v)\n        if isinstance(v, xlnsnp):\n            t = xlnsnpr('')\n            t.ndp = np.where(v.nd | 1 == -9223372036854775807, self.ndp, np.where(v.nd & 1, self.ndp, np.maximum(self.ndp, v.nd // 2) + sbdb_ufunc(-np.abs(v.nd // 2 - self.ndp), 0) // 2))\n            t.ndn = np.where(v.nd | 1 == -9223372036854775807, self.ndn, np.where(v.nd & 1, np.maximum(self.ndn, v.nd // 2) + sbdb_ufunc(-np.abs(v.nd // 2 - self.ndn), 0) // 2, self.ndn))\n            return t\n        if isinstance(v, xlnsnpr):\n            t = xlnsnpr('')\n            t.ndp = np.where(self.ndp == self.ndn, v.ndp, np.where(v.ndp == v.ndn, self.ndp, np.maximum(self.ndp, v.ndp) + sbdb_ufunc(-np.abs(v.ndp - self.ndp), 0) // 2))\n            t.ndn = np.where(self.ndp == self.ndn, v.ndn, np.where(v.ndp == v.ndn, self.ndn, np.maximum(self.ndn, v.ndn) + sbdb_ufunc(-np.abs(v.ndn - self.ndn), 0) // 2))\n            return t\n        else:\n            return self + xlnsnp(v)\n\n    def __lt__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self < xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp < t.ndn)\n\n    def __gt__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self > xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp > t.ndn)\n\n    def __ge__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self >= xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp >= t.ndn)\n\n    def __le__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self <= xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp <= t.ndn)\n\n    def __eq__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self == xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp == t.ndn)\n\n    def __ne__(self, v):\n        if not isinstance(v, xlnsnpr):\n            return self != xlnsnpr(v)\n        t = self - v\n        return xlnsnpr.zeros(self.shape()) + (t.ndp != t.ndn)\n\n    def xlns(self):\n        \"\"\"convert xlnsnpr to similar-shaped list of xlns\"\"\"\n        t = []\n        yp = np.ravel(self.ndp)\n        yn = np.ravel(self.ndn)\n        for i in range(len(yp)):\n            txlns = xlns(0)\n            if yp[i] == yn[i]:\n                txlns.x = -1e309\n            else:\n                txlns.x = np.maximum(yp[i], yn[i]) + sbdb_ufunc(-np.abs(yp[i] - yn[i]), 1) // 2\n            txlns.s = True if yn[i] > yp[i] else False\n            t += [txlns]\n        return np.reshape(t, np.shape(self.ndp))\n\n\ndef test_xlns():\n    # Test case 1: Simple float value\n    obj = xlnsnpr(2.0)\n    assert obj.xlns().tolist() == obj.xlns_new_implementation().tolist(), \"Test failed for simple float input\"\n\n    # Test case 2: Zero input\n    obj = xlnsnpr(0)\n    assert obj.xlns().tolist() == obj.xlns_new_implementation().tolist(), \"Test failed for zero input\"\n\n    # Test case 3: Negative float value\n    obj = xlnsnpr(-3.7)\n    assert obj.xlns().tolist() == obj.xlns_new_implementation().tolist(), \"Test failed for negative float input\"\n\nif __name__ == '__main__':\n    test_xlns()"
    },
    {
        "func_name": "get_nodes",
        "idx": "153",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/file_generators/uai_generator.py",
        "orig_func": "def get_nodes(edges: List[Tuple[str, str]]) -> Tuple[List[str], Dict[str, List[str]], Dict[str, List[str]]]:\n    \"\"\"\n    Extracts nodes, their parents, and their children from a list of edges.\n\n    Args:\n        edges (List[Tuple[str, str]]): A list of tuples where each tuple\n            represents an edge in the format (parent, child).\n\n    Returns:\n        Tuple:\n            - nodes (List[str]): A list of all unique nodes in the graph.\n            - node_parents (Dict[str, List[str]]): A dictionary where keys\n              are nodes and values are lists of parent nodes.\n            - node_children (Dict[str, List[str]]): A dictionary where keys\n              are nodes and values are lists of child nodes.\n\n    Example:\n        >>> edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"D\")]\n        >>> nodes, node_parents, node_children = get_nodes(edges)\n        >>> nodes\n        ['A', 'B', 'C', 'D']\n        >>> node_parents\n        {'B': ['A'], 'C': ['A'], 'D': ['B']}\n        >>> node_children\n        {'A': ['B', 'C'], 'B': ['D']}\n    \"\"\"\n    node_parents = {}\n    node_children = {}\n    nodes = set()\n    for (parent, child) in edges:\n        node_parents.setdefault(child, []).append(parent)\n        node_children.setdefault(parent, []).append(child)\n        nodes.update([parent, child])\n    return (list(nodes), node_parents, node_children)",
        "orig_context": "```python\n## utils/file_generators/uai_generator.py\nfrom typing import Dict, List, Tuple, Union\n\ndef get_nodes(\n    edges: List[Tuple[str, str]]\n) -> Tuple[List[str], Dict[str, List[str]], Dict[str, List[str]]]:\n    \"\"\"\n    Extracts nodes, their parents, and their children from a list of edges.\n\n    Args:\n        edges (List[Tuple[str, str]]): A list of tuples where each tuple\n            represents an edge in the format (parent, child).\n\n    Returns:\n        Tuple:\n            - nodes (List[str]): A list of all unique nodes in the graph.\n            - node_parents (Dict[str, List[str]]): A dictionary where keys\n              are nodes and values are lists of parent nodes.\n            - node_children (Dict[str, List[str]]): A dictionary where keys\n              are nodes and values are lists of child nodes.\n\n    Example:\n        >>> edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"D\")]\n        >>> nodes, node_parents, node_children = get_nodes(edges)\n        >>> nodes\n        ['A', 'B', 'C', 'D']\n        >>> node_parents\n        {'B': ['A'], 'C': ['A'], 'D': ['B']}\n        >>> node_children\n        {'A': ['B', 'C'], 'B': ['D']}\n    \"\"\"\n    node_parents = {}\n    node_children = {}\n    nodes = set()\n    for parent, child in edges:\n        node_parents.setdefault(child, []).append(parent)\n        node_children.setdefault(parent, []).append(child)\n        nodes.update([parent, child])\n\n    return list(nodes), node_parents, node_children\n\n```\n\n\n",
        "eval_script": "## utils/file_generators/uai_generator.py\nfrom typing import Dict, List, Tuple, Union\n\ndef get_nodes(\n    edges: List[Tuple[str, str]]\n) -> Tuple[List[str], Dict[str, List[str]], Dict[str, List[str]]]:\n    \"\"\"\n    Extracts nodes, their parents, and their children from a list of edges.\n\n    Args:\n        edges (List[Tuple[str, str]]): A list of tuples where each tuple\n            represents an edge in the format (parent, child).\n\n    Returns:\n        Tuple:\n            - nodes (List[str]): A list of all unique nodes in the graph.\n            - node_parents (Dict[str, List[str]]): A dictionary where keys\n              are nodes and values are lists of parent nodes.\n            - node_children (Dict[str, List[str]]): A dictionary where keys\n              are nodes and values are lists of child nodes.\n\n    Example:\n        >>> edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"D\")]\n        >>> nodes, node_parents, node_children = get_nodes(edges)\n        >>> nodes\n        ['A', 'B', 'C', 'D']\n        >>> node_parents\n        {'B': ['A'], 'C': ['A'], 'D': ['B']}\n        >>> node_children\n        {'A': ['B', 'C'], 'B': ['D']}\n    \"\"\"\n    node_parents = {}\n    node_children = {}\n    nodes = set()\n    for parent, child in edges:\n        node_parents.setdefault(child, []).append(parent)\n        node_children.setdefault(parent, []).append(child)\n        nodes.update([parent, child])\n\n    return list(nodes), node_parents, node_children\n\n\n\ndef test_get_nodes():\n    # Basic test case\n    edges1 = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"D\")]\n    assert get_nodes(edges1) == get_nodes_new_implementation(edges1)\n\n    # Case with no edges\n    edges2 = []\n    assert get_nodes(edges2) == get_nodes_new_implementation(edges2)\n\n    # Case with multiple parents for a single node\n    edges3 = [(\"A\", \"B\"), (\"C\", \"B\"), (\"D\", \"E\")]\n    assert get_nodes(edges3) == get_nodes_new_implementation(edges3)\n\nif __name__ == \"__main__\":\n    test_get_nodes()"
    },
    {
        "func_name": "define_nodes",
        "idx": "154",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/file_generators/uai_generator.py",
        "orig_func": "def define_nodes(nodes: List[str], node_parents: Dict[str, List[str]]) -> Tuple[List[str], List[str]]:\n    \"\"\"Define endogenous and exogenous nodes\n\n    Args:\n        nodes (List[str]): List of all unique nodes in the graph.\n        node_parents (Dict[str, List[str]]): A dictionary where keys are nodes\n            and values are lists of parent nodes.\n\n    Returns:\n        Tuple:\n            - endogenous (List[str]): A list of endogenous nodes.\n            - exogenous (List[str]): A list of exogenous nodes.\n\n    Example:\n        >>> nodes = ['A', 'B', 'C', 'D']\n        >>> node_parents = {'B': ['A'], 'C': ['A'], 'D': ['B']}\n        >>> endogenous, exogenous = define_nodes(nodes, node_parents)\n        >>> endogenous\n        ['B', 'C', 'D']\n        >>> exogenous\n        ['A']\n    \"\"\"\n    endogenous = [node for node in nodes if node in node_parents]\n    exogenous = [node for node in nodes if node not in node_parents]\n    return (endogenous, exogenous)",
        "orig_context": "```python\n## utils/file_generators/uai_generator.py\nfrom typing import Dict, List, Tuple, Union\n\ndef define_nodes(\n    nodes: List[str],\n    node_parents: Dict[str, List[str]]\n) -> Tuple[List[str], List[str]]:\n    \"\"\"Define endogenous and exogenous nodes\n\n    Args:\n        nodes (List[str]): List of all unique nodes in the graph.\n        node_parents (Dict[str, List[str]]): A dictionary where keys are nodes\n            and values are lists of parent nodes.\n\n    Returns:\n        Tuple:\n            - endogenous (List[str]): A list of endogenous nodes.\n            - exogenous (List[str]): A list of exogenous nodes.\n\n    Example:\n        >>> nodes = ['A', 'B', 'C', 'D']\n        >>> node_parents = {'B': ['A'], 'C': ['A'], 'D': ['B']}\n        >>> endogenous, exogenous = define_nodes(nodes, node_parents)\n        >>> endogenous\n        ['B', 'C', 'D']\n        >>> exogenous\n        ['A']\n    \"\"\"\n    endogenous = [node for node in nodes if node in node_parents]\n    exogenous = [node for node in nodes if node not in node_parents]\n    return endogenous, exogenous\n\n```\n\n\n",
        "eval_script": "## utils/file_generators/uai_generator.py\nfrom typing import Dict, List, Tuple, Union\n\ndef define_nodes(\n    nodes: List[str],\n    node_parents: Dict[str, List[str]]\n) -> Tuple[List[str], List[str]]:\n    \"\"\"Define endogenous and exogenous nodes\n\n    Args:\n        nodes (List[str]): List of all unique nodes in the graph.\n        node_parents (Dict[str, List[str]]): A dictionary where keys are nodes\n            and values are lists of parent nodes.\n\n    Returns:\n        Tuple:\n            - endogenous (List[str]): A list of endogenous nodes.\n            - exogenous (List[str]): A list of exogenous nodes.\n\n    Example:\n        >>> nodes = ['A', 'B', 'C', 'D']\n        >>> node_parents = {'B': ['A'], 'C': ['A'], 'D': ['B']}\n        >>> endogenous, exogenous = define_nodes(nodes, node_parents)\n        >>> endogenous\n        ['B', 'C', 'D']\n        >>> exogenous\n        ['A']\n    \"\"\"\n    endogenous = [node for node in nodes if node in node_parents]\n    exogenous = [node for node in nodes if node not in node_parents]\n    return endogenous, exogenous\n\n\n\ndef test_define_nodes():\n    # Test 1: Basic functionality with simple node-parent relationships\n    nodes1 = ['A', 'B', 'C', 'D']\n    node_parents1 = {'B': ['A'], 'C': ['A'], 'D': ['B']}\n    assert define_nodes(nodes1, node_parents1) == define_nodes_new_implementation(nodes1, node_parents1)\n\n    # Test 2: Nodes with no parents listed\n    nodes2 = ['X', 'Y', 'Z']\n    node_parents2 = {}\n    assert define_nodes(nodes2, node_parents2) == define_nodes_new_implementation(nodes2, node_parents2)\n\n    # Test 3: All nodes are endogenous\n    nodes3 = ['M', 'N', 'O']\n    node_parents3 = {'M': ['N'], 'N': ['O'], 'O': []}\n    assert define_nodes(nodes3, node_parents3) == define_nodes_new_implementation(nodes3, node_parents3)\n\nif __name__ == \"__main__\":\n    test_define_nodes()\n"
    },
    {
        "func_name": "generateRelaxed",
        "idx": "155",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/canonical_partitions/canonicalPartitions.py",
        "orig_func": "def generateRelaxed(graph: Graph, latentCardinalities: list[int]):\n    relaxedGraph: str = ''\n    unob: str = ''\n    unobCardinalities: str = ', '.join(map(lambda x: str(int(x)), latentCardinalities))\n    for (index, component) in enumerate(graph.dag_components):\n        currUnobLabel: str = 'U' + str(index)\n        unob += f', {currUnobLabel}'\n        for node in component.nodes:\n            if graph.cardinalities[node] > 1:\n                relaxedGraph += f', {currUnobLabel} -> {graph.index_to_label[node]}'\n    for (index, label) in graph.index_to_label.items():\n        if graph.cardinalities[index] > 1:\n            for node in graph.adj[index]:\n                relaxedGraph += f', {label} -> {graph.index_to_label[node]}'\n    return (relaxedGraph[2:], unob[2:], unobCardinalities)",
        "orig_context": "```python\n## utils/canonical_partitions/graph.py\nclass Graph:\n    def __init__(\n        self, num_nodes: int, curr_nodes: list[int], visited: list[bool],\n        cardinalities: list[int], parents: list[int], adj: list[list[int]],\n        label_to_index: dict[str, int], index_to_label: dict[int, str],\n        dag_components: list[list[int]], exogenous: list[int],\n        endogenous: list[int]\n    ):\n        self.num_nodes = num_nodes\n        self.curr_nodes = curr_nodes\n        self.visited = visited\n        self.cardinalities = cardinalities\n        self.parents = parents\n        self.adj = adj\n        self.label_to_index = label_to_index\n        self.index_to_label = index_to_label\n        self.dag_components = dag_components\n        self.endogenous = endogenous\n        self.exogenous = exogenous\n\n    def parse(predefined_data=None, input_path=None):\n        if predefined_data:\n            num_nodes = predefined_data['num_nodes']\n            num_edges = predefined_data['num_edges']\n        elif input_path:\n            num_nodes, num_edges, nodes, nodes_cardinality, edges = file_parser(input_path)\n        else:\n            num_nodes = int(input())\n            num_edges = int(input())\n\n        label_to_index_ex: dict[str, int] = {}\n        index_to_label_ex: dict[int, str] = {}\n        adj_ex = [[] for _ in range(num_nodes + 1)]\n        cardinalities_ex = [0] * (num_nodes + 1)\n        visited_ex = [False] * (num_nodes + 1)\n        parents_ex = [[] for _ in range(num_nodes + 1)]\n        endogenIndex: list[int] = []\n        exogenIndex: list[int] = []\n\n        for i in range(1, num_nodes + 1):\n            if predefined_data:\n                label, cardinality = predefined_data['nodes'][i - 1].split()\n            elif input_path:\n                label, cardinality = nodes[i-1], nodes_cardinality[i-1]\n            else:\n                label, cardinality = input().split()\n            cardinality = int(cardinality)\n            label_to_index_ex[label] = i\n            index_to_label_ex[i] = label\n            cardinalities_ex[i] = cardinality\n\n        for i in range(num_edges):\n            if predefined_data:\n                u, v = predefined_data['edges'][i].split(\" -> \")\n            elif (input_path):\n                print(edges[i])\n                u, v = edges[i]\n            else:\n                u, v = input().split()\n            u_index = label_to_index_ex[u]\n            v_index = label_to_index_ex[v]\n            adj_ex[u_index].append(v_index)\n            parents_ex[v_index].append(u_index)\n\n        for i in range(1, num_nodes + 1):\n\n            if not (bool(parents_ex[i])):\n                exogenIndex.append(i)\n            else:\n                endogenIndex.append(i)\n\n        return Graph(\n            num_nodes=num_nodes,\n            curr_nodes=[],\n            visited=visited_ex,\n            cardinalities=cardinalities_ex,\n            parents=parents_ex,\n            adj=adj_ex,\n            index_to_label=index_to_label_ex,\n            label_to_index=label_to_index_ex,\n            dag_components=[],\n            exogenous=exogenIndex,\n            endogenous=endogenIndex\n        )\n\ndef file_parser(input_path):\n    with open(input_path, 'r') as file:\n        num_nodes = int(file.readline().strip())\n        num_edges = int(file.readline().strip())\n        nodes = []\n        nodes_cardinality = []\n\n        for _ in range(num_nodes):\n            tuple_node_cardinality = file.readline().strip().split(' ')\n            node = tuple_node_cardinality[0]\n            cardinality = int(tuple_node_cardinality[1])\n            nodes.append(node)\n            nodes_cardinality.append(cardinality)\n        edges = []\n        for _ in range(num_edges):\n            edge = file.readline().strip().split(' ')\n            origin_node = edge[0]\n            target_node = edge[1]\n            edges.append((origin_node, target_node))\n        return num_nodes, num_edges, nodes, nodes_cardinality, edges\n\n```\n\n\n```python\n## utils/canonical_partitions/canonicalPartitions.py\nfrom utils.canonical_partitions.graph import Graph\n\ndef generateRelaxed(graph: Graph, latentCardinalities: list[int]):\n    relaxedGraph: str = \"\"\n    unob: str = \"\"\n    unobCardinalities: str = \", \".join(\n        map(lambda x: str(int(x)), latentCardinalities))\n\n    for index, component in enumerate(graph.dag_components):\n        currUnobLabel: str = \"U\" + str(index)\n        unob += f\", {currUnobLabel}\"\n        for node in component.nodes:\n            if graph.cardinalities[node] > 1:\n                relaxedGraph += (\n                    f\", {currUnobLabel} -> {graph.index_to_label[node]}\"\n                )\n\n    # adicionar arestas que saem do node depois, pois se nao tiver pai exogeno\n    # nao pertence a nenhum c-component\n    for index, label in graph.index_to_label.items():\n        if graph.cardinalities[index] > 1:\n            for node in graph.adj[index]:\n                relaxedGraph += f\", {label} -> {graph.index_to_label[node]}\"\n\n    return relaxedGraph[2:], unob[2:], unobCardinalities\n\n```\n\n\n",
        "eval_script": "# Consolidated Code\n\nclass Graph:\n    def __init__(\n        self, num_nodes: int, curr_nodes: list[int], visited: list[bool],\n        cardinalities: list[int], parents: list[int], adj: list[list[int]],\n        label_to_index: dict[str, int], index_to_label: dict[int, str],\n        dag_components: list[list[int]], exogenous: list[int],\n        endogenous: list[int]\n    ):\n        self.num_nodes = num_nodes\n        self.curr_nodes = curr_nodes\n        self.visited = visited\n        self.cardinalities = cardinalities\n        self.parents = parents\n        self.adj = adj\n        self.label_to_index = label_to_index\n        self.index_to_label = index_to_label\n        self.dag_components = dag_components\n        self.endogenous = endogenous\n        self.exogenous = exogenous\n\n    @staticmethod\n    def parse(predefined_data=None, input_path=None):\n        if predefined_data:\n            num_nodes = predefined_data['num_nodes']\n            num_edges = predefined_data['num_edges']\n        elif input_path:\n            num_nodes, num_edges, nodes, nodes_cardinality, edges = file_parser(input_path)\n        else:\n            num_nodes = int(input())\n            num_edges = int(input())\n\n        label_to_index_ex: dict[str, int] = {}\n        index_to_label_ex: dict[int, str] = {}\n        adj_ex = [[] for _ in range(num_nodes + 1)]\n        cardinalities_ex = [0] * (num_nodes + 1)\n        visited_ex = [False] * (num_nodes + 1)\n        parents_ex = [[] for _ in range(num_nodes + 1)]\n        endogenIndex: list[int] = []\n        exogenIndex: list[int] = []\n\n        for i in range(1, num_nodes + 1):\n            if predefined_data:\n                label, cardinality = predefined_data['nodes'][i - 1].split()\n            elif input_path:\n                label, cardinality = nodes[i-1], nodes_cardinality[i-1]\n            else:\n                label, cardinality = input().split()\n            cardinality = int(cardinality)\n            label_to_index_ex[label] = i\n            index_to_label_ex[i] = label\n            cardinalities_ex[i] = cardinality\n\n        for i in range(num_edges):\n            if predefined_data:\n                u, v = predefined_data['edges'][i].split(\" -> \")\n            elif (input_path):\n                print(edges[i])\n                u, v = edges[i]\n            else:\n                u, v = input().split()\n            u_index = label_to_index_ex[u]\n            v_index = label_to_index_ex[v]\n            adj_ex[u_index].append(v_index)\n            parents_ex[v_index].append(u_index)\n\n        for i in range(1, num_nodes + 1):\n\n            if not (bool(parents_ex[i])):\n                exogenIndex.append(i)\n            else:\n                endogenIndex.append(i)\n\n        return Graph(\n            num_nodes=num_nodes,\n            curr_nodes=[],\n            visited=visited_ex,\n            cardinalities=cardinalities_ex,\n            parents=parents_ex,\n            adj=adj_ex,\n            index_to_label=index_to_label_ex,\n            label_to_index=label_to_index_ex,\n            dag_components=[],\n            exogenous=exogenIndex,\n            endogenous=endogenIndex\n        )\n\n\ndef file_parser(input_path):\n    with open(input_path, 'r') as file:\n        num_nodes = int(file.readline().strip())\n        num_edges = int(file.readline().strip())\n        nodes = []\n        nodes_cardinality = []\n\n        for _ in range(num_nodes):\n            tuple_node_cardinality = file.readline().strip().split(' ')\n            node = tuple_node_cardinality[0]\n            cardinality = int(tuple_node_cardinality[1])\n            nodes.append(node)\n            nodes_cardinality.append(cardinality)\n        edges = []\n        for _ in range(num_edges):\n            edge = file.readline().strip().split(' ')\n            origin_node = edge[0]\n            target_node = edge[1]\n            edges.append((origin_node, target_node))\n        return num_nodes, num_edges, nodes, nodes_cardinality, edges\n\ndef generateRelaxed(graph: Graph, latentCardinalities: list[int]):\n    relaxedGraph: str = \"\"\n    unob: str = \"\"\n    unobCardinalities: str = \", \".join(\n        map(lambda x: str(int(x)), latentCardinalities))\n\n    for index, component in enumerate(graph.dag_components):\n        currUnobLabel: str = \"U\" + str(index)\n        unob += f\", {currUnobLabel}\"\n        for node in component.nodes:\n            if graph.cardinalities[node] > 1:\n                relaxedGraph += (\n                    f\", {currUnobLabel} -> {graph.index_to_label[node]}\"\n                )\n\n    # adicionar arestas que saem do node depois, pois se nao tiver pai exogeno\n    # nao pertence a nenhum c-component\n    for index, label in graph.index_to_label.items():\n        if graph.cardinalities[index] > 1:\n            for node in graph.adj[index]:\n                relaxedGraph += f\", {label} -> {graph.index_to_label[node]}\"\n\n    return relaxedGraph[2:], unob[2:], unobCardinalities\n\n# Mock new implementation for testing\n\n\ndef test_generateRelaxed():\n    # Test case 1\n    predefined_data_1 = {\n        'num_nodes': 3,\n        'num_edges': 2,\n        'nodes': ['A 2', 'B 1', 'C 3'],\n        'edges': ['A -> B', 'C -> A']\n    }\n    graph_1 = Graph.parse(predefined_data=predefined_data_1)\n    latentCardinalities_1 = [2, 3]\n    result_original_1 = generateRelaxed(graph_1, latentCardinalities_1)\n    result_new_1 = generateRelaxed_new_implementation(graph_1, latentCardinalities_1)\n    assert result_original_1 == result_new_1\n\n    # Test case 2\n    predefined_data_2 = {\n        'num_nodes': 2,\n        'num_edges': 1,\n        'nodes': ['X 3', 'Y 2'],\n        'edges': ['X -> Y']\n    }\n    graph_2 = Graph.parse(predefined_data=predefined_data_2)\n    latentCardinalities_2 = [1, 4]\n    result_original_2 = generateRelaxed(graph_2, latentCardinalities_2)\n    result_new_2 = generateRelaxed_new_implementation(graph_2, latentCardinalities_2)\n    assert result_original_2 == result_new_2\n\n    # Test case 3\n    predefined_data_3 = {\n        'num_nodes': 4,\n        'num_edges': 3,\n        'nodes': ['P 2', 'Q 2', 'R 1', 'S 3'],\n        'edges': ['P -> Q', 'Q -> S', 'R -> S']\n    }\n    graph_3 = Graph.parse(predefined_data=predefined_data_3)\n    latentCardinalities_3 = [3, 2]\n    result_original_3 = generateRelaxed(graph_3, latentCardinalities_3)\n    result_new_3 = generateRelaxed_new_implementation(graph_3, latentCardinalities_3)\n    assert result_original_3 == result_new_3\n\nif __name__ == \"__main__\":\n    test_generateRelaxed()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "get_valid_unobservable",
        "idx": "156",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/validator.py",
        "orig_func": "def get_valid_unobservable(var: str, edges: str) -> str:\n    if not is_valid_string(var):\n        raise InvalidVariableError(f\"Invalid variable: '{var}'.\")\n    if var == '-':\n        return None\n    if var.upper() not in edges:\n        raise InvalidVariableError(f\"Invalid variable: '{var}'. Not present in the edges.\")\n    return var.upper()",
        "orig_context": "```python\n## utils/validator.py\ndef is_valid_string(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str):\n        return False\n    if s.strip() == \"\":\n        return False\n    return True\n\nclass InvalidVariableError(Exception):\n    pass\n\ndef get_valid_unobservable(var: str, edges: str) -> str:\n    if not is_valid_string(var):\n        raise InvalidVariableError(f\"Invalid variable: '{var}'.\")\n    if var == \"-\":\n        return None\n    if var.upper() not in edges:\n        raise InvalidVariableError(f\"Invalid variable: '{var}'. Not present in the edges.\")\n    return var.upper()\n\n```\n\n\n",
        "eval_script": "def is_valid_string(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str):\n        return False\n    if s.strip() == \"\":\n        return False\n    return True\n\nclass InvalidVariableError(Exception):\n    pass\n\ndef get_valid_unobservable(var: str, edges: str) -> str:\n    if not is_valid_string(var):\n        raise InvalidVariableError(f\"Invalid variable: '{var}'.\")\n    if var == \"-\":\n        return None\n    if var.upper() not in edges:\n        raise InvalidVariableError(f\"Invalid variable: '{var}'. Not present in the edges.\")\n    return var.upper()\n\n# Assuming this function is provided elsewhere in the codebase\n\n\ndef test_get_valid_unobservable():\n    # Test cases\n    assert get_valid_unobservable(\"test\", \"TEST,ANOTHER\") == get_valid_unobservable_new_implementation(\"test\", \"TEST,ANOTHER\")\n    assert get_valid_unobservable(\"-\", \"ANY\") == get_valid_unobservable_new_implementation(\"-\", \"ANY\")\n    \n    try:\n        get_valid_unobservable(\"invalid\", \"TEST,ANOTHER\")\n    except InvalidVariableError as e:\n        assert True\n    else:\n        assert False\n    \n    try:\n        get_valid_unobservable_new_implementation(\"invalid\", \"TEST,ANOTHER\")\n    except InvalidVariableError as e:\n        assert True\n    else:\n        assert False\n\nif __name__ == \"__main__\":\n    test_get_valid_unobservable()"
    },
    {
        "func_name": "generate_twin_network",
        "idx": "157",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/file_generators/lcn_file_generator.py",
        "orig_func": "def generate_twin_network(dag, intervention, unob):\n    twin_sentences = []\n    (observed_var, intervened_var, value) = intervention\n    twin_var_intervened = f'{intervened_var}L'\n    twin_var_observed = f'{observed_var}L'\n    modified_edges = {child: [parent for parent in parents if child != intervened_var] for (child, parents) in dag.items()}\n    reachable_nodes = find_connected_nodes(modified_edges, observed_var)\n    twin_sentences.append(f\"1 <= P({('!' if value == 0 else '')}{twin_var_intervened}) <= 1\")\n    for var in reachable_nodes:\n        if var == intervened_var:\n            continue\n        if var in dag and var not in unob:\n            twin_mechanisms = generate_mechanism(var, dag[var], unob, twin=True)\n            twin_sentences.extend(twin_mechanisms)\n    return twin_sentences",
        "orig_context": "```python\n## utils/file_generators/lcn_file_generator.py\nfrom itertools import product\n\ndef generate_mechanism(var, parents, unob, twin=False):\n    mechanisms = []\n    endogenous_parents = [p for p in parents if p not in unob]\n    exogenous_parents = [p for p in parents if p in unob]\n    print(f\"var: {var}\")\n    print(f\"dag[var]: {parents}\")\n    print(f\"endogenous_parents: {endogenous_parents}\")\n    print(f\"exogenous_parents: {exogenous_parents}\")\n\n    if exogenous_parents:\n        conditions = list(product([0, 1], repeat=len(endogenous_parents) + len(exogenous_parents)))\n        exo_value_index = 0\n        for condition in conditions:\n            endo_condition = condition[len(exogenous_parents):]\n            exo_condition = condition[:len(exogenous_parents)]\n\n            endo_str = \" and \".join(f\"!{p}L\" if c == 0 else f\"{p}L\" for p, c in zip(endogenous_parents, endo_condition)) if twin else \" and \".join(f\"!{p}\" if c == 0 else p for p, c in zip(endogenous_parents, endo_condition))\n            exo_str = \" and \".join(f\"!{p}\" if c == 0 else p for p, c in zip(exogenous_parents, exo_condition))\n            cond_str = f\"{exo_str} and {endo_str}\" if endo_str and exo_str else (endo_str or exo_str)\n            \n            var_value = exo_condition[exo_value_index]\n            mech_str = f\"{var_value} <= P({var}L | {cond_str}) <= {var_value}\" if twin else f\"{var_value} <= P({var} | {cond_str}) <= {var_value}\"\n            mechanisms.append(mech_str)\n\n            exo_value_index = (exo_value_index + 1) % len(exogenous_parents)\n\n    return mechanisms\n\ndef find_connected_nodes(graph, target):\n        reachable = set()\n        stack = [target]\n        while stack:\n            node = stack.pop()\n            if node not in reachable:\n                reachable.add(node)\n                stack.extend([n for n in graph.get(node, []) if n not in reachable])\n        return reachable\n\ndef generate_twin_network(dag, intervention, unob):\n    twin_sentences = []\n    observed_var, intervened_var, value = intervention\n    twin_var_intervened = f\"{intervened_var}L\"\n    twin_var_observed = f\"{observed_var}L\"\n\n    modified_edges = {child: [parent for parent in parents if child != intervened_var] for child, parents in dag.items()}\n    reachable_nodes = find_connected_nodes(modified_edges, observed_var)\n\n    twin_sentences.append(f\"1 <= P({'!' if value == 0 else ''}{twin_var_intervened}) <= 1\")\n    for var in reachable_nodes:\n        if var == intervened_var:\n            continue  # Skip intervened variable\n        if var in dag and var not in unob:\n            twin_mechanisms = generate_mechanism(var, dag[var], unob, twin=True)\n            twin_sentences.extend(twin_mechanisms)\n\n    return twin_sentences\n\n```\n\n\n",
        "eval_script": "## utils/file_generators/lcn_file_generator.py\nfrom itertools import product\n\ndef generate_mechanism(var, parents, unob, twin=False):\n    mechanisms = []\n    endogenous_parents = [p for p in parents if p not in unob]\n    exogenous_parents = [p for p in parents if p in unob]\n    print(f\"var: {var}\")\n    print(f\"dag[var]: {parents}\")\n    print(f\"endogenous_parents: {endogenous_parents}\")\n    print(f\"exogenous_parents: {exogenous_parents}\")\n\n    if exogenous_parents:\n        conditions = list(product([0, 1], repeat=len(endogenous_parents) + len(exogenous_parents)))\n        exo_value_index = 0\n        for condition in conditions:\n            endo_condition = condition[len(exogenous_parents):]\n            exo_condition = condition[:len(exogenous_parents)]\n\n            endo_str = \" and \".join(\n                f\"!{p}L\" if c == 0 else f\"{p}L\"\n                for p, c in zip(endogenous_parents, endo_condition)\n            ) if twin else \" and \".join(f\"!{p}\" if c == 0 else p for p, c in zip(endogenous_parents, endo_condition))\n            exo_str = \" and \".join(f\"!{p}\" if c == 0 else p for p, c in zip(exogenous_parents, exo_condition))\n            cond_str = f\"{exo_str} and {endo_str}\" if endo_str and exo_str else (endo_str or exo_str)\n\n            var_value = exo_condition[exo_value_index]\n            mech_str = f\"{var_value} <= P({var}L | {cond_str}) <= {var_value}\" if twin else f\"{var_value} <= P({var} | {cond_str}) <= {var_value}\"\n            mechanisms.append(mech_str)\n\n            exo_value_index = (exo_value_index + 1) % len(exogenous_parents)\n\n    return mechanisms\n\ndef find_connected_nodes(graph, target):\n        reachable = set()\n        stack = [target]\n        while stack:\n            node = stack.pop()\n            if node not in reachable:\n                reachable.add(node)\n                stack.extend([n for n in graph.get(node, []) if n not in reachable])\n        return reachable\n\ndef generate_twin_network(dag, intervention, unob):\n    twin_sentences = []\n    observed_var, intervened_var, value = intervention\n    twin_var_intervened = f\"{intervened_var}L\"\n    twin_var_observed = f\"{observed_var}L\"\n\n    modified_edges = {child: [parent for parent in parents if child != intervened_var] for child, parents in dag.items()}\n    reachable_nodes = find_connected_nodes(modified_edges, observed_var)\n\n    twin_sentences.append(f\"1 <= P({'!' if value == 0 else ''}{twin_var_intervened}) <= 1\")\n    for var in reachable_nodes:\n        if var == intervened_var:\n            continue  # Skip intervened variable\n        if var in dag and var not in unob:\n            twin_mechanisms = generate_mechanism(var, dag[var], unob, twin=True)\n            twin_sentences.extend(twin_mechanisms)\n\n    return twin_sentences\n\n\n\ndef test_generate_twin_network():\n    dag1 = {'X': ['U'], 'Y': ['X']}\n    intervention1 = ('Y', 'X', 0)\n    unob1 = ['U']\n    assert generate_twin_network(dag1, intervention1, unob1) == generate_twin_network_new_implementation(dag1, intervention1, unob1)\n\n    dag2 = {'A': ['B', 'C'], 'B': [], 'C': []}\n    intervention2 = ('A', 'B', 1)\n    unob2 = []\n    assert generate_twin_network(dag2, intervention2, unob2) == generate_twin_network_new_implementation(dag2, intervention2, unob2)\n\n    dag3 = {'V': ['W'], 'W': ['U'], 'U': []}\n    intervention3 = ('V', 'W', 1)\n    unob3 = ['U']\n    assert generate_twin_network(dag3, intervention3, unob3) == generate_twin_network_new_implementation(dag3, intervention3, unob3)\n\nif __name__ == \"__main__\":\n    test_generate_twin_network()"
    },
    {
        "func_name": "get_valid_edges_in_string",
        "idx": "158",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/validator.py",
        "orig_func": "def get_valid_edges_in_string(edges_str: str) -> Tuple[str, List[Tuple[str, str]]]:\n    if not is_valid_string(edges_str):\n        raise InvalidEdgeFormatError(f\"Invalid edges format: '{edges_str}'.\")\n    edges_str = edges_str.upper()\n    edges_list = get_valid_edge_tuple_list(edges_str)\n    return (edges_str, edges_list)",
        "orig_context": "```python\n## utils/validator.py\nfrom typing import Dict, List, Tuple\n\ndef is_valid_string(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str):\n        return False\n    if s.strip() == \"\":\n        return False\n    return True\n\nclass InvalidEdgeFormatError(Exception):\n    pass\n\ndef get_valid_edge_tuple_list(edges_str: str) -> List[Tuple[str, str]]:\n    edges = []\n    for edge in edges_str.split(', '):\n        nodes = edge.split(' -> ')\n        if len(nodes) != 2 or not nodes[0].strip() or not nodes[1].strip():\n            raise InvalidEdgeFormatError(f\"Invalid format for edge: '{edge}'\")\n        edges.append((nodes[0].strip(), nodes[1].strip()))\n    return edges\n\ndef get_valid_edges_in_string(edges_str: str) -> Tuple[str, List[Tuple[str, str]]]:\n    if not is_valid_string(edges_str):\n        raise InvalidEdgeFormatError(f\"Invalid edges format: '{edges_str}'.\")\n    edges_str = edges_str.upper()\n    edges_list = get_valid_edge_tuple_list(edges_str)\n    return edges_str, edges_list\n\n```\n\n\n",
        "eval_script": "## utils/validator.py\nfrom typing import Dict, List, Tuple\n\ndef is_valid_string(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str):\n        return False\n    if s.strip() == \"\":\n        return False\n    return True\n\nclass InvalidEdgeFormatError(Exception):\n    pass\n\ndef get_valid_edge_tuple_list(edges_str: str) -> List[Tuple[str, str]]:\n    edges = []\n    for edge in edges_str.split(', '):\n        nodes = edge.split(' -> ')\n        if len(nodes) != 2 or not nodes[0].strip() or not nodes[1].strip():\n            raise InvalidEdgeFormatError(f\"Invalid format for edge: '{edge}'\")\n        edges.append((nodes[0].strip(), nodes[1].strip()))\n    return edges\n\ndef get_valid_edges_in_string(edges_str: str) -> Tuple[str, List[Tuple[str, str]]]:\n    if not is_valid_string(edges_str):\n        raise InvalidEdgeFormatError(f\"Invalid edges format: '{edges_str}'.\")\n    edges_str = edges_str.upper()\n    edges_list = get_valid_edge_tuple_list(edges_str)\n    return edges_str, edges_list\n\n\n\ndef test_get_valid_edges_in_string():\n    # Edge case: Normal case\n    input_data = \"A -> B, C -> D\"\n    assert get_valid_edges_in_string(input_data) == get_valid_edges_in_string_new_implementation(input_data)\n\n    # Edge case: Lowercase and extra spaces\n    input_data = \" a -> b, c -> d \"\n    assert get_valid_edges_in_string(input_data) == get_valid_edges_in_string_new_implementation(input_data)\n\n    # Edge case: Invalid format\n    input_data = \"A - B, C -> D\"\n    try:\n        get_valid_edges_in_string(input_data)\n    except InvalidEdgeFormatError:\n        pass\n    else:\n        assert False, \"Expected InvalidEdgeFormatError for get_valid_edges_in_string\"\n\n    try:\n        get_valid_edges_in_string_new_implementation(input_data)\n    except InvalidEdgeFormatError:\n        pass\n    else:\n        assert False, \"Expected InvalidEdgeFormatError for get_valid_edges_in_string_new_implementation\"\n\nif __name__ == \"__main__\":\n    test_get_valid_edges_in_string()"
    },
    {
        "func_name": "get_valid_edge_tuple_list",
        "idx": "159",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/validator.py",
        "orig_func": "def get_valid_edge_tuple_list(edges_str: str) -> List[Tuple[str, str]]:\n    edges = []\n    for edge in edges_str.split(', '):\n        nodes = edge.split(' -> ')\n        if len(nodes) != 2 or not nodes[0].strip() or (not nodes[1].strip()):\n            raise InvalidEdgeFormatError(f\"Invalid format for edge: '{edge}'\")\n        edges.append((nodes[0].strip(), nodes[1].strip()))\n    return edges",
        "orig_context": "```python\n## utils/validator.py\nfrom typing import Dict, List, Tuple\n\nclass InvalidEdgeFormatError(Exception):\n    pass\n\ndef get_valid_edge_tuple_list(edges_str: str) -> List[Tuple[str, str]]:\n    edges = []\n    for edge in edges_str.split(', '):\n        nodes = edge.split(' -> ')\n        if len(nodes) != 2 or not nodes[0].strip() or not nodes[1].strip():\n            raise InvalidEdgeFormatError(f\"Invalid format for edge: '{edge}'\")\n        edges.append((nodes[0].strip(), nodes[1].strip()))\n    return edges\n\n```\n\n\n",
        "eval_script": "## utils/validator.py\nfrom typing import List, Tuple\n\nclass InvalidEdgeFormatError(Exception):\n    pass\n\ndef get_valid_edge_tuple_list(edges_str: str) -> List[Tuple[str, str]]:\n    edges = []\n    for edge in edges_str.split(', '):\n        nodes = edge.split(' -> ')\n        if len(nodes) != 2 or not nodes[0].strip() or not nodes[1].strip():\n            raise InvalidEdgeFormatError(f\"Invalid format for edge: '{edge}'\")\n        edges.append((nodes[0].strip(), nodes[1].strip()))\n    return edges\n\n\n\ndef test_get_valid_edge_tuple_list():\n    # Test Case 1: Normal Input\n    input_str = \"A -> B, C -> D\"\n    expected_output = get_valid_edge_tuple_list(input_str)\n    assert get_valid_edge_tuple_list_new_implementation(input_str) == expected_output\n\n    # Test Case 2: Invalid Input that should raise an exception\n    invalid_input_str = \"A ->, C -> D\"\n    try:\n        get_valid_edge_tuple_list(invalid_input_str)\n    except InvalidEdgeFormatError:\n        pass\n    else:\n        raise AssertionError(\"get_valid_edge_tuple_list did not raise InvalidEdgeFormatError\")\n\n    try:\n        get_valid_edge_tuple_list_new_implementation(invalid_input_str)\n    except InvalidEdgeFormatError:\n        pass\n    else:\n        raise AssertionError(\"get_valid_edge_tuple_list_new_implementation did not raise InvalidEdgeFormatError\")\n\n    # Test Case 3: Edge Case with Extra Spaces\n    input_str_with_spaces = \" A  -> B ,  C ->  D \"\n    expected_output_with_spaces = get_valid_edge_tuple_list(input_str_with_spaces)\n    assert get_valid_edge_tuple_list_new_implementation(input_str_with_spaces) == expected_output_with_spaces\n\nif __name__ == \"__main__\":\n    test_get_valid_edge_tuple_list()"
    },
    {
        "func_name": "parse_input",
        "idx": "160",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/file_generators/lcn_file_generator.py",
        "orig_func": "def parse_input(edges_str, unob_str):\n    edges = edges_str.split(', ')\n    unob = unob_str.split(', ') if unob_str else []\n    dag = {}\n    children = {}\n    for edge in edges:\n        (parent, child) = edge.split(' -> ')\n        if child in dag:\n            dag[child].append(parent)\n        else:\n            dag[child] = [parent]\n        if parent not in dag:\n            dag[parent] = []\n        if parent not in children:\n            children[parent] = []\n        children[parent].append(child)\n    print('Parsed DAG:', dag)\n    return (dag, unob, children)",
        "orig_context": "```python\n## utils/file_generators/lcn_file_generator.py\ndef parse_input(edges_str, unob_str):\n    edges = edges_str.split(\", \")\n    unob = unob_str.split(\", \") if unob_str else []\n\n    dag = {}\n    children = {}\n\n    for edge in edges:\n        parent, child = edge.split(\" -> \")\n        if child in dag:\n            dag[child].append(parent)\n        else:\n            dag[child] = [parent]\n        if parent not in dag:\n            dag[parent] = []\n        if parent not in children:\n            children[parent] = []\n        children[parent].append(child)\n\n    print(\"Parsed DAG:\", dag)\n    return dag, unob, children\n\n```\n\n\n",
        "eval_script": "## utils/file_generators/lcn_file_generator.py\ndef parse_input(edges_str, unob_str):\n    edges = edges_str.split(\", \")\n    unob = unob_str.split(\", \") if unob_str else []\n\n    dag = {}\n    children = {}\n\n    for edge in edges:\n        parent, child = edge.split(\" -> \")\n        if child in dag:\n            dag[child].append(parent)\n        else:\n            dag[child] = [parent]\n        if parent not in dag:\n            dag[parent] = []\n        if parent not in children:\n            children[parent] = []\n        children[parent].append(child)\n\n    print(\"Parsed DAG:\", dag)\n    return dag, unob, children\n\n\n\ndef test_parse_input():\n    # Test Case 1\n    edges_str1 = \"A -> B, B -> C\"\n    unob_str1 = \"\"\n    assert parse_input(edges_str1, unob_str1) == parse_input_new_implementation(edges_str1, unob_str1), \"Test Case 1 Failed\"\n\n    # Test Case 2\n    edges_str2 = \"X -> Y, Y -> Z\"\n    unob_str2 = \"W, V\"\n    assert parse_input(edges_str2, unob_str2) == parse_input_new_implementation(edges_str2, unob_str2), \"Test Case 2 Failed\"\n\n    # Test Case 3\n    edges_str3 = \"M -> N\"\n    unob_str3 = \"\"\n    assert parse_input(edges_str3, unob_str3) == parse_input_new_implementation(edges_str3, unob_str3), \"Test Case 3 Failed\"\n\nif __name__ == \"__main__\":\n    test_parse_input()"
    },
    {
        "func_name": "generate_empirical_distributions",
        "idx": "162",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/file_generators/lcn_file_generator.py",
        "orig_func": "def generate_empirical_distributions(empirical_distributions, var_order):\n    empirical_sentences = []\n    for (i, (config, prob)) in enumerate(empirical_distributions):\n        cond_str = ' and '.join((f\"{('!' if c == 0 else '')}{var_order[j]}\" for (j, c) in enumerate(config)))\n        empirical_sentences.append(f'{prob} <= P({cond_str}) <= {prob} ; False')\n    return empirical_sentences",
        "orig_context": "```python\n## utils/file_generators/lcn_file_generator.py\ndef generate_empirical_distributions(empirical_distributions, var_order):\n    empirical_sentences = []\n    for i, (config, prob) in enumerate(empirical_distributions):\n        cond_str = \" and \".join(f\"{'!' if c == 0 else ''}{var_order[j]}\" for j, c in enumerate(config))\n        empirical_sentences.append(f\"{prob} <= P({cond_str}) <= {prob} ; False\")\n    return empirical_sentences\n\n```\n\n\n",
        "eval_script": "## utils/file_generators/lcn_file_generator.py\ndef generate_empirical_distributions(empirical_distributions, var_order):\n    empirical_sentences = []\n    for i, (config, prob) in enumerate(empirical_distributions):\n        cond_str = \" and \".join(f\"{'!' if c == 0 else ''}{var_order[j]}\" for j, c in enumerate(config))\n        empirical_sentences.append(f\"{prob} <= P({cond_str}) <= {prob} ; False\")\n    return empirical_sentences\n\n# Placeholder function for the new implementation. This should be replaced by the actual function.\n\n\ndef test_generate_empirical_distributions():\n    # Test case 1\n    empirical_distributions_1 = [([0, 1], 0.5), ([1, 0], 0.8)]\n    var_order_1 = ['X', 'Y']\n    assert generate_empirical_distributions(empirical_distributions_1, var_order_1) == generate_empirical_distributions_new_implementation(empirical_distributions_1, var_order_1)\n\n    # Test case 2\n    empirical_distributions_2 = [([1, 1, 0], 0.6)]\n    var_order_2 = ['A', 'B', 'C']\n    assert generate_empirical_distributions(empirical_distributions_2, var_order_2) == generate_empirical_distributions_new_implementation(empirical_distributions_2, var_order_2)\n\n    # Test case 3\n    empirical_distributions_3 = [([0, 0, 1, 1], 0.3), ([1, 1, 1, 0], 0.2)]\n    var_order_3 = ['D', 'E', 'F', 'G']\n    assert generate_empirical_distributions(empirical_distributions_3, var_order_3) == generate_empirical_distributions_new_implementation(empirical_distributions_3, var_order_3)\n\nif __name__ == '__main__':\n    test_generate_empirical_distributions()"
    },
    {
        "func_name": "OutputWriter.silent_run",
        "idx": "163",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/output_writer.py",
        "orig_func": "def silent_run(self, func, output_file=None, new=False):\n    \"\"\"Run a function and redirect output to a specified file.\n\n        Args:\n            func (function): The function to run.\n            output_file (str, optional): The file path to redirect output to.\n                Defaults to None.\n            new (bool, optional): Whether to write a new file or append to an\n                existing one. Defaults to False.\n        \"\"\"\n    mode = 'w' if new else 'a'\n    if output_file is None:\n        with open(self.output_path, mode) as f:\n            with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n                return func()\n    else:\n        with open(output_file, mode) as f:\n            with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n                return func()",
        "orig_context": "```python\n## utils/output_writer.py\nimport contextlib\n\nclass OutputWriter:\n    def __init__(self, output_path=\"outputs/DEFAULT_OUTPUT.txt\"):\n        self.output_path = output_path\n        self.reset()\n\n    def __call__(self, output, new=False):\n        \"\"\"\n        Write the output to the file.\n\n        Args:\n            output (str): The output to write to the file.\n            new (bool, optional): Whether to write a new file or append to an\n                existing one. Defaults to False\n        \"\"\"\n        mode = \"w\" if new else \"a\"\n        with open(self.output_path, mode) as f:\n            f.write(output + \"\\n\")\n\n    def reset(self) -> None:\n        \"\"\"Resets the output file by clearing its content.\"\"\"\n        try:\n            with open(self.output_path, 'w') as file:\n                file.write(\"\")\n        except IOError as e:\n            print(f\"Error resetting file {self.output_path}: {e}\")\n\n    def silent_run(self, func, output_file=None, new=False):\n        \"\"\"Run a function and redirect output to a specified file.\n\n        Args:\n            func (function): The function to run.\n            output_file (str, optional): The file path to redirect output to.\n                Defaults to None.\n            new (bool, optional): Whether to write a new file or append to an\n                existing one. Defaults to False.\n        \"\"\"\n        mode = \"w\" if new else \"a\"\n\n        if output_file is None:\n            with open(self.output_path, mode) as f:\n                with contextlib.redirect_stdout(f), \\\n                        contextlib.redirect_stderr(f):\n                    return func()\n\n        else:\n            with open(output_file, mode) as f:\n                with contextlib.redirect_stdout(f), \\\n                        contextlib.redirect_stderr(f):\n                    return func()\n\n```\n\n\n",
        "eval_script": "## utils/output_writer.py\nimport contextlib\nimport os\n\nclass OutputWriter:\n    def __init__(self, output_path=\"outputs/DEFAULT_OUTPUT.txt\"):\n        self.output_path = output_path\n        self.reset()\n\n    def __call__(self, output, new=False):\n        \"\"\"\n        Write the output to the file.\n\n        Args:\n            output (str): The output to write to the file.\n            new (bool, optional): Whether to write a new file or append to an\n                existing one. Defaults to False\n        \"\"\"\n        mode = \"w\" if new else \"a\"\n        with open(self.output_path, mode) as f:\n            f.write(output + \"\\n\")\n\n    def reset(self) -> None:\n        \"\"\"Resets the output file by clearing its content.\"\"\"\n        try:\n            with open(self.output_path, 'w') as file:\n                file.write(\"\")\n        except IOError as e:\n            print(f\"Error resetting file {self.output_path}: {e}\")\n\n    def silent_run(self, func, output_file=None, new=False):\n        \"\"\"Run a function and redirect output to a specified file.\n\n        Args:\n            func (function): The function to run.\n            output_file (str, optional): The file path to redirect output to.\n                Defaults to None.\n            new (bool, optional): Whether to write a new file or append to an\n                existing one. Defaults to False.\n        \"\"\"\n        mode = \"w\" if new else \"a\"\n\n        if output_file is None:\n            with open(self.output_path, mode) as f:\n                with contextlib.redirect_stdout(f), \\\n                        contextlib.redirect_stderr(f):\n                    return func()\n\n        else:\n            with open(output_file, mode) as f:\n                with contextlib.redirect_stdout(f), \\\n                        contextlib.redirect_stderr(f):\n                    return func()\n\n\n\ndef test_silent_run():\n    def test_function():\n        print(\"Hello, World!\")\n        print(\"This is a test.\")\n        return 42\n\n    output_writer = OutputWriter(output_path=\"/home/user/tmp/DEFAULT_OUTPUT.txt\")\n    test_output_path1 = \"/home/user/tmp/test_output1.txt\"\n    test_output_path2 = \"/home/user/tmp/test_output2.txt\"\n\n    # Run the original implementation\n    result1 = output_writer.silent_run(test_function, output_file=test_output_path1, new=True)\n\n    # Run the new implementation\n    result2 = output_writer.silent_run_new_implementation(test_function, output_file=test_output_path2, new=True)\n\n    # Asserts to check functionality\n    with open(test_output_path1, 'r') as f1, open(test_output_path2, 'r') as f2:\n        content1 = f1.read()\n        content2 = f2.read()\n\n    assert content1 == content2, \"Output content mismatch.\"\n    assert result1 == result2, \"Return value mismatch.\"\n    assert os.path.getsize(test_output_path1) == os.path.getsize(test_output_path2), \"File size mismatch.\"\n\nif __name__ == \"__main__\":\n    test_silent_run()"
    },
    {
        "func_name": "UAIParser.calculate_probability",
        "idx": "164",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/file_generators/parser_uai.py",
        "orig_func": "def calculate_probability(self, outcome: list[int]) -> float:\n    \"\"\"\n        Calculate the probability of a given outcome.\n\n        Args:\n            outcome (list[int]): list of variable values\n\n        Returns:\n            float: probability of the outcome\n        \"\"\"\n    probability = 1.0\n    for (i, value) in enumerate(outcome):\n        if len(self.parents[i]) == 0:\n            probability *= self.tables[i][value]\n        else:\n            parent_values = tuple((outcome[parent] for parent in self.parents[i]))\n            probability *= self.tables[i][parent_values + (value,)]\n    return probability",
        "orig_context": "```python\n## utils/file_generators/csv_generator.py\nimport csv\n\nfrom typing import List, Tuple\n\nimport pandas as pd\n\nArrayType = List[Tuple[List[int], float]]\n\ndef probsHelper(\n    header: str,\n    probCombinations: ArrayType,\n    filename: str = \"\",\n    csv_flag: bool = True\n) -> pd.DataFrame | None:\n    maxDecimals: int = 0\n    for _sublist, val in probCombinations:\n        probStr: str = str(val)\n        if \".\" in probStr:\n            maxDecimals = max(maxDecimals, len(probStr.split(\".\")[1]))\n\n    if csv_flag:\n        file_path = \"./csv_data_examples/\" + filename + \".csv\"\n        with open(file_path, mode=\"w\", newline=\"\") as file:\n            writer = csv.writer(file)\n            writer.writerow(header)\n            for sublist, val in probCombinations:\n                numberOfRows: int = (int)(val * pow(10, maxDecimals))\n                for _ in range(numberOfRows):\n                    writer.writerow(sublist)\n    else:\n        data: List[List[int]] = []\n        for sublist, val in probCombinations:\n            numberOfRows: int = (int)(val * pow(10, maxDecimals))\n            for _ in range(numberOfRows):\n                data.append(sublist)\n        df = pd.DataFrame(data, columns=header)\n        return df\n\n```\n\n\n```python\n## utils/file_generators/parser_uai.py\nimport itertools\n\nimport networkx as nx\n\nimport numpy as np\n\nfrom csv_generator import probsHelper\n\nfrom pandas import DataFrame\n\nclass UAIParser:\n    \"\"\"\n    A class to parse a UAI file and generate data from it.\n\n    Attributes:\n        filepath (str): path to the UAI file\n        nodes (list[str]): list of node names in order of appearance\n            in the UAI file\n        network_type: type of network\n        num_variables: number of variables\n        domain_sizes: list of domain sizes\n        parents: list of parents for each variable\n        tables: list of conditional probability tables\n        graph: networkx DiGraph object representing the network\n        data: pandas DataFrame with the generated data\n    \"\"\"\n\n    def __init__(self, filepath: str, nodes: list[str]) -> None:\n        \"\"\"\n        Initialize the parser with the file path and node names.\n\n        Args:\n            filepath (str): path to the UAI file\n            nodes (list[str]): list of node names in order of appearance\n                in the UAI file\n        \"\"\"\n        self.filepath = filepath\n        self.info = None\n        self.nodes = nodes\n        self.network_type = None\n        self.num_variables = None\n        self.domain_sizes = []\n        self.parents = []\n        self.tables = []\n        self.graph = nx.DiGraph()\n        self.data = []\n        self.index = 0\n\n    def parse(self) -> None:\n        \"\"\"\n        Parse the UAI file and store the network information.\n        \"\"\"\n        with open(self.filepath, 'r') as file:\n            self.info = file.read().replace('\\n', ' ').split()\n            self.index = 0\n\n            # Cabe\u00e7alho: tipo de rede\n            self.network_type = self.info[self.index]\n            self.index += 1\n\n            # N\u00famero de vari\u00e1veis\n            self.num_variables = int(self.info[self.index])\n            self.index += 1\n\n            # Tamanhos dos dom\u00ednios\n            self.domain_sizes = list(\n                map(int, self.info[2:2+self.num_variables]))\n            self.index += self.num_variables\n\n            # Lendo a lista de pais\n            num_factors = int(self.info[self.index])\n            self.index += 1\n            self.parents = []\n            for _ in range(num_factors):\n                num_parents = int(self.info[self.index]) - 1\n                self.index += 1\n                self.parents.append(\n                    list(map(int,\n                             self.info[self.index:self.index + num_parents])))\n                self.index += num_parents + 1\n\n            # Lendo as tabelas de mecanismos\n            self.tables = []\n            if self.network_type == 'BAYES':\n                self.index = self.bayes_parser()\n            elif self.network_type == 'CAUSAL':\n                self.index = self.causal_parser()\n\n        # Gerar o grafo\n        self.graph = nx.DiGraph()\n        for i in range(self.num_variables):\n            self.graph.add_node(i)\n        for i, parents in enumerate(self.parents):\n            for parent in parents:\n                self.graph.add_edge(parent, i)\n\n        for i, title in enumerate(self.nodes):\n            nx.relabel_nodes(self.graph, {i: title}, copy=False)\n\n    def bayes_parser(self) -> None:\n        \"\"\"Parse the network for a Bayesian network.\"\"\"\n        for i in range(self.num_variables):\n            # Quantidade de entradas na tabela\n            num_entries = int(self.info[self.index])\n            self.index += 1\n            flat_table = list(\n                map(float, self.info[self.index:self.index + num_entries])\n            )\n            self.index += num_entries\n\n            # Obtenha o n\u00famero de dimens\u00f5es da tabela (pais + n\u00f3)\n            dims = [\n                self.domain_sizes[parent] for parent in self.parents[i]\n            ] + [self.domain_sizes[i]]\n\n            # Reformatar a tabela plana em uma matriz multidimensional\n            table = np.array(flat_table).reshape(dims)\n            self.tables.append(table)\n\n    def causal_parser(self) -> None:\n        \"\"\"Parse the network for a Causal network.\"\"\"\n        for i in range(self.num_variables):\n            # Quantidade de entradas na tabela\n            num_entries = int(self.info[self.index])\n            self.index += 1\n            flat_table = []\n            for value in self.info[self.index:self.index + num_entries]:\n                if \".\" in value:\n                    flat_table.append(float(value))\n                else:\n                    one_hot = [0] * self.domain_sizes[i]\n                    one_hot[int(value)] = 1\n                    flat_table.extend(one_hot)\n            self.index += num_entries\n\n            # Obtenha o n\u00famero de dimens\u00f5es da tabela (pais + n\u00f3)\n            dims = [self.domain_sizes[parent]\n                    for parent in self.parents[i]] + [self.domain_sizes[i]]\n\n            # Reformatar a tabela plana em uma matriz multidimensional\n            table = np.array(flat_table).reshape(dims)\n            self.tables.append(table)\n\n    def calculate_probability(self, outcome: list[int]) -> float:\n        \"\"\"\n        Calculate the probability of a given outcome.\n\n        Args:\n            outcome (list[int]): list of variable values\n\n        Returns:\n            float: probability of the outcome\n        \"\"\"\n        probability = 1.0\n\n        for i, value in enumerate(outcome):\n            if len(self.parents[i]) == 0:\n                probability *= self.tables[i][value]\n            else:\n                parent_values = tuple(outcome[parent]\n                                      for parent in self.parents[i])\n                probability *= self.tables[i][parent_values + (value,)]\n\n        return probability\n\n    def calculate_probabilities_for_outcomes(\n        self, outcomes: list[list[int]]\n    ) -> list[float]:\n        \"\"\"\n        Calculate the probabilities of a list of outcomes.\n\n        Args:\n            outcomes (list[list[int]]): list of outcomes\n\n        Returns:\n            list[float]: list of probabilities\n        \"\"\"\n        probabilities = []\n        for outcome in outcomes:\n            prob = self.calculate_probability(outcome)\n            probabilities.append(prob)\n        return probabilities\n\n    def generate_data(self) -> DataFrame:\n        \"\"\"\n        Generate data from the network.\n\n        Returns:\n            DataFrame: DataFrame with the generated data\n        \"\"\"\n        values = [list(range(size)) for size in self.domain_sizes]\n        outcomes = list(itertools.product(*values))\n        probs = self.calculate_probabilities_for_outcomes(outcomes)\n        probabilities = [[sublist, val]\n                         for sublist, val in zip(outcomes, probs)]\n        self.data = probsHelper(self.nodes, probabilities, csv_flag=False)\n        return self.data\n\n    def display(self) -> None:\n        \"\"\"\n        Display the network information.\n        \"\"\"\n        print(f\"Tipo de rede: {self.network_type}\")\n        print(f\"N\u00famero de vari\u00e1veis: {self.num_variables}\")\n        print(f\"Tamanhos dos dom\u00ednios: {self.domain_sizes}\")\n        print(\"Pais das vari\u00e1veis:\")\n        for parents in self.parents:\n            print(f\"  {parents}\")\n        print(\"Tabelas de probabilidades condicionais:\")\n        for table in self.tables:\n            print(table, end=\"\\n\\n\\n\")\n\n```\n\n\n",
        "eval_script": "# utils/file_generators/parser_uai.py\nimport itertools\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple\n\n# Defining the probsHelper function here as described in the context\nArrayType = List[Tuple[List[int], float]]\n\ndef probsHelper(\n    header: str,\n    probCombinations: ArrayType,\n    filename: str = \"\",\n    csv_flag: bool = True\n) -> pd.DataFrame | None:\n    maxDecimals: int = 0\n    for _sublist, val in probCombinations:\n        probStr: str = str(val)\n        if \".\" in probStr:\n            maxDecimals = max(maxDecimals, len(probStr.split(\".\")[1]))\n\n    if csv_flag:\n        file_path = \"./csv_data_examples/\" + filename + \".csv\"\n        with open(file_path, mode=\"w\", newline=\"\") as file:\n            writer = csv.writer(file)\n            writer.writerow(header)\n            for sublist, val in probCombinations:\n                numberOfRows: int = (int)(val * pow(10, maxDecimals))\n                for _ in range(numberOfRows):\n                    writer.writerow(sublist)\n    else:\n        data: List[List[int]] = []\n        for sublist, val in probCombinations:\n            numberOfRows: int = (int)(val * pow(10, maxDecimals))\n            for _ in range(numberOfRows):\n                data.append(sublist)\n        df = pd.DataFrame(data, columns=header)\n        return df\n\nclass UAIParser:\n    \"\"\"\n    A class to parse a UAI file and generate data from it.\n\n    Attributes:\n        filepath (str): path to the UAI file\n        nodes (list[str]): list of node names in order of appearance\n            in the UAI file\n        network_type: type of network\n        num_variables: number of variables\n        domain_sizes: list of domain sizes\n        parents: list of parents for each variable\n        tables: list of conditional probability tables\n        graph: networkx DiGraph object representing the network\n        data: pandas DataFrame with the generated data\n    \"\"\"\n\n    def __init__(self, filepath: str, nodes: list[str]) -> None:\n        \"\"\"\n        Initialize the parser with the file path and node names.\n\n        Args:\n            filepath (str): path to the UAI file\n            nodes (list[str]): list of node names in order of appearance\n                in the UAI file\n        \"\"\"\n        self.filepath = filepath\n        self.info = None\n        self.nodes = nodes\n        self.network_type = None\n        self.num_variables = None\n        self.domain_sizes = []\n        self.parents = []\n        self.tables = []\n        self.graph = nx.DiGraph()\n        self.data = []\n        self.index = 0\n\n    def parse(self) -> None:\n        \"\"\"\n        Parse the UAI file and store the network information.\n        \"\"\"\n        with open(self.filepath, 'r') as file:\n            self.info = file.read().replace('\\n', ' ').split()\n            self.index = 0\n\n            # Cabe\u00e7alho: tipo de rede\n            self.network_type = self.info[self.index]\n            self.index += 1\n\n            # N\u00famero de vari\u00e1veis\n            self.num_variables = int(self.info[self.index])\n            self.index += 1\n\n            # Tamanhos dos dom\u00ednios\n            self.domain_sizes = list(\n                map(int, self.info[2:2+self.num_variables]))\n            self.index += self.num_variables\n\n            # Lendo a lista de pais\n            num_factors = int(self.info[self.index])\n            self.index += 1\n            self.parents = []\n            for _ in range(num_factors):\n                num_parents = int(self.info[self.index]) - 1\n                self.index += 1\n                self.parents.append(\n                    list(map(int,\n                             self.info[self.index:self.index + num_parents])))\n                self.index += num_parents + 1\n\n            # Lendo as tabelas de mecanismos\n            self.tables = []\n            if self.network_type == 'BAYES':\n                self.index = self.bayes_parser()\n            elif self.network_type == 'CAUSAL':\n                self.index = self.causal_parser()\n\n        # Gerar o grafo\n        self.graph = nx.DiGraph()\n        for i in range(self.num_variables):\n            self.graph.add_node(i)\n        for i, parents in enumerate(self.parents):\n            for parent in parents:\n                self.graph.add_edge(parent, i)\n\n        for i, title in enumerate(self.nodes):\n            nx.relabel_nodes(self.graph, {i: title}, copy=False)\n\n    def bayes_parser(self) -> None:\n        \"\"\"Parse the network for a Bayesian network.\"\"\"\n        for i in range(self.num_variables):\n            # Quantidade de entradas na tabela\n            num_entries = int(self.info[self.index])\n            self.index += 1\n            flat_table = list(\n                map(float, self.info[self.index:self.index + num_entries])\n            )\n            self.index += num_entries\n\n            # Obtenha o n\u00famero de dimens\u00f5es da tabela (pais + n\u00f3)\n            dims = [\n                self.domain_sizes[parent] for parent in self.parents[i]\n            ] + [self.domain_sizes[i]]\n\n            # Reformatar a tabela plana em uma matriz multidimensional\n            table = np.array(flat_table).reshape(dims)\n            self.tables.append(table)\n\n    def causal_parser(self) -> None:\n        \"\"\"Parse the network for a Causal network.\"\"\"\n        for i in range(self.num_variables):\n            # Quantidade de entradas na tabela\n            num_entries = int(self.info[self.index])\n            self.index += 1\n            flat_table = []\n            for value in self.info[self.index:self.index + num_entries]:\n                if \".\" in value:\n                    flat_table.append(float(value))\n                else:\n                    one_hot = [0] * self.domain_sizes[i]\n                    one_hot[int(value)] = 1\n                    flat_table.extend(one_hot)\n            self.index += num_entries\n\n            # Obtenha o n\u00famero de dimens\u00f5es da tabela (pais + n\u00f3)\n            dims = [self.domain_sizes[parent]\n                    for parent in self.parents[i]] + [self.domain_sizes[i]]\n\n            # Reformatar a tabela plana em uma matriz multidimensional\n            table = np.array(flat_table).reshape(dims)\n            self.tables.append(table)\n\n    def calculate_probability(self, outcome: list[int]) -> float:\n        \"\"\"\n        Calculate the probability of a given outcome.\n\n        Args:\n            outcome (list[int]): list of variable values\n\n        Returns:\n            float: probability of the outcome\n        \"\"\"\n        probability = 1.0\n\n        for i, value in enumerate(outcome):\n            if len(self.parents[i]) == 0:\n                probability *= self.tables[i][value]\n            else:\n                parent_values = tuple(outcome[parent]\n                                      for parent in self.parents[i])\n                probability *= self.tables[i][parent_values + (value,)]\n\n        return probability\n\n\n\n    def calculate_probabilities_for_outcomes(\n        self, outcomes: list[list[int]]\n    ) -> list[float]:\n        \"\"\"\n        Calculate the probabilities of a list of outcomes.\n\n        Args:\n            outcomes (list[list[int]]): list of outcomes\n\n        Returns:\n            list[float]: list of probabilities\n        \"\"\"\n        probabilities = []\n        for outcome in outcomes:\n            prob = self.calculate_probability(outcome)\n            probabilities.append(prob)\n        return probabilities\n\n    def generate_data(self) -> pd.DataFrame:\n        \"\"\"\n        Generate data from the network.\n\n        Returns:\n            DataFrame: DataFrame with the generated data\n        \"\"\"\n        values = [list(range(size)) for size in self.domain_sizes]\n        outcomes = list(itertools.product(*values))\n        probs = self.calculate_probabilities_for_outcomes(outcomes)\n        probabilities = [[sublist, val]\n                         for sublist, val in zip(outcomes, probs)]\n        self.data = probsHelper(self.nodes, probabilities, csv_flag=False)\n        return self.data\n\n    def display(self) -> None:\n        \"\"\"\n        Display the network information.\n        \"\"\"\n        print(f\"Tipo de rede: {self.network_type}\")\n        print(f\"N\u00famero de vari\u00e1veis: {self.num_variables}\")\n        print(f\"Tamanhos dos dom\u00ednios: {self.domain_sizes}\")\n        print(\"Pais das vari\u00e1veis:\")\n        for parents in self.parents:\n            print(f\"  {parents}\")\n        print(\"Tabelas de probabilidades condicionais:\")\n        for table in self.tables:\n            print(table, end=\"\\n\\n\\n\")\n\ndef test_calculate_probability():\n    # Example test setup\n    nodes = [\"A\", \"B\", \"C\"]\n    domain_sizes = [2, 2, 2]\n    parents = [[], [0], [1]]\n    tables = [\n        np.array([0.6, 0.4]),  # P(A)\n        np.array([[0.2, 0.8], [0.5, 0.5]]),  # P(B|A)\n        np.array([[0.7, 0.3], [0.4, 0.6]])   # P(C|B)\n    ]\n\n    # Create UAIParser instance\n    parser = UAIParser(filepath=\"\", nodes=nodes)\n    parser.domain_sizes = domain_sizes\n    parser.parents = parents\n    parser.tables = tables\n\n    # Outcomes to test\n    outcome1 = [0, 0, 0]\n    outcome2 = [1, 1, 1]\n    outcome3 = [0, 1, 0]\n\n    # Assertions\n    assert parser.calculate_probability(outcome1) == parser.calculate_probability_new_implementation(outcome1)\n    assert parser.calculate_probability(outcome2) == parser.calculate_probability_new_implementation(outcome2)\n    assert parser.calculate_probability(outcome3) == parser.calculate_probability_new_implementation(outcome3)\n\nif __name__ == \"__main__\":\n    test_calculate_probability()"
    },
    {
        "func_name": "DfsRunner.dfs",
        "idx": "168",
        "repo_name": "Causal-Inference-Group-C4AI___interface-solver",
        "func_path": "utils/canonical_partitions/dfsRunner.py",
        "orig_func": "def dfs(node: int, graph: Graph):\n    graph.visited[node] = True\n    graph.curr_nodes.append(node)\n    is_observable = graph.cardinalities[node] > 1\n    if not is_observable:\n        for adj_node in graph.adj[node]:\n            if not graph.visited[adj_node]:\n                DfsRunner.dfs(adj_node, graph)\n    else:\n        for parent_node in graph.parents[node]:\n            if not graph.visited[parent_node] and graph.cardinalities[parent_node] < 1:\n                DfsRunner.dfs(parent_node, graph)",
        "orig_context": "```python\n## utils/canonical_partitions/graph.py\nclass Graph:\n    def __init__(\n        self, num_nodes: int, curr_nodes: list[int], visited: list[bool],\n        cardinalities: list[int], parents: list[int], adj: list[list[int]],\n        label_to_index: dict[str, int], index_to_label: dict[int, str],\n        dag_components: list[list[int]], exogenous: list[int],\n        endogenous: list[int]\n    ):\n        self.num_nodes = num_nodes\n        self.curr_nodes = curr_nodes\n        self.visited = visited\n        self.cardinalities = cardinalities\n        self.parents = parents\n        self.adj = adj\n        self.label_to_index = label_to_index\n        self.index_to_label = index_to_label\n        self.dag_components = dag_components\n        self.endogenous = endogenous\n        self.exogenous = exogenous\n\n    def parse(predefined_data=None, input_path=None):\n        if predefined_data:\n            num_nodes = predefined_data['num_nodes']\n            num_edges = predefined_data['num_edges']\n        elif input_path:\n            num_nodes, num_edges, nodes, nodes_cardinality, edges = file_parser(input_path)\n        else:\n            num_nodes = int(input())\n            num_edges = int(input())\n\n        label_to_index_ex: dict[str, int] = {}\n        index_to_label_ex: dict[int, str] = {}\n        adj_ex = [[] for _ in range(num_nodes + 1)]\n        cardinalities_ex = [0] * (num_nodes + 1)\n        visited_ex = [False] * (num_nodes + 1)\n        parents_ex = [[] for _ in range(num_nodes + 1)]\n        endogenIndex: list[int] = []\n        exogenIndex: list[int] = []\n\n        for i in range(1, num_nodes + 1):\n            if predefined_data:\n                label, cardinality = predefined_data['nodes'][i - 1].split()\n            elif input_path:\n                label, cardinality = nodes[i-1], nodes_cardinality[i-1]\n            else:\n                label, cardinality = input().split()\n            cardinality = int(cardinality)\n            label_to_index_ex[label] = i\n            index_to_label_ex[i] = label\n            cardinalities_ex[i] = cardinality\n\n        for i in range(num_edges):\n            if predefined_data:\n                u, v = predefined_data['edges'][i].split(\" -> \")\n            elif (input_path):\n                print(edges[i])\n                u, v = edges[i]\n            else:\n                u, v = input().split()\n            u_index = label_to_index_ex[u]\n            v_index = label_to_index_ex[v]\n            adj_ex[u_index].append(v_index)\n            parents_ex[v_index].append(u_index)\n\n        for i in range(1, num_nodes + 1):\n\n            if not (bool(parents_ex[i])):\n                exogenIndex.append(i)\n            else:\n                endogenIndex.append(i)\n\n        return Graph(\n            num_nodes=num_nodes,\n            curr_nodes=[],\n            visited=visited_ex,\n            cardinalities=cardinalities_ex,\n            parents=parents_ex,\n            adj=adj_ex,\n            index_to_label=index_to_label_ex,\n            label_to_index=label_to_index_ex,\n            dag_components=[],\n            exogenous=exogenIndex,\n            endogenous=endogenIndex\n        )\n\ndef file_parser(input_path):\n    with open(input_path, 'r') as file:\n        num_nodes = int(file.readline().strip())\n        num_edges = int(file.readline().strip())\n        nodes = []\n        nodes_cardinality = []\n\n        for _ in range(num_nodes):\n            tuple_node_cardinality = file.readline().strip().split(' ')\n            node = tuple_node_cardinality[0]\n            cardinality = int(tuple_node_cardinality[1])\n            nodes.append(node)\n            nodes_cardinality.append(cardinality)\n        edges = []\n        for _ in range(num_edges):\n            edge = file.readline().strip().split(' ')\n            origin_node = edge[0]\n            target_node = edge[1]\n            edges.append((origin_node, target_node))\n        return num_nodes, num_edges, nodes, nodes_cardinality, edges\n\n```\n\n\n```python\n## utils/canonical_partitions/dfsRunner.py\nfrom utils.canonical_partitions.graph import Graph\n\nclass DfsRunner:\n    def dfs(node: int, graph: Graph):\n        graph.visited[node] = True\n        graph.curr_nodes.append(node)\n        is_observable = graph.cardinalities[node] > 1\n\n        if not is_observable:\n            for adj_node in graph.adj[node]:\n                if not graph.visited[adj_node]:\n                    DfsRunner.dfs(adj_node, graph)\n        else:\n            for parent_node in graph.parents[node]:\n                if not graph.visited[parent_node] and \\\n                        graph.cardinalities[parent_node] < 1:\n                    DfsRunner.dfs(parent_node, graph)\n\n```\n\n\n",
        "eval_script": "class Graph:\n    def __init__(\n        self, num_nodes: int, curr_nodes: list[int], visited: list[bool],\n        cardinalities: list[int], parents: list[list[int]], adj: list[list[int]],\n        label_to_index: dict[str, int], index_to_label: dict[int, str],\n        dag_components: list[list[int]], exogenous: list[int],\n        endogenous: list[int]\n    ):\n        self.num_nodes = num_nodes\n        self.curr_nodes = curr_nodes\n        self.visited = visited\n        self.cardinalities = cardinalities\n        self.parents = parents\n        self.adj = adj\n        self.label_to_index = label_to_index\n        self.index_to_label = index_to_label\n        self.dag_components = dag_components\n        self.endogenous = endogenous\n        self.exogenous = exogenous\n\n    @staticmethod\n    def parse(predefined_data=None, input_path=None):\n        if predefined_data:\n            num_nodes = predefined_data['num_nodes']\n            num_edges = predefined_data['num_edges']\n        elif input_path:\n            num_nodes, num_edges, nodes, nodes_cardinality, edges = file_parser(input_path)\n        else:\n            num_nodes = int(input())\n            num_edges = int(input())\n\n        label_to_index_ex: dict[str, int] = {}\n        index_to_label_ex: dict[int, str] = {}\n        adj_ex = [[] for _ in range(num_nodes + 1)]\n        cardinalities_ex = [0] * (num_nodes + 1)\n        visited_ex = [False] * (num_nodes + 1)\n        parents_ex = [[] for _ in range(num_nodes + 1)]\n        endogenIndex: list[int] = []\n        exogenIndex: list[int] = []\n\n        for i in range(1, num_nodes + 1):\n            if predefined_data:\n                label, cardinality = predefined_data['nodes'][i - 1].split()\n            elif input_path:\n                label, cardinality = nodes[i-1], nodes_cardinality[i-1]\n            else:\n                label, cardinality = input().split()\n            cardinality = int(cardinality)\n            label_to_index_ex[label] = i\n            index_to_label_ex[i] = label\n            cardinalities_ex[i] = cardinality\n\n        for i in range(num_edges):\n            if predefined_data:\n                u, v = predefined_data['edges'][i].split(\" -> \")\n            elif (input_path):\n                print(edges[i])\n                u, v = edges[i]\n            else:\n                u, v = input().split()\n            u_index = label_to_index_ex[u]\n            v_index = label_to_index_ex[v]\n            adj_ex[u_index].append(v_index)\n            parents_ex[v_index].append(u_index)\n\n        for i in range(1, num_nodes + 1):\n            if not (bool(parents_ex[i])):\n                exogenIndex.append(i)\n            else:\n                endogenIndex.append(i)\n\n        return Graph(\n            num_nodes=num_nodes,\n            curr_nodes=[],\n            visited=visited_ex,\n            cardinalities=cardinalities_ex,\n            parents=parents_ex,\n            adj=adj_ex,\n            index_to_label=index_to_label_ex,\n            label_to_index=label_to_index_ex,\n            dag_components=[],\n            exogenous=exogenIndex,\n            endogenous=endogenIndex\n        )\n\n\ndef file_parser(input_path):\n    with open(input_path, 'r') as file:\n        num_nodes = int(file.readline().strip())\n        num_edges = int(file.readline().strip())\n        nodes = []\n        nodes_cardinality = []\n\n        for _ in range(num_nodes):\n            tuple_node_cardinality = file.readline().strip().split(' ')\n            node = tuple_node_cardinality[0]\n            cardinality = int(tuple_node_cardinality[1])\n            nodes.append(node)\n            nodes_cardinality.append(cardinality)\n        edges = []\n        for _ in range(num_edges):\n            edge = file.readline().strip().split(' ')\n            origin_node = edge[0]\n            target_node = edge[1]\n            edges.append((origin_node, target_node))\n        return num_nodes, num_edges, nodes, nodes_cardinality, edges\n\n\nclass DfsRunner:\n    @staticmethod\n    def dfs(node: int, graph: Graph):\n        graph.visited[node] = True\n        graph.curr_nodes.append(node)\n        is_observable = graph.cardinalities[node] > 1\n\n        if not is_observable:\n            for adj_node in graph.adj[node]:\n                if not graph.visited[adj_node]:\n                    DfsRunner.dfs(adj_node, graph)\n        else:\n            for parent_node in graph.parents[node]:\n                if not graph.visited[parent_node] and \\\n                        graph.cardinalities[parent_node] < 1:\n                    DfsRunner.dfs(parent_node, graph)\n\n\n\n\n\ndef test_dfs():\n    predefined_data1 = {\n        'num_nodes': 3,\n        'num_edges': 2,\n        'nodes': ['A 2', 'B 1', 'C 3'],\n        'edges': ['A -> B', 'B -> C']\n    }\n    graph1 = Graph.parse(predefined_data=predefined_data1)\n    results_dfs = []\n    results_dfs_new = []\n    \n    for exogenous_node in graph1.exogenous:\n        if not graph1.visited[exogenous_node]:\n            DfsRunner.dfs(exogenous_node, graph1)\n    results_dfs = graph1.curr_nodes\n\n    graph1 = Graph.parse(predefined_data=predefined_data1)  # Resetting the graph\n    for exogenous_node in graph1.exogenous:\n        if not graph1.visited[exogenous_node]:\n            DfsRunner.dfs_new_implementation(exogenous_node, graph1)\n    results_dfs_new = graph1.curr_nodes\n\n    # Assert statements for testing equivalence\n    assert results_dfs == results_dfs_new\n\n    predefined_data2 = {\n        'num_nodes': 4,\n        'num_edges': 3,\n        'nodes': ['X 1', 'Y 2', 'Z 3', 'W 1'],\n        'edges': ['X -> Y', 'Y -> Z', 'Z -> W']\n    }\n    graph2 = Graph.parse(predefined_data=predefined_data2)\n    \n    results_dfs = []\n    results_dfs_new = []\n    \n    for exogenous_node in graph2.exogenous:\n        if not graph2.visited[exogenous_node]:\n            DfsRunner.dfs(exogenous_node, graph2)\n    results_dfs = graph2.curr_nodes\n\n    graph2 = Graph.parse(predefined_data=predefined_data2)  # Resetting the graph\n    for exogenous_node in graph2.exogenous:\n        if not graph2.visited[exogenous_node]:\n            DfsRunner.dfs_new_implementation(exogenous_node, graph2)\n    results_dfs_new = graph2.curr_nodes\n\n    # Assert statements for testing equivalence\n    assert results_dfs == results_dfs_new\n\n    predefined_data3 = {\n        'num_nodes': 4,\n        'num_edges': 4,\n        'nodes': ['D 1', 'E 2', 'F 1', 'G 3'],\n        'edges': ['D -> E', 'E -> F', 'F -> G', 'G -> E']\n    }\n    graph3 = Graph.parse(predefined_data=predefined_data3)\n    \n    results_dfs = []\n    results_dfs_new = []\n    \n    for exogenous_node in graph3.exogenous:\n        if not graph3.visited[exogenous_node]:\n            DfsRunner.dfs(exogenous_node, graph3)\n    results_dfs = graph3.curr_nodes\n\n    graph3 = Graph.parse(predefined_data=predefined_data3)  # Resetting the graph\n    for exogenous_node in graph3.exogenous:\n        if not graph3.visited[exogenous_node]:\n            DfsRunner.dfs_new_implementation(exogenous_node, graph3)\n    results_dfs_new = graph3.curr_nodes\n\n    # Assert statements for testing equivalence\n    assert results_dfs == results_dfs_new\n\n\nif __name__ == \"__main__\":\n    test_dfs()"
    },
    {
        "func_name": "count_capture",
        "idx": "170",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "helpers.py",
        "orig_func": "def count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    (r, c) = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    captured = 0\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board, move_pos, player, dir)\n    return captured",
        "orig_context": "```python\n## helpers.py\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\n```\n\n\n",
        "eval_script": "## helpers.py\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board, move_pos, player, dir)\n\n    return captured\n\n# Placeholder for the new implementation function\n\n\ndef test_count_capture():\n    import numpy as np\n    \n    # Test Case 1\n    board1 = np.array([\n        [1, -1, 0],\n        [-1, -1, 1],\n        [0, 1, 0]\n    ])\n    assert count_capture(board1, (0, 2), 1) == count_capture_new_implementation(board1, (0, 2), 1)\n    \n    # Test Case 2\n    board2 = np.array([\n        [0, -1, 1],\n        [-1, 1, -1],\n        [1, -1, 0]\n    ])\n    assert count_capture(board2, (2, 2), 1) == count_capture_new_implementation(board2, (2, 2), 1)\n    \n    # Test Case 3\n    board3 = np.array([\n        [1, 0, 0],\n        [0, -1, 0],\n        [0, 0, 1]\n    ])\n    assert count_capture(board3, (1, 0), 1) == count_capture_new_implementation(board3, (1, 0), 1)\n\nif __name__ == \"__main__\":\n    test_count_capture()"
    },
    {
        "func_name": "check_endgame",
        "idx": "171",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "helpers.py",
        "orig_func": "def check_endgame(chess_board, player, opponent):\n    \"\"\"\n    Check if the game ends and compute the final score. \n    \n    Note that the game may end when a) the board is full or \n    b) when it's not full yet but both players are unable to make a valid move.\n    One reason for b) occurring is when one player has no stones left. In human\n    play this is sometimes scored as the max possible win (e.g. 64-0), but \n    we do not implement this scoring here and simply count the stones.\n\n    Returns\n    -------\n    is_endgame : bool\n        Whether the game ends.\n    player_1_score : int\n        The score of player 1.\n    player_2_score : int\n        The score of player 2.\n    \"\"\"\n    is_endgame = False\n    valid_moves = get_valid_moves(chess_board, player)\n    if not valid_moves:\n        opponent_valid_moves = get_valid_moves(chess_board, opponent)\n        if not opponent_valid_moves:\n            is_endgame = True\n    p0_score = np.sum(chess_board == 1)\n    p1_score = np.sum(chess_board == 2)\n    return (is_endgame, p0_score, p1_score)",
        "orig_context": "```python\n## helpers.py\nimport numpy as np\n\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\ndef check_endgame(chess_board,player,opponent):\n    \"\"\"\n    Check if the game ends and compute the final score. \n    \n    Note that the game may end when a) the board is full or \n    b) when it's not full yet but both players are unable to make a valid move.\n    One reason for b) occurring is when one player has no stones left. In human\n    play this is sometimes scored as the max possible win (e.g. 64-0), but \n    we do not implement this scoring here and simply count the stones.\n\n    Returns\n    -------\n    is_endgame : bool\n        Whether the game ends.\n    player_1_score : int\n        The score of player 1.\n    player_2_score : int\n        The score of player 2.\n    \"\"\"\n\n    is_endgame = False\n\n    valid_moves = get_valid_moves(chess_board,player)\n    if not valid_moves:\n        opponent_valid_moves = get_valid_moves(chess_board,opponent)\n        if not opponent_valid_moves:\n            is_endgame = True  # When no-one can play, the game is over, score is current piece count\n\n    p0_score = np.sum(chess_board == 1)\n    p1_score = np.sum(chess_board == 2)\n    return is_endgame, p0_score, p1_score\n\n```\n\n\n",
        "eval_script": "## helpers.py\nimport numpy as np\n\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\ndef check_endgame(chess_board,player,opponent):\n    \"\"\"\n    Check if the game ends and compute the final score. \n    \n    Note that the game may end when a) the board is full or \n    b) when it's not full yet but both players are unable to make a valid move.\n    One reason for b) occurring is when one player has no stones left. In human\n    play this is sometimes scored as the max possible win (e.g. 64-0), but \n    we do not implement this scoring here and simply count the stones.\n\n    Returns\n    -------\n    is_endgame : bool\n        Whether the game ends.\n    player_1_score : int\n        The score of player 1.\n    player_2_score : int\n        The score of player 2.\n    \"\"\"\n\n    is_endgame = False\n\n    valid_moves = get_valid_moves(chess_board,player)\n    if not valid_moves:\n        opponent_valid_moves = get_valid_moves(chess_board,opponent)\n        if not opponent_valid_moves:\n            is_endgame = True  # When no-one can play, the game is over, score is current piece count\n\n    p0_score = np.sum(chess_board == 1)\n    p1_score = np.sum(chess_board == 2)\n    return is_endgame, p0_score, p1_score\n\n\n\ndef test_check_endgame():\n    board1 = np.array([[1, 2, 0],\n                       [2, 1, 0],\n                       [0, 0, 0]])\n    result1 = check_endgame(board1, 1, 2)\n    result1_new = check_endgame_new_implementation(board1, 1, 2)\n    assert result1 == result1_new, f\"Test case 1 failed: {result1} != {result1_new}\"\n\n    board2 = np.array([[1, 1, 1],\n                       [1, 1, 1],\n                       [1, 1, 1]])\n    result2 = check_endgame(board2, 1, 2)\n    result2_new = check_endgame_new_implementation(board2, 1, 2)\n    assert result2 == result2_new, f\"Test case 2 failed: {result2} != {result2_new}\"\n\n    board3 = np.array([[1, 2, 2],\n                       [2, 1, 0],\n                       [1, 0, 2]])\n    result3 = check_endgame(board3, 1, 2)\n    result3_new = check_endgame_new_implementation(board3, 1, 2)\n    assert result3 == result3_new, f\"Test case 3 failed: {result3} != {result3_new}\"\n\nif __name__ == \"__main__\":\n    test_check_endgame()"
    },
    {
        "func_name": "get_valid_moves",
        "idx": "172",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "helpers.py",
        "orig_func": "def get_valid_moves(chess_board, player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board, (r, c), player) > 0:\n                valid_moves.append((r, c))\n    return valid_moves",
        "orig_context": "```python\n## helpers.py\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\n```\n\n\n",
        "eval_script": "## helpers.py\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\n\n\ndef test_get_valid_moves():\n    import numpy as np\n\n    # Test Case 1: Default board\n    board1 = np.array([\n        [0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 2, 0, 0, 0],\n        [0, 0, 1, 1, 2, 0, 0, 0],\n        [0, 0, 2, 1, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0]\n    ])\n    player = 1\n    assert get_valid_moves(board1, player) == get_valid_moves_new_implementation(board1, player)\n\n    # Test Case 2: Empty board\n    board2 = np.zeros((8, 8), dtype=int)\n    player = 2\n    assert get_valid_moves(board2, player) == get_valid_moves_new_implementation(board2, player)\n\n    # Test Case 3: Full board\n    board3 = np.ones((8, 8), dtype=int)\n    player = 1\n    assert get_valid_moves(board3, player) == get_valid_moves_new_implementation(board3, player)\n\nif __name__ == \"__main__\":\n    test_get_valid_moves()"
    },
    {
        "func_name": "random_move",
        "idx": "173",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "helpers.py",
        "orig_func": "def random_move(chess_board, player):\n    \"\"\"\n    random move from the list of valid moves.\n\n    Returns\n\n    ------\n    (tuple)\n\n\n    \"\"\"\n    valid_moves = get_valid_moves(chess_board, player)\n    if len(valid_moves) == 0:\n        print(f'No valid moves left for player {player}.')\n        return None\n    return valid_moves[np.random.randint(len(valid_moves))]",
        "orig_context": "```python\n## helpers.py\nimport numpy as np\n\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\ndef random_move(chess_board, player):\n    \"\"\"\n    random move from the list of valid moves.\n\n    Returns\n\n    ------\n    (tuple)\n\n\n    \"\"\"\n\n    valid_moves = get_valid_moves(chess_board,player)\n\n    if len(valid_moves) == 0:\n        # If no valid moves are available, return None\n        print(f\"No valid moves left for player {player}.\")\n        return None\n    \n    return valid_moves[np.random.randint(len(valid_moves))]\n\n```\n\n\n",
        "eval_script": "## helpers.py\nimport numpy as np\n\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\ndef random_move(chess_board, player):\n    \"\"\"\n    random move from the list of valid moves.\n\n    Returns\n\n    ------\n    (tuple)\n\n\n    \"\"\"\n\n    valid_moves = get_valid_moves(chess_board,player)\n    np.random.seed(0)  # Ensure reproducibility for testing\n\n    if len(valid_moves) == 0:\n        # If no valid moves are available, return None\n        print(f\"No valid moves left for player {player}.\")\n        return None\n    \n    return valid_moves[np.random.randint(len(valid_moves))]\n\n\ndef test_random_move():\n    # Test scenario 1\n    board1 = np.array([\n        [0, 1, -1],\n        [-1, 1, 0],\n        [0, -1, 1]\n    ])\n    player1 = 1\n    assert random_move(board1, player1) == random_move_new_implementation(board1, player1)\n    \n    # Test scenario 2\n    board2 = np.array([\n        [1, -1, 0],\n        [0, 1, -1],\n        [-1, 0, 1]\n    ])\n    player2 = -1\n    assert random_move(board2, player2) == random_move_new_implementation(board2, player2)\n    \n    # Test scenario 3: No valid moves\n    board3 = np.array([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]\n    ])\n    player3 = 1\n    assert random_move(board3, player3) == random_move_new_implementation(board3, player3)\n\nif __name__ == \"__main__\":\n    test_random_move()"
    },
    {
        "func_name": "RandomAgent.step",
        "idx": "175",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "agents/random_agent.py",
        "orig_func": "def step(self, chess_board, player, opponent):\n    \"\"\"\n        Randomly selects a valid position to place a disc.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n    return random_move(chess_board, player)",
        "orig_context": "```python\n## store.py\nAGENT_REGISTRY = {}\n\ndef register_agent(agent_name=\"\"):\n    def decorator(func):\n        if agent_name not in AGENT_REGISTRY:\n            AGENT_REGISTRY[agent_name] = func\n        else:\n            raise AssertionError(\n                f\"Agent {AGENT_REGISTRY[agent_name]} is already registered.\"\n            )\n        return func\n\n    return decorator\n\n```\n\n\n```python\n## agents/agent.py\nclass Agent:\n    def __init__(self):\n        \"\"\"\n        Initialize the agent, add a name which is used to register the agent\n        \"\"\"\n        self.name = \"DummyAgent\"\n        # Flag to indicate whether the agent can be used to autoplay\n        self.autoplay = True\n\n    def __str__(self) -> str:\n        return self.name\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Main decision logic of the agent, which is called by the simulator.\n        Extend this method to implement your own agent to play the game.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n        pass\n\n```\n\n\n```python\n## helpers.py\nimport numpy as np\n\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\ndef random_move(chess_board, player):\n    \"\"\"\n    random move from the list of valid moves.\n\n    Returns\n\n    ------\n    (tuple)\n\n\n    \"\"\"\n\n    valid_moves = get_valid_moves(chess_board,player)\n\n    if len(valid_moves) == 0:\n        # If no valid moves are available, return None\n        print(f\"No valid moves left for player {player}.\")\n        return None\n    \n    return valid_moves[np.random.randint(len(valid_moves))]\n\n```\n\n\n```python\n## agents/random_agent.py\nfrom agents.agent import Agent\n\nfrom helpers import random_move\n\nfrom store import register_agent\n\nclass RandomAgent(Agent):\n    \"\"\"\n    Example of an agent which takes random decisions\n    \"\"\"\n\n    def __init__(self):\n        super(RandomAgent, self).__init__()\n        self.name = \"RandomAgent\"\n        self.autoplay = True\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Randomly selects a valid position to place a disc.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n\n        return random_move(chess_board, player)\n\n```\n\n\n",
        "eval_script": "import numpy as np\n\n# Store Module\nAGENT_REGISTRY = {}\n\ndef register_agent(agent_name=\"\"):\n    def decorator(func):\n        if agent_name not in AGENT_REGISTRY:\n            AGENT_REGISTRY[agent_name] = func\n        else:\n            raise AssertionError(\n                f\"Agent {AGENT_REGISTRY[agent_name]} is already registered.\"\n            )\n        return func\n\n    return decorator\n\n# Agents Module\nclass Agent:\n    def __init__(self):\n        \"\"\"\n        Initialize the agent, add a name which is used to register the agent\n        \"\"\"\n        self.name = \"DummyAgent\"\n        # Flag to indicate whether the agent can be used to autoplay\n        self.autoplay = True\n\n    def __str__(self) -> str:\n        return self.name\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Main decision logic of the agent, which is called by the simulator.\n        Extend this method to implement your own agent to play the game.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n        pass\n\n# Helpers Module\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n\n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board, move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef get_valid_moves(chess_board, player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n    -------\n    valid_moves : [(tuple)]\n    \"\"\"\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board, (r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\ndef random_move(chess_board, player):\n    \"\"\"\n    Randomly selects a valid move from the list of valid moves.\n\n    Returns\n    -------\n    move : (tuple)\n        The selected move position.\n    \"\"\"\n    valid_moves = get_valid_moves(chess_board, player)\n\n    if len(valid_moves) == 0:\n        # If no valid moves are available, return None\n        print(f\"No valid moves left for player {player}.\")\n        return None\n    \n    return valid_moves[np.random.randint(len(valid_moves))]\n\n# RandomAgent Definition\nclass RandomAgent(Agent):\n    \"\"\"\n    Example of an agent which takes random decisions\n    \"\"\"\n\n    def __init__(self):\n        super(RandomAgent, self).__init__()\n        self.name = \"RandomAgent\"\n        self.autoplay = True\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Randomly selects a valid position to place a disc.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n        return random_move(chess_board, player)\n\n\n\n\ndef test_step():\n    agent = RandomAgent()\n    \n    # Test case 1: Basic chess board\n    board1 = np.array([\n        [0, 0, 0, 0],\n        [0, 1, 2, 0],\n        [0, 2, 1, 0],\n        [0, 0, 0, 0]\n    ])\n    player1, opponent1 = 1, 2\n    assert agent.step(board1, player1, opponent1) in get_valid_moves(board1, player1)\n    assert agent.step_new_implementation(board1, player1, opponent1) in get_valid_moves(board1, player1)\n    \n    # Test case 2: No valid moves\n    board2 = np.array([\n        [1, 1, 1, 1],\n        [1, 2, 2, 1],\n        [1, 2, 2, 1],\n        [1, 1, 1, 1]\n    ])\n    player2, opponent2 = 1, 2\n    assert agent.step(board2, player2, opponent2) is None\n    assert agent.step_new_implementation(board2, player2, opponent2) is None\n    \n    # Test case 3: Almost filled board\n    board3 = np.array([\n        [1, 2, 1, 2],\n        [1, 2, 2, 1],\n        [2, 1, 1, 2],\n        [1, 0, 2, 1]\n    ])\n    player3, opponent3 = 1, 2\n    valid_moves3 = get_valid_moves(board3, player3)\n    assert agent.step(board3, player3, opponent3) in valid_moves3\n    assert agent.step_new_implementation(board3, player3, opponent3) in valid_moves3\n\nif __name__ == \"__main__\":\n    test_step()"
    },
    {
        "func_name": "StudentAgent.evaluate_board",
        "idx": "176",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "agents/gpt_greedy_corners_agent.py",
        "orig_func": "def evaluate_board(self, board, color, player_score, opponent_score):\n    \"\"\"\n        Evaluate the board state based on multiple factors.\n\n        Parameters:\n        - board: 2D numpy array representing the game board.\n        - color: Integer representing the agent's color (1 for Player 1/Blue, 2 for Player 2/Brown).\n        - player_score: Score of the current player.\n        - opponent_score: Score of the opponent.\n\n        Returns:\n        - int: The evaluated score of the board.\n        \"\"\"\n    corners = [(0, 0), (0, board.shape[1] - 1), (board.shape[0] - 1, 0), (board.shape[0] - 1, board.shape[1] - 1)]\n    corner_score = sum((1 for corner in corners if board[corner] == color)) * 10\n    corner_penalty = sum((1 for corner in corners if board[corner] == 3 - color)) * -10\n    opponent_moves = len(get_valid_moves(board, 3 - color))\n    mobility_score = -opponent_moves\n    total_score = player_score - opponent_score + corner_score + corner_penalty + mobility_score\n    return total_score",
        "orig_context": "```python\n## store.py\nAGENT_REGISTRY = {}\n\ndef register_agent(agent_name=\"\"):\n    def decorator(func):\n        if agent_name not in AGENT_REGISTRY:\n            AGENT_REGISTRY[agent_name] = func\n        else:\n            raise AssertionError(\n                f\"Agent {AGENT_REGISTRY[agent_name]} is already registered.\"\n            )\n        return func\n\n    return decorator\n\n```\n\n\n```python\n## helpers.py\nimport numpy as np\n\ndef get_directions():\n    \"\"\"\n    Get all directions (8 directions: up, down, left, right, and diagonals)\n\n    Returns\n    -------\n    list of tuple\n        List of direction vectors\n    \"\"\"\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    \"\"\"\n    Check how many opponent's discs are captured.\n\n    Returns\n    -------\n    int\n        The number of stones that will be captured making this move, including all directions.\n        Zero indicates any form of invalid move.\n    \"\"\"\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    # Check if move captures any opponent discs in any direction\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    \"\"\"\n    Check if placing a disc at move_pos captures any discs in the specified direction.\n\n    Returns\n    -------\n    int\n        Number of stones captured in this direction\n    \"\"\"\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef execute_move(chess_board, move_pos, player):\n    \"\"\"\n    Play the move specified by altering the chess_board.\n    Note that chess_board is a pass-by-reference in/output parameter.\n    Consider copy.deepcopy() of the chess_board if you want to consider numerous possibilities.\n    \"\"\"\n    r, c = move_pos\n    chess_board[r, c] = player\n\n    # Flip opponent's discs in all directions where captures occur\n    for direction in get_directions():\n        flip_discs(chess_board,move_pos, player, direction)\n\ndef flip_discs(chess_board, move_pos, player, direction):\n    \n    if count_capture_dir(chess_board,move_pos, player, direction) == 0:\n        return\n    \n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n\n    while chess_board[r, c] != player:\n        chess_board[r, c] = player\n        r += dx\n        c += dy\n\ndef check_endgame(chess_board,player,opponent):\n    \"\"\"\n    Check if the game ends and compute the final score. \n    \n    Note that the game may end when a) the board is full or \n    b) when it's not full yet but both players are unable to make a valid move.\n    One reason for b) occurring is when one player has no stones left. In human\n    play this is sometimes scored as the max possible win (e.g. 64-0), but \n    we do not implement this scoring here and simply count the stones.\n\n    Returns\n    -------\n    is_endgame : bool\n        Whether the game ends.\n    player_1_score : int\n        The score of player 1.\n    player_2_score : int\n        The score of player 2.\n    \"\"\"\n\n    is_endgame = False\n\n    valid_moves = get_valid_moves(chess_board,player)\n    if not valid_moves:\n        opponent_valid_moves = get_valid_moves(chess_board,opponent)\n        if not opponent_valid_moves:\n            is_endgame = True  # When no-one can play, the game is over, score is current piece count\n\n    p0_score = np.sum(chess_board == 1)\n    p1_score = np.sum(chess_board == 2)\n    return is_endgame, p0_score, p1_score\n\ndef get_valid_moves(chess_board,player):\n    \"\"\"\n    Get all valid moves given the chess board and player.\n\n    Returns\n\n    -------\n    valid_moves : [(tuple)]\n\n    \"\"\"\n\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\n```\n\n\n```python\n## agents/agent.py\nclass Agent:\n    def __init__(self):\n        \"\"\"\n        Initialize the agent, add a name which is used to register the agent\n        \"\"\"\n        self.name = \"DummyAgent\"\n        # Flag to indicate whether the agent can be used to autoplay\n        self.autoplay = True\n\n    def __str__(self) -> str:\n        return self.name\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Main decision logic of the agent, which is called by the simulator.\n        Extend this method to implement your own agent to play the game.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n        pass\n\n```\n\n\n```python\n## agents/gpt_greedy_corners_agent.py\nfrom agents.agent import Agent\n\nfrom store import register_agent\n\nfrom helpers import get_valid_moves, count_capture, execute_move, check_endgame\n\nimport copy\n\nimport random\n\nclass StudentAgent(Agent):\n    \"\"\"\n    A custom agent for playing Reversi/Othello.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.name = \"gpt_greedy_corners_agent\"\n\n    def step(self, board, color,opponent):\n        \"\"\"\n        Choose a move based on an improved heuristic logic.\n\n        Parameters:\n        - board: 2D numpy array representing the game board.\n        - color: Integer representing the agent's color (1 for Player 1/Blue, 2 for Player 2/Brown).\n\n        Returns:\n        - Tuple (x, y): The coordinates of the chosen move.\n        \"\"\"\n        # Get all legal moves for the current player\n        legal_moves = get_valid_moves(board, color)\n\n        if not legal_moves:\n            return None  # No valid moves available, pass turn\n\n        # Advanced heuristic: prioritize corners and maximize flips while minimizing opponent's potential moves\n        best_move = None\n        best_score = float('-inf')\n\n        for move in legal_moves:\n            simulated_board = copy.deepcopy(board)\n            execute_move(simulated_board, move, color)\n            _, player_score, opponent_score = check_endgame(simulated_board, color, 3 - color)\n            move_score = self.evaluate_board(simulated_board, color, player_score, opponent_score)\n\n            if move_score > best_score:\n                best_score = move_score\n                best_move = move\n\n        # Return the best move found\n        return best_move if best_move else random.choice(legal_moves)\n\n    def evaluate_board(self, board, color, player_score, opponent_score):\n        \"\"\"\n        Evaluate the board state based on multiple factors.\n\n        Parameters:\n        - board: 2D numpy array representing the game board.\n        - color: Integer representing the agent's color (1 for Player 1/Blue, 2 for Player 2/Brown).\n        - player_score: Score of the current player.\n        - opponent_score: Score of the opponent.\n\n        Returns:\n        - int: The evaluated score of the board.\n        \"\"\"\n        # Corner positions are highly valuable\n        corners = [(0, 0), (0, board.shape[1] - 1), (board.shape[0] - 1, 0), (board.shape[0] - 1, board.shape[1] - 1)]\n        corner_score = sum(1 for corner in corners if board[corner] == color) * 10\n        corner_penalty = sum(1 for corner in corners if board[corner] == 3 - color) * -10\n\n        # Mobility: the number of moves the opponent can make\n        opponent_moves = len(get_valid_moves(board, 3 - color))\n        mobility_score = -opponent_moves\n\n        # Combine scores\n        total_score = player_score - opponent_score + corner_score + corner_penalty + mobility_score\n        return total_score\n\n```\n\n\n",
        "eval_script": "import numpy as np\nimport copy\nimport random\n\n## store.py content integrated\nAGENT_REGISTRY = {}\n\ndef register_agent(agent_name=\"\"):\n    def decorator(func):\n        if agent_name not in AGENT_REGISTRY:\n            AGENT_REGISTRY[agent_name] = func\n        else:\n            raise AssertionError(\n                f\"Agent {AGENT_REGISTRY[agent_name]} is already registered.\"\n            )\n        return func\n\n    return decorator\n\n## helpers.py content integrated\ndef get_directions():\n    return [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n\ndef count_capture(chess_board, move_pos, player):\n    r, c = move_pos\n    if chess_board[r, c] != 0:\n        return 0\n    \n    captured = 0\n\n    for dir in get_directions():\n        captured = captured + count_capture_dir(chess_board,move_pos, player, dir)\n\n    return captured\n\ndef count_capture_dir(chess_board, move_pos, player, direction):\n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n    captured = 0\n    board_size = chess_board.shape[0]\n\n    while 0 <= r < board_size and 0 <= c < board_size:\n        if chess_board[r, c] == 0:\n            return 0\n        if chess_board[r, c] == player:\n            return captured\n        captured = captured + 1\n        r += dx\n        c += dy\n\n    return 0\n\ndef execute_move(chess_board, move_pos, player):\n    r, c = move_pos\n    chess_board[r, c] = player\n\n    for direction in get_directions():\n        flip_discs(chess_board,move_pos, player, direction)\n\ndef flip_discs(chess_board, move_pos, player, direction):\n    if count_capture_dir(chess_board,move_pos, player, direction) == 0:\n        return\n    \n    r, c = move_pos\n    dx, dy = direction\n    r += dx\n    c += dy\n\n    while chess_board[r, c] != player:\n        chess_board[r, c] = player\n        r += dx\n        c += dy\n\ndef check_endgame(chess_board,player,opponent):\n    is_endgame = False\n\n    valid_moves = get_valid_moves(chess_board,player)\n    if not valid_moves:\n        opponent_valid_moves = get_valid_moves(chess_board,opponent)\n        if not opponent_valid_moves:\n            is_endgame = True\n\n    p0_score = np.sum(chess_board == 1)\n    p1_score = np.sum(chess_board == 2)\n    return is_endgame, p0_score, p1_score\n\ndef get_valid_moves(chess_board,player):\n    board_size = chess_board.shape[0]\n    valid_moves = []\n    for r in range(board_size):\n        for c in range(board_size):\n            if count_capture(chess_board,(r, c), player) > 0:\n                valid_moves.append((r, c))\n\n    return valid_moves\n\n## agents/agent.py content integrated\nclass Agent:\n    def __init__(self):\n        self.name = \"DummyAgent\"\n        self.autoplay = True\n\n    def __str__(self) -> str:\n        return self.name\n\n    def step(self, chess_board, player, opponent):\n        pass\n\n## agents/gpt_greedy_corners_agent.py\nclass StudentAgent(Agent):\n    \"\"\"\n    A custom agent for playing Reversi/Othello.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.name = \"gpt_greedy_corners_agent\"\n\n    def step(self, board, color, opponent):\n        legal_moves = get_valid_moves(board, color)\n\n        if not legal_moves:\n            return None\n\n        best_move = None\n        best_score = float('-inf')\n\n        for move in legal_moves:\n            simulated_board = copy.deepcopy(board)\n            execute_move(simulated_board, move, color)\n            _, player_score, opponent_score = check_endgame(simulated_board, color, 3 - color)\n            move_score = self.evaluate_board(simulated_board, color, player_score, opponent_score)\n\n            if move_score > best_score:\n                best_score = move_score\n                best_move = move\n\n        return best_move if best_move else random.choice(legal_moves)\n\n    def evaluate_board(self, board, color, player_score, opponent_score):\n        corners = [(0, 0), (0, board.shape[1] - 1), (board.shape[0] - 1, 0), (board.shape[0] - 1, board.shape[1] - 1)]\n        corner_score = sum(1 for corner in corners if board[corner] == color) * 10\n        corner_penalty = sum(1 for corner in corners if board[corner] == 3 - color) * -10\n\n        opponent_moves = len(get_valid_moves(board, 3 - color))\n        mobility_score = -opponent_moves\n\n        total_score = player_score - opponent_score + corner_score + corner_penalty + mobility_score\n        return total_score\n\n\n        \ndef test_evaluate_board():\n    agent = StudentAgent()\n\n    # Test case 1\n    board1 = np.array([\n        [1, 2, 0, 0],\n        [2, 1, 0, 0],\n        [0, 0, 1, 2],\n        [0, 0, 2, 1]\n    ])\n    color1 = 1\n    player_score1 = 4\n    opponent_score1 = 4\n    assert agent.evaluate_board(board1, color1, player_score1, opponent_score1) == agent.evaluate_board_new_implementation(board1, color1, player_score1, opponent_score1)\n\n    # Test case 2\n    board2 = np.array([\n        [1, 1, 1, 2],\n        [2, 2, 1, 1],\n        [1, 2, 2, 0],\n        [0, 1, 1, 1]\n    ])\n    color2 = 2\n    player_score2 = 5\n    opponent_score2 = 6\n    assert agent.evaluate_board(board2, color2, player_score2, opponent_score2) == agent.evaluate_board_new_implementation(board2, color2, player_score2, opponent_score2)\n\n    # Test case 3\n    board3 = np.array([\n        [0, 0, 2, 2],\n        [0, 1, 2, 2],\n        [1, 1, 0, 0],\n        [1, 1, 1, 0]\n    ])\n    color3 = 1\n    player_score3 = 6\n    opponent_score3 = 5\n    assert agent.evaluate_board(board3, color3, player_score3, opponent_score3) == agent.evaluate_board_new_implementation(board3, color3, player_score3, opponent_score3)\n    \nif __name__ == \"__main__\":\n    test_evaluate_board()"
    },
    {
        "func_name": "HumanAgent.check_valid_input",
        "idx": "181",
        "repo_name": "chantalzhang___reversi_othello_ai",
        "func_path": "agents/human_agent.py",
        "orig_func": "def check_valid_input(self, x, y, chess_board):\n    \"\"\"\n        Check if the input position is valid (within the board and the spot is empty)\n\n        Parameters\n        ----------\n        x : int\n            The x position on the board.\n        y : int\n            The y position on the board.\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black, and 2 for white.\n\n        Returns\n        -------\n        bool\n            True if the input is valid, False otherwise.\n        \"\"\"\n    board_size = chess_board.shape[0]\n    return 0 <= x < board_size and 0 <= y < board_size and (chess_board[x, y] == 0)",
        "orig_context": "```python\n## store.py\nAGENT_REGISTRY = {}\n\ndef register_agent(agent_name=\"\"):\n    def decorator(func):\n        if agent_name not in AGENT_REGISTRY:\n            AGENT_REGISTRY[agent_name] = func\n        else:\n            raise AssertionError(\n                f\"Agent {AGENT_REGISTRY[agent_name]} is already registered.\"\n            )\n        return func\n\n    return decorator\n\n```\n\n\n```python\n## agents/agent.py\nclass Agent:\n    def __init__(self):\n        \"\"\"\n        Initialize the agent, add a name which is used to register the agent\n        \"\"\"\n        self.name = \"DummyAgent\"\n        # Flag to indicate whether the agent can be used to autoplay\n        self.autoplay = True\n\n    def __str__(self) -> str:\n        return self.name\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Main decision logic of the agent, which is called by the simulator.\n        Extend this method to implement your own agent to play the game.\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (x, y) where the player places the disc.\n        \"\"\"\n        pass\n\n```\n\n\n```python\n## agents/human_agent.py\nimport sys\n\nfrom agents.agent import Agent\n\nfrom store import register_agent\n\nclass HumanAgent(Agent):\n    def __init__(self):\n        super(HumanAgent, self).__init__()\n        self.name = \"HumanAgent\"\n\n    def step(self, chess_board, player, opponent):\n        \"\"\"\n        Get human input for the position to place the disc\n\n        Parameters\n        ----------\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black (Player 1),\n            and 2 for white (Player 2).\n        player : int\n            The current player (1 for black, 2 for white).\n        opponent : int\n            The opponent player (1 for black, 2 for white).\n\n        Returns\n        -------\n        move_pos : tuple of int\n            The position (r,c) where the player places the disc.\n        \"\"\"\n        text = input(\"Your move (row,column) or input q to quit: \")\n        while len(text.split(\",\")) != 2 and \"q\" not in text.lower():\n            print(\"Wrong Input Format! Input should be row,column.\")\n            text = input(\"Your move (row,column) or input q to quit: \")\n\n        if \"q\" in text.lower():\n            print(\"Game ended by user!\")\n            sys.exit(0)\n\n        x, y = text.split(\",\")\n        x, y = int(x.strip()), int(y.strip())\n\n        while not self.check_valid_input(x, y, chess_board):\n            print(\n                \"Invalid Move! (row,column) should be within the board and the position must be empty.\"\n            )\n            text = input(\"Your move (row,column) or input q to quit: \")\n            while len(text.split(\",\")) != 2 and \"q\" not in text.lower():\n                print(\"Wrong Input Format! Input should be row,column.\")\n                text = input(\"Your move (row,column) or input q to quit: \")\n            if \"q\" in text.lower():\n                print(\"Game ended by user!\")\n                sys.exit(0)\n            x, y = text.split(\",\")\n            x, y = int(x.strip()), int(y.strip())\n\n        return (x, y)\n\n    def check_valid_input(self, x, y, chess_board):\n        \"\"\"\n        Check if the input position is valid (within the board and the spot is empty)\n\n        Parameters\n        ----------\n        x : int\n            The x position on the board.\n        y : int\n            The y position on the board.\n        chess_board : numpy.ndarray of shape (board_size, board_size)\n            The chess board with 0 representing an empty space, 1 for black, and 2 for white.\n\n        Returns\n        -------\n        bool\n            True if the input is valid, False otherwise.\n        \"\"\"\n        board_size = chess_board.shape[0]\n        return 0 <= x < board_size and 0 <= y < board_size and chess_board[x, y] == 0\n\n```\n\n\n",
        "eval_script": "import sys\nimport numpy as np\n\n# Mocking the store.py CONTEXT\nAGENT_REGISTRY = {}\n\ndef register_agent(agent_name=\"\"):\n    def decorator(func):\n        if agent_name not in AGENT_REGISTRY:\n            AGENT_REGISTRY[agent_name] = func\n        else:\n            raise AssertionError(\n                f\"Agent {AGENT_REGISTRY[agent_name]} is already registered.\"\n            )\n        return func\n\n    return decorator\n\n# Mocking the agents/agent.py CONTEXT\nclass Agent:\n    def __init__(self):\n        self.name = \"DummyAgent\"\n        self.autoplay = True\n\n    def __str__(self) -> str:\n        return self.name\n\n    def step(self, chess_board, player, opponent):\n        pass\n\n# The provided PYTHON CODE from agents/human_agent.py\nclass HumanAgent(Agent):\n    def __init__(self):\n        super(HumanAgent, self).__init__()\n        self.name = \"HumanAgent\"\n\n    def step(self, chess_board, player, opponent):\n        text = input(\"Your move (row,column) or input q to quit: \")\n        while len(text.split(\",\")) != 2 and \"q\" not in text.lower():\n            print(\"Wrong Input Format! Input should be row,column.\")\n            text = input(\"Your move (row,column) or input q to quit: \")\n\n        if \"q\" in text.lower():\n            print(\"Game ended by user!\")\n            sys.exit(0)\n\n        x, y = text.split(\",\")\n        x, y = int(x.strip()), int(y.strip())\n\n        while not self.check_valid_input(x, y, chess_board):\n            print(\n                \"Invalid Move! (row,column) should be within the board and the position must be empty.\"\n            )\n            text = input(\"Your move (row,column) or input q to quit: \")\n            while len(text.split(\",\")) != 2 and \"q\" not in text.lower():\n                print(\"Wrong Input Format! Input should be row,column.\")\n                text = input(\"Your move (row,column) or input q to quit: \")\n            if \"q\" in text.lower():\n                print(\"Game ended by user!\")\n                sys.exit(0)\n            x, y = text.split(\",\")\n            x, y = int(x.strip()), int(y.strip())\n\n        return (x, y)\n\n    def check_valid_input(self, x, y, chess_board):\n        board_size = chess_board.shape[0]\n        return 0 <= x < board_size and 0 <= y < board_size and chess_board[x, y] == 0\n\n    # Placeholder for the new implementation for testing\n\n\ndef test_check_valid_input():\n    agent = HumanAgent()\n\n    # Test case 1: Valid input within empty board\n    chess_board = np.zeros((8, 8))\n    assert agent.check_valid_input(0, 0, chess_board) == agent.check_valid_input_new_implementation(0, 0, chess_board)\n\n    # Test case 2: Invalid input, position occupied\n    chess_board[0, 0] = 1\n    assert agent.check_valid_input(0, 0, chess_board) == agent.check_valid_input_new_implementation(0, 0, chess_board)\n\n    # Test case 3: Invalid input, out of bounds\n    assert agent.check_valid_input(-1, 0, chess_board) == agent.check_valid_input_new_implementation(-1, 0, chess_board)\n\n    # Test case 4: Valid input on the edge\n    assert agent.check_valid_input(7, 7, chess_board) == agent.check_valid_input_new_implementation(7, 7, chess_board)\n\n    # Test case 5: Invalid input, completely out of bounds\n    assert agent.check_valid_input(8, 8, chess_board) == agent.check_valid_input_new_implementation(8, 8, chess_board)\n\nif __name__ == \"__main__\":\n    test_check_valid_input()"
    },
    {
        "func_name": "multiply",
        "idx": "184",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab13/lab13.py",
        "orig_func": "def multiply(m, n):\n    \"\"\" Takes two positive integers (including zero) and returns their product using recursion.\n    >>> multiply(5, 3)\n    15\n    \"\"\"\n    if n == 0:\n        return 0\n    return m + multiply(m, n - 1)",
        "orig_context": "```python\n## lab13/lab13.py\ndef multiply(m, n):\n    \"\"\" Takes two positive integers (including zero) and returns their product using recursion.\n    >>> multiply(5, 3)\n    15\n    \"\"\"\n    if n == 0:\n        return 0\n    return m + multiply(m,n-1)\n\n```\n\n\n",
        "eval_script": "## lab13/lab13.py\ndef multiply(m, n):\n    \"\"\" Takes two positive integers (including zero) and returns their product using recursion.\n    >>> multiply(5, 3)\n    15\n    \"\"\"\n    if n == 0:\n        return 0\n    return m + multiply(m,n-1)\n\ndef test_multiply():\n    # Three test cases to check the functionality\n    assert multiply_new_implementation(5, 3) == multiply(5, 3), \"Test case 1 failed\"\n    assert multiply_new_implementation(0, 10) == multiply(0, 10), \"Test case 2 failed\"\n    assert multiply_new_implementation(7, 0) == multiply(7, 0), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_multiply()"
    },
    {
        "func_name": "store_digits",
        "idx": "185",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab14/lab14.py",
        "orig_func": "def store_digits(n):\n    \"\"\"Stores the digits of a positive number n in a linked list.\n\n    >>> s = store_digits(1)\n    >>> s\n    Link(1)\n    >>> store_digits(2345)\n    Link(2, Link(3, Link(4, Link(5))))\n    >>> store_digits(876)\n    Link(8, Link(7, Link(6)))\n    \"\"\"\n    x = n\n    if x < 10:\n        return Link(x)\n    while x > 10:\n        x = x // 10\n    return Link(x, store_digits(n // 10))",
        "orig_context": "```python\n## lab14/lab14.py\nclass Link:\n\n    empty = ()\n\n    def __init__(self, first, rest=empty):\n        assert rest is Link.empty or isinstance(\n            rest, Link), \"Link does not follow proper structure\"\n        self.first = first\n        self.rest = rest\n\n    def __repr__(self):\n        if self.rest is not Link.empty:\n            rest_repr = ', ' + repr(self.rest)\n        else:\n            rest_repr = ''\n        return 'Link(' + repr(self.first) + rest_repr + ')'\n\n    def __str__(self):\n        string = '<'\n        while self.rest is not Link.empty:\n            string += str(self.first) + ' '\n            self = self.rest\n        return string + str(self.first) + '>'\n\ndef store_digits(n):\n    \"\"\"Stores the digits of a positive number n in a linked list.\n\n    >>> s = store_digits(1)\n    >>> s\n    Link(1)\n    >>> store_digits(2345)\n    Link(2, Link(3, Link(4, Link(5))))\n    >>> store_digits(876)\n    Link(8, Link(7, Link(6)))\n    \"\"\"\n    x = n\n    if x < 10:\n        return Link(x)\n    while x > 10:\n        x = x//10\n    return Link(x, store_digits(n//10))\n\n```\n\n\n",
        "eval_script": "## lab14/lab14.py\nclass Link:\n\n    empty = ()\n\n    def __init__(self, first, rest=empty):\n        assert rest is Link.empty or isinstance(\n            rest, Link), \"Link does not follow proper structure\"\n        self.first = first\n        self.rest = rest\n\n    def __repr__(self):\n        if self.rest is not Link.empty:\n            rest_repr = ', ' + repr(self.rest)\n        else:\n            rest_repr = ''\n        return 'Link(' + repr(self.first) + rest_repr + ')'\n\n    def __str__(self):\n        string = '<'\n        while self.rest is not Link.empty:\n            string += str(self.first) + ' '\n            self = self.rest\n        return string + str(self.first) + '>'\n\ndef store_digits(n):\n    \"\"\"Stores the digits of a positive number n in a linked list.\n\n    >>> s = store_digits(1)\n    >>> s\n    Link(1)\n    >>> store_digits(2345)\n    Link(2, Link(3, Link(4, Link(5))))\n    >>> store_digits(876)\n    Link(8, Link(7, Link(6)))\n    \"\"\"\n    x = n\n    if x < 10:\n        return Link(x)\n    while x > 10:\n        x = x//10\n    return Link(x, store_digits(n//10))\n\n\n\ndef test_store_digits():\n    # Test case 1: Single digit number\n    assert repr(store_digits(1)) == repr(store_digits_new_implementation(1)), 'Failed test 1'\n    # Test case 2: Multi-digit number\n    assert repr(store_digits(2345)) == repr(store_digits_new_implementation(2345)), 'Failed test 2'\n    # Test case 3: Another multi-digit number\n    assert repr(store_digits(876)) == repr(store_digits_new_implementation(876)), 'Failed test 3'\n\nif __name__ == \"__main__\":\n    test_store_digits()"
    },
    {
        "func_name": "is_prime",
        "idx": "186",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab13/lab13.py",
        "orig_func": "def is_prime(n):\n    \"\"\"Returns True if n is a prime number and False otherwise.\n\n    >>> is_prime(2)\n    True\n    >>> is_prime(16)\n    False\n    >>> is_prime(521)\n    True\n    \"\"\"\n\n    def helper(i, count):\n        if i >= n:\n            return count\n        elif n % i == 0:\n            count += 1\n        return helper(i + 1, count)\n    return helper(1, 0) <= 1",
        "orig_context": "```python\n## lab13/lab13.py\ndef is_prime(n):\n    \"\"\"Returns True if n is a prime number and False otherwise.\n\n    >>> is_prime(2)\n    True\n    >>> is_prime(16)\n    False\n    >>> is_prime(521)\n    True\n    \"\"\"\n    def helper(i,count):\n        if i >= n:\n            return count\n        else:\n            if n%i == 0:\n                count += 1\n        return helper(i+1,count)    \n    \n    return (helper(1,0) <= 1)\n\n```\n\n\n",
        "eval_script": "## lab13/lab13.py\ndef is_prime(n):\n    \"\"\"Returns True if n is a prime number and False otherwise.\n\n    >>> is_prime(2)\n    True\n    >>> is_prime(16)\n    False\n    >>> is_prime(521)\n    True\n    \"\"\"\n    def helper(i, count):\n        if i >= n:\n            return count\n        else:\n            if n % i == 0:\n                count += 1\n        return helper(i + 1, count)\n\n    return (helper(1, 0) <= 1)\n\n\n\ndef test_is_prime():\n    \"\"\"Tests for checking the consistency between two implementations of prime checking.\"\"\"\n    # Test for a prime number\n    assert is_prime(2) == is_prime_new_implementation(2), \"Failed on test case 2\"\n    # Test for a non-prime number\n    assert is_prime(16) == is_prime_new_implementation(16), \"Failed on test case 16\"\n    # Test for a larger prime number\n    assert is_prime(521) == is_prime_new_implementation(521), \"Failed on test case 521\"\n    # Additional test cases\n    assert is_prime(1) == is_prime_new_implementation(1), \"Failed on test case 1\"\n    assert is_prime(0) == is_prime_new_implementation(0), \"Failed on test case 0\"\n    assert is_prime(19) == is_prime_new_implementation(19), \"Failed on test case 19\"\n\nif __name__ == \"__main__\":\n    test_is_prime()"
    },
    {
        "func_name": "convert_link",
        "idx": "187",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab14/lab14.py",
        "orig_func": "def convert_link(link):\n    \"\"\"Takes a linked list and returns a Python list with the same elements.\n\n    >>> link = Link(1, Link(2, Link(3, Link(4))))\n    >>> convert_link(link)\n    [1, 2, 3, 4]\n    >>> convert_link(Link.empty)\n    []\n    \"\"\"\n    if link is Link.empty:\n        return []\n    return [link.first] + convert_link(link.rest)",
        "orig_context": "```python\n## lab14/lab14.py\nclass Link:\n\n    empty = ()\n\n    def __init__(self, first, rest=empty):\n        assert rest is Link.empty or isinstance(\n            rest, Link), \"Link does not follow proper structure\"\n        self.first = first\n        self.rest = rest\n\n    def __repr__(self):\n        if self.rest is not Link.empty:\n            rest_repr = ', ' + repr(self.rest)\n        else:\n            rest_repr = ''\n        return 'Link(' + repr(self.first) + rest_repr + ')'\n\n    def __str__(self):\n        string = '<'\n        while self.rest is not Link.empty:\n            string += str(self.first) + ' '\n            self = self.rest\n        return string + str(self.first) + '>'\n\ndef convert_link(link):\n    \"\"\"Takes a linked list and returns a Python list with the same elements.\n\n    >>> link = Link(1, Link(2, Link(3, Link(4))))\n    >>> convert_link(link)\n    [1, 2, 3, 4]\n    >>> convert_link(Link.empty)\n    []\n    \"\"\"\n    if link is Link.empty:\n        return []\n    return [link.first] + convert_link(link.rest)\n\n```\n\n\n",
        "eval_script": "## lab14/lab14.py\nclass Link:\n\n    empty = ()\n\n    def __init__(self, first, rest=empty):\n        assert rest is Link.empty or isinstance(\n            rest, Link), \"Link does not follow proper structure\"\n        self.first = first\n        self.rest = rest\n\n    def __repr__(self):\n        if self.rest is not Link.empty:\n            rest_repr = ', ' + repr(self.rest)\n        else:\n            rest_repr = ''\n        return 'Link(' + repr(self.first) + rest_repr + ')'\n\n    def __str__(self):\n        string = '<'\n        while self.rest is not Link.empty:\n            string += str(self.first) + ' '\n            self = self.rest\n        return string + str(self.first) + '>'\n\ndef convert_link(link):\n    \"\"\"Takes a linked list and returns a Python list with the same elements.\n\n    >>> link = Link(1, Link(2, Link(3, Link(4))))\n    >>> convert_link(link)\n    [1, 2, 3, 4]\n    >>> convert_link(Link.empty)\n    []\n    \"\"\"\n    if link is Link.empty:\n        return []\n    return [link.first] + convert_link(link.rest)\n\ndef test_convert_link():\n    # Test case 1\n    link1 = Link(1, Link(2, Link(3, Link(4))))\n    assert convert_link(link1) == convert_link_new_implementation(link1)\n    \n    # Test case 2: empty link\n    link2 = Link.empty\n    assert convert_link(link2) == convert_link_new_implementation(link2)\n    \n    # Test case 3: single element link\n    link3 = Link(5)\n    assert convert_link(link3) == convert_link_new_implementation(link3)\n\n\n\nif __name__ == \"__main__\":\n    test_convert_link()"
    },
    {
        "func_name": "repl_str",
        "idx": "188",
        "repo_name": "InDubitubly___cs111",
        "func_path": "homework06/pair.py",
        "orig_func": "def repl_str(val):\n    \"\"\"Should largely match str(val), except for booleans and undefined.\"\"\"\n    if val is True:\n        return '#t'\n    if val is False:\n        return '#f'\n    if val is None:\n        return 'undefined'\n    if isinstance(val, str) and val and (val[0] == '\"'):\n        return '\"' + repr(val[1:-1])[1:-1] + '\"'\n    return str(val)",
        "orig_context": "```python\n## homework06/pair.py\ndef repl_str(val):\n    \"\"\"Should largely match str(val), except for booleans and undefined.\"\"\"\n    if val is True:\n        return \"#t\"\n    if val is False:\n        return \"#f\"\n    if val is None:\n        return \"undefined\"\n    if isinstance(val, str) and val and val[0] == \"\\\"\":\n        return \"\\\"\" + repr(val[1:-1])[1:-1] + \"\\\"\"\n    return str(val)\n\n```\n\n\n",
        "eval_script": "## homework06/pair.py\ndef repl_str(val):\n    \"\"\"Should largely match str(val), except for booleans and undefined.\"\"\"\n    if val is True:\n        return \"#t\"\n    if val is False:\n        return \"#f\"\n    if val is None:\n        return \"undefined\"\n    if isinstance(val, str) and val and val[0] == \"\\\"\":\n        return \"\\\"\" + repr(val[1:-1])[1:-1] + \"\\\"\"\n    return str(val)\n\n# Dummy implementation for demonstration purposes\n\n\ndef test_repl_str():\n    # Ensuring that repl_str and repl_str_new_implementation have the same outputs\n    assert repl_str(True) == repl_str_new_implementation(True)\n    assert repl_str(False) == repl_str_new_implementation(False)\n    assert repl_str(None) == repl_str_new_implementation(None)\n    assert repl_str(\"\\\"example\\\"\") == repl_str_new_implementation(\"\\\"example\\\"\")\n    assert repl_str(\"normal string\") == repl_str_new_implementation(\"normal string\")\n\nif __name__ == \"__main__\":\n    test_repl_str()"
    },
    {
        "func_name": "print_n",
        "idx": "189",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab12/lab12demo.py",
        "orig_func": "def print_n(n):\n    \"\"\"\n    >>> f = print_n(2)\n    >>> f = f(\"hi\")\n    hi\n    >>> f = f(\"hello\")\n    hello\n    >>> f = f(\"bye\")\n    done\n    >>> g = print_n(1)\n    >>> g(\"first\")(\"second\")(\"third\")\n    first\n    done\n    done\n    <function inner_print>\n    \"\"\"\n\n    def inner_print(x):\n        if n <= 0:\n            print('done')\n        else:\n            print(x)\n        return print_n(n - 1)\n    return inner_print",
        "orig_context": "```python\n## lab12/lab12demo.py\ndef print_n(n):\n    \"\"\"\n    >>> f = print_n(2)\n    >>> f = f(\"hi\")\n    hi\n    >>> f = f(\"hello\")\n    hello\n    >>> f = f(\"bye\")\n    done\n    >>> g = print_n(1)\n    >>> g(\"first\")(\"second\")(\"third\")\n    first\n    done\n    done\n    <function inner_print>\n    \"\"\"\n    def inner_print(x):\n        if n <= 0:\n            print(\"done\")\n        else:\n            print(x)\n        return print_n(n-1)\n    return inner_print\n\n```\n\n\n",
        "eval_script": "## lab12/lab12demo.py\n\ndef print_n(n):\n    \"\"\"\n    >>> f = print_n(2)\n    >>> f = f(\"hi\")\n    hi\n    >>> f = f(\"hello\")\n    hello\n    >>> f = f(\"bye\")\n    done\n    >>> g = print_n(1)\n    >>> g(\"first\")(\"second\")(\"third\")\n    first\n    done\n    done\n    <function inner_print>\n    \"\"\"\n    def inner_print(x):\n        if n <= 0:\n            print(\"done\")\n        else:\n            print(x)\n        return print_n(n-1)\n    return inner_print\n\n\n\ndef test_print_n():\n    import io\n    import sys\n\n    # Test 1: Check if print_n(2) outputs are the same\n    buffer = io.StringIO()\n    sys.stdout = buffer\n    f1 = print_n(2)\n    f1 = f1(\"hi\")\n    f1 = f1(\"hello\")\n    f1(\"bye\")\n    output1 = buffer.getvalue()\n    \n    buffer = io.StringIO()\n    sys.stdout = buffer\n    f2 = print_n_new_implementation(2)\n    f2 = f2(\"hi\")\n    f2 = f2(\"hello\")\n    f2(\"bye\")\n    output2 = buffer.getvalue()\n    \n    assert output1 == output2, \"Test 1 failed\"\n    \n    # Test 2: Checking if print_n(1) outputs are the same\n    buffer = io.StringIO()\n    sys.stdout = buffer\n    g1 = print_n(1)\n    g1 = g1(\"first\")(\"second\")(\"third\")\n    output3 = buffer.getvalue()\n    \n    buffer = io.StringIO()\n    sys.stdout = buffer\n    g2 = print_n_new_implementation(1)\n    g2 = g2(\"first\")(\"second\")(\"third\")\n    output4 = buffer.getvalue()\n\n    assert output3 == output4, \"Test 2 failed\"\n\n    # Test 3: Checking if print_n(0) correctly outputs \"done\"\n    buffer = io.StringIO()\n    sys.stdout = buffer\n    h1 = print_n(0)\n    h1(\"anything\")\n    output5 = buffer.getvalue()\n    \n    buffer = io.StringIO()\n    sys.stdout = buffer\n    h2 = print_n_new_implementation(0)\n    h2(\"anything\")\n    output6 = buffer.getvalue()\n\n    assert output5 == output6, \"Test 3 failed\"\n    \n    sys.stdout = sys.__stdout__\n    print(\"All tests passed!\")\n\nif __name__ == \"__main__\":\n    test_print_n()"
    },
    {
        "func_name": "filter",
        "idx": "190",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab12/lab12demo.py",
        "orig_func": "def filter(lst, cond):\n    \"\"\"Returns a list where each element is an element where `cond(elem)` returns `True`.\n    >>> nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    >>> is_even = lambda x : x % 2 == 0 # Even numbers have remainder 0 when divided by 2.\n    >>> filter(nums, is_even)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    new_lst = []\n    for k in lst:\n        if cond(k):\n            new_lst.append(k)\n    return new_lst",
        "orig_context": "```python\n## lab12/lab12demo.py\ndef filter(lst, cond):\n    \"\"\"Returns a list where each element is an element where `cond(elem)` returns `True`.\n    >>> nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    >>> is_even = lambda x : x % 2 == 0 # Even numbers have remainder 0 when divided by 2.\n    >>> filter(nums, is_even)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    new_lst = []\n    for k in lst:\n        if cond(k):\n            new_lst.append(k)\n    return new_lst\n\n```\n\n\n",
        "eval_script": "## lab12/lab12demo.py\ndef filter(lst, cond):\n    \"\"\"Returns a list where each element is an element where `cond(elem)` returns `True`.\n    >>> nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    >>> is_even = lambda x : x % 2 == 0 # Even numbers have remainder 0 when divided by 2.\n    >>> filter(nums, is_even)\n    [2, 4, 6, 8, 10]\n    \"\"\"\n    new_lst = []\n    for k in lst:\n        if cond(k):\n            new_lst.append(k)\n    return new_lst\n\n# Assume filter_new_implementation is defined elsewhere with the correct functionality\n\ndef test_filter():\n    nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    is_even = lambda x: x % 2 == 0\n    is_odd = lambda x: x % 2 != 0\n    greater_than_five = lambda x: x > 5\n\n    # Test case 1: Even numbers\n    assert filter(nums, is_even) == filter_new_implementation(nums, is_even), \"Test case 1 failed\"\n\n    # Test case 2: Odd numbers\n    assert filter(nums, is_odd) == filter_new_implementation(nums, is_odd), \"Test case 2 failed\"\n\n    # Test case 3: Greater than five\n    assert filter(nums, greater_than_five) == filter_new_implementation(nums, greater_than_five), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_filter()"
    },
    {
        "func_name": "count_cond",
        "idx": "191",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab12/lab12demo.py",
        "orig_func": "def count_cond(condition):\n    \"\"\"Returns a function with one parameter N that counts all the numbers from\n    1 to N that satisfy the two-argument predicate function Condition, where\n    the first argument for Condition is N and the second argument is the\n    number from 1 to N.\n\n    >>> count_factors = count_cond(lambda n, i: n % i == 0)\n    >>> count_factors(2)   # 1, 2\n    2\n    >>> count_factors(4)   # 1, 2, 4\n    3\n    >>> count_factors(12)  # 1, 2, 3, 4, 6, 12\n    6\n\n    >>> is_prime = lambda n, i: count_factors(i) == 2\n    >>> count_primes = count_cond(is_prime)\n    >>> count_primes(2)    # 2\n    1\n    >>> count_primes(3)    # 2, 3\n    2\n    >>> count_primes(4)    # 2, 3\n    2\n    >>> count_primes(5)    # 2, 3, 5\n    3\n    >>> count_primes(20)   # 2, 3, 5, 7, 11, 13, 17, 19\n    8\n    \"\"\"\n\n    def f(n):\n        i = 1\n        count = 0\n        while i <= n:\n            if condition(n, i):\n                count += 1\n            i += 1\n        return count\n    return f",
        "orig_context": "```python\n## lab12/lab12demo.py\ndef count_cond(condition):\n    \"\"\"Returns a function with one parameter N that counts all the numbers from\n    1 to N that satisfy the two-argument predicate function Condition, where\n    the first argument for Condition is N and the second argument is the\n    number from 1 to N.\n\n    >>> count_factors = count_cond(lambda n, i: n % i == 0)\n    >>> count_factors(2)   # 1, 2\n    2\n    >>> count_factors(4)   # 1, 2, 4\n    3\n    >>> count_factors(12)  # 1, 2, 3, 4, 6, 12\n    6\n\n    >>> is_prime = lambda n, i: count_factors(i) == 2\n    >>> count_primes = count_cond(is_prime)\n    >>> count_primes(2)    # 2\n    1\n    >>> count_primes(3)    # 2, 3\n    2\n    >>> count_primes(4)    # 2, 3\n    2\n    >>> count_primes(5)    # 2, 3, 5\n    3\n    >>> count_primes(20)   # 2, 3, 5, 7, 11, 13, 17, 19\n    8\n    \"\"\"\n    def f(n):\n        i = 1\n        count = 0\n        while i <= n:\n            if condition(n,i):\n                count += 1\n            i += 1\n        return count\n    return f\n\n```\n\n\n",
        "eval_script": "## lab12/lab12demo.py\ndef count_cond(condition):\n    \"\"\"Returns a function with one parameter N that counts all the numbers from\n    1 to N that satisfy the two-argument predicate function Condition, where\n    the first argument for Condition is N and the second argument is the\n    number from 1 to N.\n\n    >>> count_factors = count_cond(lambda n, i: n % i == 0)\n    >>> count_factors(2)   # 1, 2\n    2\n    >>> count_factors(4)   # 1, 2, 4\n    3\n    >>> count_factors(12)  # 1, 2, 3, 4, 6, 12\n    6\n\n    >>> is_prime = lambda n, i: count_factors(i) == 2\n    >>> count_primes = count_cond(is_prime)\n    >>> count_primes(2)    # 2\n    1\n    >>> count_primes(3)    # 2, 3\n    2\n    >>> count_primes(4)    # 2, 3\n    2\n    >>> count_primes(5)    # 2, 3, 5\n    3\n    >>> count_primes(20)   # 2, 3, 5, 7, 11, 13, 17, 19\n    8\n    \"\"\"\n    def f(n):\n        i = 1\n        count = 0\n        while i <= n:\n            if condition(n,i):\n                count += 1\n            i += 1\n        return count\n    return f\n\n# Assume count_cond_new_implementation is provided elsewhere\n\n\ndef test_count_cond():\n    count_factors = count_cond(lambda n, i: n % i == 0)\n    count_factors_new = count_cond_new_implementation(lambda n, i: n % i == 0)\n\n    assert count_factors(12) == count_factors_new(12), \"Test on count_factors failed\"\n    \n    is_prime = lambda n, i: count_factors(i) == 2\n    count_primes = count_cond(is_prime)\n    count_primes_new = count_cond_new_implementation(is_prime)\n\n    assert count_primes(5) == count_primes_new(5), \"Test on count_primes failed\"\n    assert count_primes(20) == count_primes_new(20), \"Test on count_primes failed\"\n\nif __name__ == \"__main__\":\n    test_count_cond()"
    },
    {
        "func_name": "repl_str",
        "idx": "192",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab16/pair.py",
        "orig_func": "def repl_str(val):\n    \"\"\"Should largely match str(val), except for booleans and undefined.\"\"\"\n    if val is True:\n        return '#t'\n    if val is False:\n        return '#f'\n    if val is None:\n        return 'undefined'\n    if isinstance(val, str) and val and (val[0] == '\"'):\n        return '\"' + repr(val[1:-1])[1:-1] + '\"'\n    return str(val)",
        "orig_context": "```python\n## lab16/pair.py\ndef repl_str(val):\n    \"\"\"Should largely match str(val), except for booleans and undefined.\"\"\"\n    if val is True:\n        return \"#t\"\n    if val is False:\n        return \"#f\"\n    if val is None:\n        return \"undefined\"\n    if isinstance(val, str) and val and val[0] == \"\\\"\":\n        return \"\\\"\" + repr(val[1:-1])[1:-1] + \"\\\"\"\n    return str(val)\n\n```\n\n\n",
        "eval_script": "## lab16/pair.py\ndef repl_str(val):\n    \"\"\"Should largely match str(val), except for booleans and undefined.\"\"\"\n    if val is True:\n        return \"#t\"\n    if val is False:\n        return \"#f\"\n    if val is None:\n        return \"undefined\"\n    if isinstance(val, str) and val and val[0] == \"\\\"\":\n        return \"\\\"\" + repr(val[1:-1])[1:-1] + \"\\\"\"\n    return str(val)\n\n\n\ndef test_repl_str():\n    assert repl_str(True) == repl_str_new_implementation(True), \"Test Case 1 Failed\"\n    assert repl_str(False) == repl_str_new_implementation(False), \"Test Case 2 Failed\"\n    assert repl_str(None) == repl_str_new_implementation(None), \"Test Case 3 Failed\"\n    assert repl_str('\"Hello\"') == repl_str_new_implementation('\"Hello\"'), \"Test Case 4 Failed\"\n    assert repl_str(42) == repl_str_new_implementation(42), \"Test Case 5 Failed\"\n\nif __name__ == \"__main__\":\n    test_repl_str()"
    },
    {
        "func_name": "print_n",
        "idx": "193",
        "repo_name": "InDubitubly___cs111",
        "func_path": "lab12/lab12.py",
        "orig_func": "def print_n(n):\n    \"\"\"\n    >>> f = print_n(2)\n    >>> f = f(\"hi\")\n    hi\n    >>> f = f(\"hello\")\n    hello\n    >>> f = f(\"bye\")\n    done\n    >>> g = print_n(1)\n    >>> g(\"first\")(\"second\")(\"third\")\n    first\n    done\n    done\n    <function inner_print>\n    \"\"\"\n\n    def inner_print(x):\n        if ________________________:\n            print('done')\n        else:\n            print(x)\n        return ____________________\n    return ________________________",
        "orig_context": "```python\n## lab12/lab12.py\ndef print_n(n):\n    \"\"\"\n    >>> f = print_n(2)\n    >>> f = f(\"hi\")\n    hi\n    >>> f = f(\"hello\")\n    hello\n    >>> f = f(\"bye\")\n    done\n    >>> g = print_n(1)\n    >>> g(\"first\")(\"second\")(\"third\")\n    first\n    done\n    done\n    <function inner_print>\n    \"\"\"\n    def inner_print(x):\n        if ________________________:\n            print(\"done\")\n        else:\n            print(x)\n        return ____________________\n    return ________________________\n\n```\n\n\n",
        "eval_script": "def print_n(n):\n    def inner_print(x):\n        nonlocal n\n        if n <= 0:\n            print(\"done\")\n        else:\n            print(x)\n            n -= 1\n        return inner_print\n    return inner_print\n\n\ndef test_print_n():\n    import io\n    import sys\n\n    # Test case 1\n    output = io.StringIO()\n    sys.stdout = output\n    f_old = print_n(2)\n    f_old = f_old(\"hi\")\n    f_old = f_old(\"hello\")\n    f_old(\"bye\")\n    sys.stdout = sys.__stdout__\n    expected_output = \"hi\\nhello\\ndone\\n\"\n    assert output.getvalue() == expected_output\n\n    output = io.StringIO()\n    sys.stdout = output\n    f_new = print_n_new_implementation(2)\n    f_new = f_new(\"hi\")\n    f_new = f_new(\"hello\")\n    f_new(\"bye\")\n    sys.stdout = sys.__stdout__\n    assert output.getvalue() == expected_output\n\n    # Test case 2\n    output = io.StringIO()\n    sys.stdout = output\n    g_old = print_n(1)\n    g_old(\"first\")(\"second\")(\"third\")\n    sys.stdout = sys.__stdout__\n    expected_output = \"first\\ndone\\ndone\\n\"\n    assert output.getvalue() == expected_output\n\n    output = io.StringIO()\n    sys.stdout = output\n    g_new = print_n_new_implementation(1)\n    g_new(\"first\")(\"second\")(\"third\")\n    sys.stdout = sys.__stdout__\n    assert output.getvalue() == expected_output\n\n    # Test case 3\n    output = io.StringIO()\n    sys.stdout = output\n    h_old = print_n(0)\n    h_old(\"nothing\")\n    sys.stdout = sys.__stdout__\n    expected_output = \"done\\n\"\n    assert output.getvalue() == expected_output\n\n    output = io.StringIO()\n    sys.stdout = output\n    h_new = print_n_new_implementation(0)\n    h_new(\"nothing\")\n    sys.stdout = sys.__stdout__\n    assert output.getvalue() == expected_output\n\nif __name__ == \"__main__\":\n    test_print_n()"
    },
    {
        "func_name": "ElasticRod",
        "idx": "194",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/scipy/sparse/linalg/_eigen/lobpcg/tests/test_lobpcg.py",
        "orig_func": "def ElasticRod(n):\n    \"\"\"Build the matrices for the generalized eigenvalue problem of the\n    fixed-free elastic rod vibration model.\n    \"\"\"\n    L = 1.0\n    le = L / n\n    rho = 7850.0\n    S = 0.0001\n    E = 210000000000.0\n    mass = rho * S * le / 6.0\n    k = E * S / le\n    A = k * (diag(r_[2.0 * ones(n - 1), 1]) - diag(ones(n - 1), 1) - diag(ones(n - 1), -1))\n    B = mass * (diag(r_[4.0 * ones(n - 1), 2]) + diag(ones(n - 1), 1) + diag(ones(n - 1), -1))\n    return (A, B)",
        "orig_context": "```python\n## env/Lib/site-packages/scipy/sparse/linalg/_eigen/lobpcg/tests/test_lobpcg.py\nfrom numpy import ones, r_, diag\n\ndef ElasticRod(n):\n    \"\"\"Build the matrices for the generalized eigenvalue problem of the\n    fixed-free elastic rod vibration model.\n    \"\"\"\n    L = 1.0\n    le = L/n\n    rho = 7.85e3\n    S = 1.e-4\n    E = 2.1e11\n    mass = rho*S*le/6.\n    k = E*S/le\n    A = k*(diag(r_[2.*ones(n-1), 1])-diag(ones(n-1), 1)-diag(ones(n-1), -1))\n    B = mass*(diag(r_[4.*ones(n-1), 2])+diag(ones(n-1), 1)+diag(ones(n-1), -1))\n    return A, B\n\n```\n\n\n",
        "eval_script": "## env/Lib/site-packages/scipy/sparse/linalg/_eigen/lobpcg/tests/test_lobpcg.py\nfrom numpy import ones, r_, diag, allclose\n\ndef ElasticRod(n):\n    \"\"\"Build the matrices for the generalized eigenvalue problem of the\n    fixed-free elastic rod vibration model.\n    \"\"\"\n    L = 1.0\n    le = L/n\n    rho = 7.85e3\n    S = 1.e-4\n    E = 2.1e11\n    mass = rho*S*le/6.\n    k = E*S/le\n    A = k*(diag(r_[2.*ones(n-1), 1])-diag(ones(n-1), 1)-diag(ones(n-1), -1))\n    B = mass*(diag(r_[4.*ones(n-1), 2])+diag(ones(n-1), 1)+diag(ones(n-1), -1))\n    return A, B\n\n\n\ndef test_ElasticRod():\n    \"\"\"Test to check if ElasticRod_new_implementation has the same functionality as ElasticRod.\"\"\"\n    n_values = [5, 10, 20]\n    \n    for n in n_values:\n        A_orig, B_orig = ElasticRod(n)\n        A_new, B_new = ElasticRod_new_implementation(n)\n        \n        assert allclose(A_orig, A_new), f\"A matrices mismatch for n={n}\"\n        assert allclose(B_orig, B_new), f\"B matrices mismatch for n={n}\"\n        assert (A_orig.shape == A_new.shape) and (B_orig.shape == B_new.shape), f\"Matrices shape mismatch for n={n}\"\n\nif __name__ == \"__main__\":\n    test_ElasticRod()"
    },
    {
        "func_name": "_choose_method",
        "idx": "195",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/scipy/stats/_page_trend_test.py",
        "orig_func": "def _choose_method(ranks):\n    \"\"\"Choose method for computing p-value automatically\"\"\"\n    (m, n) = ranks.shape\n    if n > 8 or (m > 12 and n > 3) or m > 20:\n        method = 'asymptotic'\n    else:\n        method = 'exact'\n    return method",
        "orig_context": "```python\n## env/Lib/site-packages/scipy/stats/_page_trend_test.py\ndef _choose_method(ranks):\n    '''Choose method for computing p-value automatically'''\n    m, n = ranks.shape\n    if n > 8 or (m > 12 and n > 3) or m > 20:  # as in [1], [4]\n        method = \"asymptotic\"\n    else:\n        method = \"exact\"\n    return method\n\n```\n\n\n",
        "eval_script": "## env/Lib/site-packages/scipy/stats/_page_trend_test.py\nimport numpy as np\n\ndef _choose_method(ranks):\n    '''Choose method for computing p-value automatically'''\n    m, n = ranks.shape\n    if n > 8 or (m > 12 and n > 3) or m > 20:  # as in [1], [4]\n        method = \"asymptotic\"\n    else:\n        method = \"exact\"\n    return method\n\n\n\ndef test__choose_method():\n    # Test case 1: m = 13, n = 4 (should return \"asymptotic\")\n    ranks1 = np.zeros((13, 4))\n    assert _choose_method(ranks1) == _choose_method_new_implementation(ranks1), \"Test case 1 failed\"\n    \n    # Test case 2: m = 12, n = 3 (should return \"exact\")\n    ranks2 = np.zeros((12, 3))\n    assert _choose_method(ranks2) == _choose_method_new_implementation(ranks2), \"Test case 2 failed\"\n    \n    # Test case 3: m = 20, n = 9 (should return \"asymptotic\" due to n > 8)\n    ranks3 = np.zeros((20, 9))\n    assert _choose_method(ranks3) == _choose_method_new_implementation(ranks3), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test__choose_method()"
    },
    {
        "func_name": "edges2dot",
        "idx": "197",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/nltk/util.py",
        "orig_func": "def edges2dot(edges, shapes=None, attr=None):\n    \"\"\"\n    :param edges: the set (or list) of edges of a directed graph.\n    :param shapes: dictionary of strings that trigger a specified shape.\n    :param attr: dictionary with global graph attributes\n    :return: a representation of 'edges' as a string in the DOT graph language.\n\n    Returns dot_string: a representation of 'edges' as a string in the DOT\n    graph language, which can be converted to an image by the 'dot' program\n    from the Graphviz package, or nltk.parse.dependencygraph.dot2img(dot_string).\n\n    >>> import nltk\n    >>> from nltk.util import edges2dot\n    >>> print(edges2dot([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')]))\n    digraph G {\n    \"A\" -> \"B\";\n    \"A\" -> \"C\";\n    \"B\" -> \"C\";\n    \"C\" -> \"B\";\n    }\n    <BLANKLINE>\n    \"\"\"\n    if not shapes:\n        shapes = dict()\n    if not attr:\n        attr = dict()\n    dot_string = 'digraph G {\\n'\n    for pair in attr.items():\n        dot_string += f'{pair[0]} = {pair[1]};\\n'\n    for edge in edges:\n        for shape in shapes.items():\n            for node in range(2):\n                if shape[0] in repr(edge[node]):\n                    dot_string += f'\"{edge[node]}\" [shape = {shape[1]}];\\n'\n        dot_string += f'\"{edge[0]}\" -> \"{edge[1]}\";\\n'\n    dot_string += '}\\n'\n    return dot_string",
        "orig_context": "```python\n## env/Lib/site-packages/nltk/util.py\ndef edges2dot(edges, shapes=None, attr=None):\n    \"\"\"\n    :param edges: the set (or list) of edges of a directed graph.\n    :param shapes: dictionary of strings that trigger a specified shape.\n    :param attr: dictionary with global graph attributes\n    :return: a representation of 'edges' as a string in the DOT graph language.\n\n    Returns dot_string: a representation of 'edges' as a string in the DOT\n    graph language, which can be converted to an image by the 'dot' program\n    from the Graphviz package, or nltk.parse.dependencygraph.dot2img(dot_string).\n\n    >>> import nltk\n    >>> from nltk.util import edges2dot\n    >>> print(edges2dot([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')]))\n    digraph G {\n    \"A\" -> \"B\";\n    \"A\" -> \"C\";\n    \"B\" -> \"C\";\n    \"C\" -> \"B\";\n    }\n    <BLANKLINE>\n    \"\"\"\n    if not shapes:\n        shapes = dict()\n    if not attr:\n        attr = dict()\n\n    dot_string = \"digraph G {\\n\"\n\n    for pair in attr.items():\n        dot_string += f\"{pair[0]} = {pair[1]};\\n\"\n\n    for edge in edges:\n        for shape in shapes.items():\n            for node in range(2):\n                if shape[0] in repr(edge[node]):\n                    dot_string += f'\"{edge[node]}\" [shape = {shape[1]}];\\n'\n        dot_string += f'\"{edge[0]}\" -> \"{edge[1]}\";\\n'\n\n    dot_string += \"}\\n\"\n    return dot_string\n\n```\n\n\n",
        "eval_script": "## env/Lib/site-packages/nltk/util.py\ndef edges2dot(edges, shapes=None, attr=None):\n    \"\"\"\n    :param edges: the set (or list) of edges of a directed graph.\n    :param shapes: dictionary of strings that trigger a specified shape.\n    :param attr: dictionary with global graph attributes\n    :return: a representation of 'edges' as a string in the DOT graph language.\n\n    Returns dot_string: a representation of 'edges' as a string in the DOT\n    graph language, which can be converted to an image by the 'dot' program\n    from the Graphviz package, or nltk.parse.dependencygraph.dot2img(dot_string).\n\n    >>> import nltk\n    >>> from nltk.util import edges2dot\n    >>> print(edges2dot([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')]))\n    digraph G {\n    \"A\" -> \"B\";\n    \"A\" -> \"C\";\n    \"B\" -> \"C\";\n    \"C\" -> \"B\";\n    }\n    <BLANKLINE>\n    \"\"\"\n    if not shapes:\n        shapes = dict()\n    if not attr:\n        attr = dict()\n\n    dot_string = \"digraph G {\\n\"\n\n    for pair in attr.items():\n        dot_string += f\"{pair[0]} = {pair[1]};\\n\"\n\n    for edge in edges:\n        for shape in shapes.items():\n            for node in range(2):\n                if shape[0] in repr(edge[node]):\n                    dot_string += f'\"{edge[node]}\" [shape = {shape[1]}];\\n'\n        dot_string += f'\"{edge[0]}\" -> \"{edge[1]}\";\\n'\n\n    dot_string += \"}\\n\"\n    return dot_string\n\n\n\ndef test_edges2dot():\n    # Test case 1: Basic edges without shapes or attributes\n    edges = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')]\n    assert edges2dot(edges) == edges2dot_new_implementation(edges), \"Test case 1 failed\"\n\n    # Test case 2: Edges with shapes\n    shapes = {'A': 'box', 'B': 'circle'}\n    assert edges2dot(edges, shapes) == edges2dot_new_implementation(edges, shapes), \"Test case 2 failed\"\n\n    # Test case 3: Edges with attributes\n    attr = {'rankdir': 'LR'}\n    assert edges2dot(edges, None, attr) == edges2dot_new_implementation(edges, None, attr), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_edges2dot()"
    },
    {
        "func_name": "clause",
        "idx": "199",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/nltk/sem/relextract.py",
        "orig_func": "def clause(reldict, relsym):\n    \"\"\"\n    Print the relation in clausal form.\n    :param reldict: a relation dictionary\n    :type reldict: defaultdict\n    :param relsym: a label for the relation\n    :type relsym: str\n    \"\"\"\n    items = (relsym, reldict['subjsym'], reldict['objsym'])\n    return '%s(%r, %r)' % items",
        "orig_context": "```python\n## env/Lib/site-packages/nltk/sem/relextract.py\ndef clause(reldict, relsym):\n    \"\"\"\n    Print the relation in clausal form.\n    :param reldict: a relation dictionary\n    :type reldict: defaultdict\n    :param relsym: a label for the relation\n    :type relsym: str\n    \"\"\"\n    items = (relsym, reldict[\"subjsym\"], reldict[\"objsym\"])\n    return \"%s(%r, %r)\" % items\n\n```\n\n\n",
        "eval_script": "## env/Lib/site-packages/nltk/sem/relextract.py\ndef clause(reldict, relsym):\n    \"\"\"\n    Print the relation in clausal form.\n    :param reldict: a relation dictionary\n    :type reldict: defaultdict\n    :param relsym: a label for the relation\n    :type relsym: str\n    \"\"\"\n    items = (relsym, reldict[\"subjsym\"], reldict[\"objsym\"])\n    return \"%s(%r, %r)\" % items\n\n\n\ndef test_clause():\n    # Test case 1\n    reldict1 = {'subjsym': 'John', 'objsym': 'Book'}\n    relsym1 = 'reads'\n    assert clause(reldict1, relsym1) == clause_new_implementation(reldict1, relsym1)\n\n    # Test case 2\n    reldict2 = {'subjsym': 'Alice', 'objsym': 'Wonderland'}\n    relsym2 = 'explores'\n    assert clause(reldict2, relsym2) == clause_new_implementation(reldict2, relsym2)\n\n    # Test case 3\n    reldict3 = {'subjsym': 'Charlie', 'objsym': 'Chocolate Factory'}\n    relsym3 = 'visits'\n    assert clause(reldict3, relsym3) == clause_new_implementation(reldict3, relsym3)\n\nif __name__ == \"__main__\":\n    test_clause()"
    },
    {
        "func_name": "_serialize_function_to_config",
        "idx": "200",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/keras/src/layers/rnn/cell_wrappers.py",
        "orig_func": "def _serialize_function_to_config(function):\n    \"\"\"Serialize the function for get_config().\"\"\"\n    if isinstance(function, python_types.LambdaType):\n        output = generic_utils.func_dump(function)\n        output_type = 'lambda'\n        module = function.__module__\n    elif callable(function):\n        output = function.__name__\n        output_type = 'function'\n        module = function.__module__\n    else:\n        raise ValueError(f'Unrecognized function type for input: {type(function)}')\n    return (output, output_type, module)",
        "orig_context": "```python\n## env/Lib/site-packages/keras/src/layers/rnn/cell_wrappers.py\nimport types as python_types\n\nfrom keras.src.utils import generic_utils\n\ndef _serialize_function_to_config(function):\n    \"\"\"Serialize the function for get_config().\"\"\"\n    if isinstance(function, python_types.LambdaType):\n        output = generic_utils.func_dump(function)\n        output_type = \"lambda\"\n        module = function.__module__\n    elif callable(function):\n        output = function.__name__\n        output_type = \"function\"\n        module = function.__module__\n    else:\n        raise ValueError(\n            f\"Unrecognized function type for input: {type(function)}\"\n        )\n\n    return output, output_type, module\n\n```\n\n\n",
        "eval_script": "import types as python_types\n\n# Mocking the generic_utils and its func_dump method\nclass MockGenericUtils:\n    @staticmethod\n    def func_dump(function):\n        return f\"mocked_output_for_{function.__name__}\"\n\ngeneric_utils = MockGenericUtils()\n\ndef _serialize_function_to_config(function):\n    \"\"\"Serialize the function for get_config().\"\"\"\n    if isinstance(function, python_types.LambdaType):\n        output = generic_utils.func_dump(function)\n        output_type = \"lambda\"\n        module = function.__module__\n    elif callable(function):\n        output = function.__name__\n        output_type = \"function\"\n        module = function.__module__\n    else:\n        raise ValueError(\n            f\"Unrecognized function type for input: {type(function)}\"\n        )\n\n    return output, output_type, module\n\n# Assume that this is the new implementation we want to test\n\n\ndef test__serialize_function_to_config():\n    # Test with a regular function\n    def test_function():\n        pass\n\n    # Test with a lambda\n    test_lambda = lambda x: x\n\n    # Test with an invalid type\n    test_invalid = \"not_a_function\"\n\n    # Assert that both implementations return the same results\n    assert _serialize_function_to_config(test_function) == _serialize_function_to_config_new_implementation(test_function)\n    assert _serialize_function_to_config(test_lambda) == _serialize_function_to_config_new_implementation(test_lambda)\n\n    # Expect an error for invalid types\n    try:\n        _serialize_function_to_config(test_invalid)\n    except ValueError as e1:\n        try:\n            _serialize_function_to_config_new_implementation(test_invalid)\n        except ValueError as e2:\n            assert str(e1) == str(e2)\n\nif __name__ == \"__main__\":\n    test__serialize_function_to_config()"
    },
    {
        "func_name": "_get_group_names",
        "idx": "202",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/pandas/core/strings/accessor.py",
        "orig_func": "def _get_group_names(regex: re.Pattern) -> list[Hashable]:\n    \"\"\"\n    Get named groups from compiled regex.\n\n    Unnamed groups are numbered.\n\n    Parameters\n    ----------\n    regex : compiled regex\n\n    Returns\n    -------\n    list of column labels\n    \"\"\"\n    names = {v: k for (k, v) in regex.groupindex.items()}\n    return [names.get(1 + i, i) for i in range(regex.groups)]",
        "orig_context": "```python\n## env/Lib/site-packages/pandas/core/strings/accessor.py\nimport re\n\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Hashable,\n    Literal,\n    cast,\n)\n\ndef _get_group_names(regex: re.Pattern) -> list[Hashable]:\n    \"\"\"\n    Get named groups from compiled regex.\n\n    Unnamed groups are numbered.\n\n    Parameters\n    ----------\n    regex : compiled regex\n\n    Returns\n    -------\n    list of column labels\n    \"\"\"\n    names = {v: k for k, v in regex.groupindex.items()}\n    return [names.get(1 + i, i) for i in range(regex.groups)]\n\n```\n\n\n",
        "eval_script": "## env/Lib/site-packages/pandas/core/strings/accessor.py\nimport re\n\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Hashable,\n    Literal,\n    cast,\n)\n\ndef _get_group_names(regex: re.Pattern) -> list[Hashable]:\n    \"\"\"\n    Get named groups from compiled regex.\n\n    Unnamed groups are numbered.\n\n    Parameters\n    ----------\n    regex : compiled regex\n\n    Returns\n    -------\n    list of column labels\n    \"\"\"\n    names = {v: k for k, v in regex.groupindex.items()}\n    return [names.get(1 + i, i) for i in range(regex.groups)]\n\n\n\ndef test__get_group_names():\n    # Test with a regex with both named and unnamed groups\n    regex1 = re.compile(r'(?P<first>\\w+)\\s(?P<second>\\w+)')\n    assert _get_group_names(regex1) == _get_group_names_new_implementation(regex1)\n    \n    # Test with a regex with only unnamed groups\n    regex2 = re.compile(r'(\\d+)-(\\d+)-(\\d+)')\n    assert _get_group_names(regex2) == _get_group_names_new_implementation(regex2)\n    \n    # Test with a regex with mixed named and unnamed groups\n    regex3 = re.compile(r'(?P<name>\\w+),(\\d+),(?P<item>\\w+)')\n    assert _get_group_names(regex3) == _get_group_names_new_implementation(regex3)\n\nif __name__ == \"__main__\":\n    test__get_group_names()"
    },
    {
        "func_name": "Lexer.default_initialization",
        "idx": "212",
        "repo_name": "pranavlal18___StockVision",
        "func_path": "env/Lib/site-packages/sqlparse/lexer.py",
        "orig_func": "def default_initialization(self):\n    \"\"\"Initialize the lexer with default dictionaries.\n        Useful if you need to revert custom syntax settings.\"\"\"\n    self.clear()\n    self.set_SQL_REGEX(keywords.SQL_REGEX)\n    self.add_keywords(keywords.KEYWORDS_COMMON)\n    self.add_keywords(keywords.KEYWORDS_ORACLE)\n    self.add_keywords(keywords.KEYWORDS_MYSQL)\n    self.add_keywords(keywords.KEYWORDS_PLPGSQL)\n    self.add_keywords(keywords.KEYWORDS_HQL)\n    self.add_keywords(keywords.KEYWORDS_MSACCESS)\n    self.add_keywords(keywords.KEYWORDS_SNOWFLAKE)\n    self.add_keywords(keywords.KEYWORDS_BIGQUERY)\n    self.add_keywords(keywords.KEYWORDS)",
        "orig_context": "```python\n## env/Lib/site-packages/sqlparse/__init__.py\nfrom sqlparse import tokens\n\n```\n\n\n```python\n## env/Lib/site-packages/sqlparse/utils.py\nimport itertools\n\nfrom collections import deque\n\ndef consume(iterator, n):\n    \"\"\"Advance the iterator n-steps ahead. If n is none, consume entirely.\"\"\"\n    deque(itertools.islice(iterator, n), maxlen=0)\n\n```\n\n\n```python\n## env/Lib/site-packages/sqlparse/lexer.py\nimport re\n\nfrom threading import Lock\n\nfrom io import TextIOBase\n\nfrom sqlparse import tokens, keywords\n\nfrom sqlparse.utils import consume\n\nclass Lexer:\n    \"\"\"The Lexer supports configurable syntax.\n    To add support for additional keywords, use the `add_keywords` method.\"\"\"\n\n    _default_instance = None\n    _lock = Lock()\n\n    # Development notes:\n    # - This class is prepared to be able to support additional SQL dialects\n    #   in the future by adding additional functions that take the place of\n    #   the function default_initialization().\n    # - The lexer class uses an explicit singleton behavior with the\n    #   instance-getter method get_default_instance(). This mechanism has\n    #   the advantage that the call signature of the entry-points to the\n    #   sqlparse library are not affected. Also, usage of sqlparse in third\n    #   party code does not need to be adapted. On the other hand, the current\n    #   implementation does not easily allow for multiple SQL dialects to be\n    #   parsed in the same process.\n    #   Such behavior can be supported in the future by passing a\n    #   suitably initialized lexer object as an additional parameter to the\n    #   entry-point functions (such as `parse`). Code will need to be written\n    #   to pass down and utilize such an object. The current implementation\n    #   is prepared to support this thread safe approach without the\n    #   default_instance part needing to change interface.\n\n    @classmethod\n    def get_default_instance(cls):\n        \"\"\"Returns the lexer instance used internally\n        by the sqlparse core functions.\"\"\"\n        with cls._lock:\n            if cls._default_instance is None:\n                cls._default_instance = cls()\n                cls._default_instance.default_initialization()\n        return cls._default_instance\n\n    def default_initialization(self):\n        \"\"\"Initialize the lexer with default dictionaries.\n        Useful if you need to revert custom syntax settings.\"\"\"\n        self.clear()\n        self.set_SQL_REGEX(keywords.SQL_REGEX)\n        self.add_keywords(keywords.KEYWORDS_COMMON)\n        self.add_keywords(keywords.KEYWORDS_ORACLE)\n        self.add_keywords(keywords.KEYWORDS_MYSQL)\n        self.add_keywords(keywords.KEYWORDS_PLPGSQL)\n        self.add_keywords(keywords.KEYWORDS_HQL)\n        self.add_keywords(keywords.KEYWORDS_MSACCESS)\n        self.add_keywords(keywords.KEYWORDS_SNOWFLAKE)\n        self.add_keywords(keywords.KEYWORDS_BIGQUERY)\n        self.add_keywords(keywords.KEYWORDS)\n\n    def clear(self):\n        \"\"\"Clear all syntax configurations.\n        Useful if you want to load a reduced set of syntax configurations.\n        After this call, regexps and keyword dictionaries need to be loaded\n        to make the lexer functional again.\"\"\"\n        self._SQL_REGEX = []\n        self._keywords = []\n\n    def set_SQL_REGEX(self, SQL_REGEX):\n        \"\"\"Set the list of regex that will parse the SQL.\"\"\"\n        FLAGS = re.IGNORECASE | re.UNICODE\n        self._SQL_REGEX = [\n            (re.compile(rx, FLAGS).match, tt)\n            for rx, tt in SQL_REGEX\n        ]\n\n    def add_keywords(self, keywords):\n        \"\"\"Add keyword dictionaries. Keywords are looked up in the same order\n        that dictionaries were added.\"\"\"\n        self._keywords.append(keywords)\n\n    def is_keyword(self, value):\n        \"\"\"Checks for a keyword.\n\n        If the given value is in one of the KEYWORDS_* dictionary\n        it's considered a keyword. Otherwise, tokens.Name is returned.\n        \"\"\"\n        val = value.upper()\n        for kwdict in self._keywords:\n            if val in kwdict:\n                return kwdict[val], value\n        else:\n            return tokens.Name, value\n\n    def get_tokens(self, text, encoding=None):\n        \"\"\"\n        Return an iterable of (tokentype, value) pairs generated from\n        `text`. If `unfiltered` is set to `True`, the filtering mechanism\n        is bypassed even if filters are defined.\n\n        Also preprocess the text, i.e. expand tabs and strip it if\n        wanted and applies registered filters.\n\n        Split ``text`` into (tokentype, text) pairs.\n\n        ``stack`` is the initial stack (default: ``['root']``)\n        \"\"\"\n        if isinstance(text, TextIOBase):\n            text = text.read()\n\n        if isinstance(text, str):\n            pass\n        elif isinstance(text, bytes):\n            if encoding:\n                text = text.decode(encoding)\n            else:\n                try:\n                    text = text.decode('utf-8')\n                except UnicodeDecodeError:\n                    text = text.decode('unicode-escape')\n        else:\n            raise TypeError(\"Expected text or file-like object, got {!r}\".\n                            format(type(text)))\n\n        iterable = enumerate(text)\n        for pos, char in iterable:\n            for rexmatch, action in self._SQL_REGEX:\n                m = rexmatch(text, pos)\n\n                if not m:\n                    continue\n                elif isinstance(action, tokens._TokenType):\n                    yield action, m.group()\n                elif action is keywords.PROCESS_AS_KEYWORD:\n                    yield self.is_keyword(m.group())\n\n                consume(iterable, m.end() - pos - 1)\n                break\n            else:\n                yield tokens.Error, char\n\n```\n\n\n",
        "eval_script": "import re\nfrom threading import Lock\nfrom io import TextIOBase\n\n# Mock tokens module\nclass tokens:\n    Name = \"Name\"\n    Error = \"Error\"\n    _TokenType = type('TokenType', (), {})\n\n# Mock keywords module\nclass keywords:\n    SQL_REGEX = [(r'SELECT', tokens.Name)]\n    KEYWORDS_COMMON = {'SELECT': 'Keyword'}\n    KEYWORDS_ORACLE = {}\n    KEYWORDS_MYSQL = {}\n    KEYWORDS_PLPGSQL = {}\n    KEYWORDS_HQL = {}\n    KEYWORDS_MSACCESS = {}\n    KEYWORDS_SNOWFLAKE = {}\n    KEYWORDS_BIGQUERY = {}\n    KEYWORDS = {}\n    PROCESS_AS_KEYWORD = \"PROCESS_AS_KEYWORD\"\n\n# Consume function from context\nimport itertools\nfrom collections import deque\n\ndef consume(iterator, n):\n    \"\"\"Advance the iterator n-steps ahead. If n is none, consume entirely.\"\"\"\n    deque(itertools.islice(iterator, n), maxlen=0)\n\n# Lexer class definition as given\nclass Lexer:\n    _default_instance = None\n    _lock = Lock()\n\n    @classmethod\n    def get_default_instance(cls):\n        with cls._lock:\n            if cls._default_instance is None:\n                cls._default_instance = cls()\n                cls._default_instance.default_initialization()\n        return cls._default_instance\n\n    def default_initialization(self):\n        self.clear()\n        self.set_SQL_REGEX(keywords.SQL_REGEX)\n        self.add_keywords(keywords.KEYWORDS_COMMON)\n        self.add_keywords(keywords.KEYWORDS_ORACLE)\n        self.add_keywords(keywords.KEYWORDS_MYSQL)\n        self.add_keywords(keywords.KEYWORDS_PLPGSQL)\n        self.add_keywords(keywords.KEYWORDS_HQL)\n        self.add_keywords(keywords.KEYWORDS_MSACCESS)\n        self.add_keywords(keywords.KEYWORDS_SNOWFLAKE)\n        self.add_keywords(keywords.KEYWORDS_BIGQUERY)\n        self.add_keywords(keywords.KEYWORDS)\n\n\n\n    def clear(self):\n        self._SQL_REGEX = []\n        self._keywords = []\n\n    def set_SQL_REGEX(self, SQL_REGEX):\n        FLAGS = re.IGNORECASE | re.UNICODE\n        self._SQL_REGEX = [\n            (re.compile(rx, FLAGS).match, tt)\n            for rx, tt in SQL_REGEX\n        ]\n\n    def add_keywords(self, keywords):\n        self._keywords.append(keywords)\n\n    def is_keyword(self, value):\n        val = value.upper()\n        for kwdict in self._keywords:\n            if val in kwdict:\n                return kwdict[val], value\n        else:\n            return tokens.Name, value\n\n    def get_tokens(self, text, encoding=None):\n        if isinstance(text, TextIOBase):\n            text = text.read()\n\n        if isinstance(text, str):\n            pass\n        elif isinstance(text, bytes):\n            if encoding:\n                text = text.decode(encoding)\n            else:\n                try:\n                    text = text.decode('utf-8')\n                except UnicodeDecodeError:\n                    text = text.decode('unicode-escape')\n        else:\n            raise TypeError(\"Expected text or file-like object, got {!r}\".\n                            format(type(text)))\n\n        iterable = enumerate(text)\n        for pos, char in iterable:\n            for rexmatch, action in self._SQL_REGEX:\n                m = rexmatch(text, pos)\n\n                if not m:\n                    continue\n                elif isinstance(action, tokens._TokenType):\n                    yield action, m.group()\n                elif action is keywords.PROCESS_AS_KEYWORD:\n                    yield self.is_keyword(m.group())\n\n                consume(iterable, m.end() - pos - 1)\n                break\n            else:\n                yield tokens.Error, char\n\ndef test_default_initialization():\n    lexer1 = Lexer()\n    lexer2 = Lexer()\n    lexer1.default_initialization()\n    lexer2.default_initialization_new_implementation()\n    \n    assert len(lexer1._SQL_REGEX) == len(lexer2._SQL_REGEX), \"SQL_REGEX lengths differ\"\n    assert lexer1._SQL_REGEX == lexer2._SQL_REGEX, \"SQL_REGEX content differs\"\n    assert lexer1._keywords == lexer2._keywords, \"Keywords lists differ\"\n\nif __name__ == \"__main__\":\n    test_default_initialization()\n    print(\"All tests passed successfully.\")"
    },
    {
        "func_name": "CPUOptimizedNoamTransformer._create_rotary_embedding",
        "idx": "224",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def _create_rotary_embedding(self) -> Tensor:\n    \"\"\"Create rotary position embeddings.\"\"\"\n    inv_freq = 1.0 / 10000 ** (torch.arange(0, self.config.d_model, 2).float() / self.config.d_model)\n    pos = torch.arange(self.config.max_seq_length).float()\n    sincos = torch.einsum('i,j->ij', pos, inv_freq)\n    emb = torch.cat((sincos, sincos), dim=-1)\n    return emb.unsqueeze(0)",
        "orig_context": "```python\n## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\nclass CPUOptimizedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.attention = CPUOptimizedMultiQueryAttention(config)\n        self.feed_forward = CPUOptimizedFeedForward(config)\n\n        # RMSNorm for pre-normalization\n        self.norm1 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n        self.norm2 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        logger.info(\"Initialized CPUOptimizedTransformerLayer\")\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with pre-normalization.\"\"\"\n        # Pre-norm architecture\n        x = x + self.attention(self.norm1(x), x, x, mask)\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\nclass CPUOptimizedNoamTransformer(nn.Module):\n    \"\"\"Complete Noam transformer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        # Token embeddings with optimal memory layout\n        self.embedding = nn.Embedding(\n            config.vocab_size, config.d_model\n        )\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Pre-compute rotary position embeddings\n        self.register_buffer(\n            \"pos_embedding\", self._create_rotary_embedding()\n        )\n\n        # Transformer layers\n        self.layers = nn.ModuleList(\n            [\n                CPUOptimizedTransformerLayer(config)\n                for _ in range(config.n_layers)\n            ]\n        )\n\n        # Final normalization\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        self._init_parameters()\n        logger.info(\"Initialized CPUOptimizedNoamTransformer\")\n\n    def _create_rotary_embedding(self) -> Tensor:\n        \"\"\"Create rotary position embeddings.\"\"\"\n        inv_freq = 1.0 / (\n            10000\n            ** (\n                torch.arange(0, self.config.d_model, 2).float()\n                / self.config.d_model\n            )\n        )\n        pos = torch.arange(self.config.max_seq_length).float()\n        sincos = torch.einsum(\"i,j->ij\", pos, inv_freq)\n        emb = torch.cat((sincos, sincos), dim=-1)\n        return emb.unsqueeze(0)\n\n    def _init_parameters(self):\n        \"\"\"Initialize parameters with specific CPU optimization.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                # Use Pytorch's native CPU optimized initialization\n                nn.init.xavier_uniform_(p)\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with optimized computation flow.\"\"\"\n        # Generate embeddings\n        x = self.embedding(x) * math.sqrt(self.config.d_model)\n\n        # Add rotary position embeddings\n        x = x + self.pos_embedding[:, : x.size(1)]\n        x = self.dropout(x)\n\n        # Apply transformer layers\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        return self.norm(x)\n\n```\n\n\n",
        "eval_script": "## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\nclass CPUOptimizedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.attention = CPUOptimizedMultiQueryAttention(config)\n        self.feed_forward = CPUOptimizedFeedForward(config)\n\n        # RMSNorm for pre-normalization\n        self.norm1 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n        self.norm2 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        logger.info(\"Initialized CPUOptimizedTransformerLayer\")\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with pre-normalization.\"\"\"\n        # Pre-norm architecture\n        x = x + self.attention(self.norm1(x), x, x, mask)\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\nclass CPUOptimizedNoamTransformer(nn.Module):\n    \"\"\"Complete Noam transformer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        # Token embeddings with optimal memory layout\n        self.embedding = nn.Embedding(\n            config.vocab_size, config.d_model\n        )\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Pre-compute rotary position embeddings\n        self.register_buffer(\n            \"pos_embedding\", self._create_rotary_embedding()\n        )\n\n        # Transformer layers\n        self.layers = nn.ModuleList(\n            [\n                CPUOptimizedTransformerLayer(config)\n                for _ in range(config.n_layers)\n            ]\n        )\n\n        # Final normalization\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        self._init_parameters()\n        logger.info(\"Initialized CPUOptimizedNoamTransformer\")\n\n    def _create_rotary_embedding(self) -> Tensor:\n        \"\"\"Create rotary position embeddings.\"\"\"\n        inv_freq = 1.0 / (\n            10000\n            ** (\n                torch.arange(0, self.config.d_model, 2).float()\n                / self.config.d_model\n            )\n        )\n        pos = torch.arange(self.config.max_seq_length).float()\n        sincos = torch.einsum(\"i,j->ij\", pos, inv_freq)\n        emb = torch.cat((sincos, sincos), dim=-1)\n        return emb.unsqueeze(0)\n\n\n\n    def _init_parameters(self):\n        \"\"\"Initialize parameters with specific CPU optimization.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                # Use Pytorch's native CPU optimized initialization\n                nn.init.xavier_uniform_(p)\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with optimized computation flow.\"\"\"\n        # Generate embeddings\n        x = self.embedding(x) * math.sqrt(self.config.d_model)\n\n        # Add rotary position embeddings\n        x = x + self.pos_embedding[:, : x.size(1)]\n        x = self.dropout(x)\n\n        # Apply transformer layers\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        return self.norm(x)\n\ndef test__create_rotary_embedding():\n    config = NoamConfig()\n    transformer = CPUOptimizedNoamTransformer(config)\n\n    # Call the original implementation\n    original_emb = transformer._create_rotary_embedding()\n\n    # Call the new implementation\n    new_emb = transformer._create_rotary_embedding_new_implementation()\n\n    # Assertions to check equivalence\n    assert original_emb.shape == new_emb.shape, \"Shapes do not match\"\n    assert torch.allclose(original_emb, new_emb), \"Tensor values do not match\"\n    assert torch.equal(original_emb[0][0:10], new_emb[0][0:10]), \"Tensor slices do not match\"\n\nif __name__ == \"__main__\":\n    test__create_rotary_embedding()"
    },
    {
        "func_name": "CPUOptimizedFeedForward._vectorized_swish",
        "idx": "226",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def _vectorized_swish(self, x: Tensor) -> Tensor:\n    \"\"\"Vectorized SwiGLU activation.\"\"\"\n    return x * torch.sigmoid(x)",
        "orig_context": "```python\n## main.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\n```\n\n\n",
        "eval_script": "## main.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n    \n\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\ndef test__vectorized_swish():\n    config = NoamConfig()\n    model = CPUOptimizedFeedForward(config)\n    \n    # Test case 1: Input is a tensor of zeros\n    input_tensor = torch.zeros((10, 512))\n    assert torch.equal(\n        model._vectorized_swish(input_tensor),\n        model._vectorized_swish_new_implementation(input_tensor)\n    ), \"Test case 1 failed!\"\n    \n    # Test case 2: Input is a tensor with positive values\n    input_tensor = torch.ones((10, 512)) * 5\n    assert torch.equal(\n        model._vectorized_swish(input_tensor),\n        model._vectorized_swish_new_implementation(input_tensor)\n    ), \"Test case 2 failed!\"\n    \n    # Test case 3: Input is a tensor with random values\n    input_tensor = torch.randn((10, 512))\n    assert torch.equal(\n        model._vectorized_swish(input_tensor),\n        model._vectorized_swish_new_implementation(input_tensor)\n    ), \"Test case 3 failed!\"\n    \n    print(\"All test cases passed!\")\n\nif __name__ == \"__main__\":\n    test__vectorized_swish()"
    },
    {
        "func_name": "CPUOptimizedLinear.forward",
        "idx": "227",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def forward(self, x: Tensor) -> Tensor:\n    \"\"\"Forward pass with blocked computation.\"\"\"\n    outputs = []\n    with ThreadPoolExecutor(max_workers=self.config.n_threads) as executor:\n        futures = [executor.submit(self._blocked_matmul, x, block) for block in self.weight_blocks]\n        outputs = [future.result() for future in futures]\n    output = torch.cat(outputs, dim=-1)\n    return output + self.bias",
        "orig_context": "```python\n## main.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\n```\n\n\n",
        "eval_script": "## main.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\n\n\ndef test_forward():\n    config = CPUOptimizedConfig()\n    model = CPUOptimizedLinear(in_features=10, out_features=20, config=config)\n\n    # Create a random tensor as input\n    x = torch.randn(5, 10, 10)  # (batch_size, seq_len, in_features)\n\n    # Calculate outputs of each implementation\n    output_old = model.forward(x)\n    output_new = model.forward_new_implementation(x)\n\n    # Assert that the outputs are almost equal\n    assert torch.allclose(output_old, output_new, atol=1e-6), \"Outputs differ!\"\n    assert output_old.shape == output_new.shape, \"Output shapes differ!\"\n    assert output_old.dtype == output_new.dtype, \"Output data types differ!\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "CPUOptimizedMultiQueryAttention.forward",
        "idx": "228",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n    \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n    batch_size = q.size(0)\n    q = self.q_proj(q).view(batch_size, -1, self.config.n_heads, self.d_k)\n    k = self.k_proj(k).unsqueeze(1)\n    v = self.v_proj(v).unsqueeze(1)\n    k = k.expand(-1, self.config.n_heads, -1, -1)\n    v = v.expand(-1, self.config.n_heads, -1, -1)\n    q = q.transpose(1, 2)\n    context = self._cached_attention(q, k, v, self.config.chunk_size)\n    context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.d_model)\n    return self.o_proj(context)",
        "orig_context": "```python\n## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\n```\n\n\n",
        "eval_script": "## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\n\n\ndef test_forward():\n    # Setup configurations and model\n    config = NoamConfig()\n    attention = CPUOptimizedMultiQueryAttention(config)\n\n    # Input tensors\n    q = torch.randn(2, 10, config.d_model)\n    k = torch.randn(2, 10, config.d_model)\n    v = torch.randn(2, 10, config.d_model)\n\n    # Test case 1: Ensure outputs are identical\n    output1 = attention.forward(q, k, v)\n    output2 = attention.forward_new_implementation(q, k, v)\n    assert torch.allclose(output1, output2), \"Test Case 1 Failed\"\n\n    # Test case 2: Different input sizes\n    q = torch.randn(2, 15, config.d_model)\n    output1 = attention.forward(q, k, v)\n    output2 = attention.forward_new_implementation(q, k, v)\n    assert torch.allclose(output1, output2), \"Test Case 2 Failed\"\n\n    # Test case 3: Masked attention\n    mask = torch.ones(2, 10, 15)\n    output1 = attention.forward(q, k, v, mask)\n    output2 = attention.forward_new_implementation(q, k, v, mask)\n    assert torch.allclose(output1, output2), \"Test Case 3 Failed\"\n\n\nif __name__ == \"__main__\":\n    test_forward()\n    print(\"All test cases passed successfully.\")"
    },
    {
        "func_name": "CPUOptimizedLinear._blocked_matmul",
        "idx": "229",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def _blocked_matmul(self, x: Tensor, weight_block: Tensor) -> Tensor:\n    \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n    (batch_size, seq_len, _) = x.shape\n    out_features = weight_block.size(0)\n    x_blocked = x.view(batch_size * seq_len, -1)\n    cache_key = (x_blocked.shape, weight_block.shape)\n    if cache_key in self.cache:\n        result = torch.matmul(x_blocked, weight_block.t())\n    else:\n        result = torch.matmul(x_blocked, weight_block.t())\n        if self.cache_size < self.max_cache_size:\n            self.cache[cache_key] = result\n            self.cache_size += result.element_size() * result.nelement()\n    return result.view(batch_size, seq_len, -1)",
        "orig_context": "```python\n## main.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\n```\n\n\n",
        "eval_script": "# The new PYTHON CODE containing your test function test__blocked_matmul and the __main__ function.\n\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n    \n    # Placeholder for new implementation\n\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\ndef test__blocked_matmul():\n    config = CPUOptimizedConfig()\n    model = CPUOptimizedLinear(in_features=64, out_features=128, config=config)\n    \n    x = torch.rand(4, 16, 64)\n    weight_block = torch.rand(64, 64)  # Shape as expected in matmul\n\n    # Outputs using both methods\n    output_original = model._blocked_matmul(x, weight_block)\n    output_new = model._blocked_matmul_new_implementation(x, weight_block)\n    \n    # Assert that both outputs are the same\n    assert output_original.shape == output_new.shape, \"Output shapes differ\"\n    assert torch.allclose(output_original, output_new), \"Outputs values differ\"\n    assert output_original.dtype == output_new.dtype, \"Output dtypes differ\"\n\nif __name__ == \"__main__\":\n    test__blocked_matmul()\n    print(\"All test cases passed!\")"
    },
    {
        "func_name": "CPUOptimizedTransformerLayer.forward",
        "idx": "230",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def forward(self, x: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n    \"\"\"Forward pass with pre-normalization.\"\"\"\n    x = x + self.attention(self.norm1(x), x, x, mask)\n    x = x + self.feed_forward(self.norm2(x))\n    return x",
        "orig_context": "```python\n## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\nclass CPUOptimizedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.attention = CPUOptimizedMultiQueryAttention(config)\n        self.feed_forward = CPUOptimizedFeedForward(config)\n\n        # RMSNorm for pre-normalization\n        self.norm1 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n        self.norm2 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        logger.info(\"Initialized CPUOptimizedTransformerLayer\")\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with pre-normalization.\"\"\"\n        # Pre-norm architecture\n        x = x + self.attention(self.norm1(x), x, x, mask)\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\n```\n\n\n",
        "eval_script": "## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\nclass CPUOptimizedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.attention = CPUOptimizedMultiQueryAttention(config)\n        self.feed_forward = CPUOptimizedFeedForward(config)\n\n        # RMSNorm for pre-normalization\n        self.norm1 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n        self.norm2 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        logger.info(\"Initialized CPUOptimizedTransformerLayer\")\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with pre-normalization.\"\"\"\n        # Pre-norm architecture\n        x = x + self.attention(self.norm1(x), x, x, mask)\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\n\n\ndef test_forward():\n    \"\"\"Test to compare forward implementations.\"\"\"\n    config = NoamConfig()\n    layer = CPUOptimizedTransformerLayer(config)\n    x = torch.rand(2, 10, config.d_model)  # Example input\n\n    # Get outputs from both methods\n    output_original = layer.forward(x)\n    output_new = layer.forward_new_implementation(x)\n\n    # Assert outputs are identical\n    assert output_original.shape == output_new.shape, \"Shape mismatch\"\n    assert torch.allclose(output_original, output_new), \"Value mismatch\"\n\n    # Verify further properties, e.g., same device\n    assert output_original.device == output_new.device, \"Device mismatch\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "NoamLRScheduler.step",
        "idx": "231",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def step(self):\n    \"\"\"Update learning rate.\"\"\"\n    self.current_step += 1\n    rate = self._get_rate(self.current_step)\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = rate\n    return rate",
        "orig_context": "```python\n## main.py\nimport torch\n\nfrom loguru import logger\n\nclass NoamLRScheduler:\n    \"\"\"Noam Learning Rate Scheduler with CPU optimization.\"\"\"\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        d_model: int,\n        warmup_steps: int,\n    ):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.current_step = 0\n\n        # Pre-compute constants\n        self.scale = d_model**-0.5\n        self._update_rate_cache = {}\n        logger.info(\n            f\"Initialized NoamLRScheduler with warmup_steps={warmup_steps}\"\n        )\n\n    def _get_rate(self, step: int) -> float:\n        \"\"\"Compute learning rate with caching.\"\"\"\n        if step in self._update_rate_cache:\n            return self._update_rate_cache[step]\n\n        arg1 = step**-0.5\n        arg2 = step * (self.warmup_steps**-1.5)\n        rate = self.scale * min(arg1, arg2)\n\n        # Cache computation\n        if len(self._update_rate_cache) < 1000:  # Limit cache size\n            self._update_rate_cache[step] = rate\n\n        return rate\n\n    def step(self):\n        \"\"\"Update learning rate.\"\"\"\n        self.current_step += 1\n        rate = self._get_rate(self.current_step)\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = rate\n        return rate\n\n```\n\n\n",
        "eval_script": "## main.py\nimport torch\n\nfrom loguru import logger\n\nclass NoamLRScheduler:\n    \"\"\"Noam Learning Rate Scheduler with CPU optimization.\"\"\"\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        d_model: int,\n        warmup_steps: int,\n    ):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.current_step = 0\n\n        # Pre-compute constants\n        self.scale = d_model**-0.5\n        self._update_rate_cache = {}\n        logger.info(\n            f\"Initialized NoamLRScheduler with warmup_steps={warmup_steps}\"\n        )\n\n    def _get_rate(self, step: int) -> float:\n        \"\"\"Compute learning rate with caching.\"\"\"\n        if step in self._update_rate_cache:\n            return self._update_rate_cache[step]\n\n        arg1 = step**-0.5\n        arg2 = step * (self.warmup_steps**-1.5)\n        rate = self.scale * min(arg1, arg2)\n\n        # Cache computation\n        if len(self._update_rate_cache) < 1000:  # Limit cache size\n            self._update_rate_cache[step] = rate\n\n        return rate\n\n    def step(self):\n        \"\"\"Update learning rate.\"\"\"\n        self.current_step += 1\n        rate = self._get_rate(self.current_step)\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = rate\n        return rate\n\n\n\ndef test_step():\n    # Setup for testing\n    optimizer = torch.optim.SGD([torch.tensor(1.0)], lr=0.0)\n    scheduler_original = NoamLRScheduler(optimizer, d_model=512, warmup_steps=4000)\n    scheduler_new = NoamLRScheduler(optimizer, d_model=512, warmup_steps=4000)\n\n    for _ in range(3):  # Run a few steps to assert correctness\n        original_rate = scheduler_original.step()\n        new_rate = scheduler_new.step_new_implementation()\n        assert original_rate == new_rate, \"Learning rates do not match!\"\n        assert scheduler_original.current_step == scheduler_new.current_step, \"Current steps do not match!\"\n        assert scheduler_original.optimizer.param_groups[0][\"lr\"] == scheduler_new.optimizer.param_groups[0][\"lr\"], \"Optimizer learning rates do not match!\"\n\nif __name__ == \"__main__\":\n    test_step()"
    },
    {
        "func_name": "CPUOptimizedFeedForward.forward",
        "idx": "232",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def forward(self, x: Tensor) -> Tensor:\n    \"\"\"Forward pass with normalized feed-forward.\"\"\"\n    x = self.norm(x)\n    x = self.fc2(self.activation(self.fc1(x)))\n    return x",
        "orig_context": "```python\n## main.py\nimport math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\n```\n\n\n",
        "eval_script": "import math\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\n\n\ndef test_forward():\n    \"\"\"Test function to validate forward_new_implementation against forward.\"\"\"\n    config = NoamConfig()\n    model = CPUOptimizedFeedForward(config)\n    \n    # Test case 1\n    x1 = torch.randn(2, 10, config.d_model)\n    expected_output1 = model.forward(x1)\n    test_output1 = model.forward_new_implementation(x1)\n    assert torch.allclose(expected_output1, test_output1), \"Test case 1 failed\"\n    \n    # Test case 2\n    x2 = torch.randn(5, 20, config.d_model)\n    expected_output2 = model.forward(x2)\n    test_output2 = model.forward_new_implementation(x2)\n    assert torch.allclose(expected_output2, test_output2), \"Test case 2 failed\"\n\n    # Test case 3\n    x3 = torch.randn(3, 15, config.d_model)\n    expected_output3 = model.forward(x3)\n    test_output3 = model.forward_new_implementation(x3)\n    assert torch.allclose(expected_output3, test_output3), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "CPUOptimizedNoamTransformer.forward",
        "idx": "233",
        "repo_name": "Agora-Lab-AI___NeoCore",
        "func_path": "main.py",
        "orig_func": "def forward(self, x: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n    \"\"\"Forward pass with optimized computation flow.\"\"\"\n    x = self.embedding(x) * math.sqrt(self.config.d_model)\n    x = x + self.pos_embedding[:, :x.size(1)]\n    x = self.dropout(x)\n    for layer in self.layers:\n        x = layer(x, mask)\n    return self.norm(x)",
        "orig_context": "```python\n## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\nclass CPUOptimizedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.attention = CPUOptimizedMultiQueryAttention(config)\n        self.feed_forward = CPUOptimizedFeedForward(config)\n\n        # RMSNorm for pre-normalization\n        self.norm1 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n        self.norm2 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        logger.info(\"Initialized CPUOptimizedTransformerLayer\")\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with pre-normalization.\"\"\"\n        # Pre-norm architecture\n        x = x + self.attention(self.norm1(x), x, x, mask)\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\nclass CPUOptimizedNoamTransformer(nn.Module):\n    \"\"\"Complete Noam transformer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        # Token embeddings with optimal memory layout\n        self.embedding = nn.Embedding(\n            config.vocab_size, config.d_model\n        )\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Pre-compute rotary position embeddings\n        self.register_buffer(\n            \"pos_embedding\", self._create_rotary_embedding()\n        )\n\n        # Transformer layers\n        self.layers = nn.ModuleList(\n            [\n                CPUOptimizedTransformerLayer(config)\n                for _ in range(config.n_layers)\n            ]\n        )\n\n        # Final normalization\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        self._init_parameters()\n        logger.info(\"Initialized CPUOptimizedNoamTransformer\")\n\n    def _create_rotary_embedding(self) -> Tensor:\n        \"\"\"Create rotary position embeddings.\"\"\"\n        inv_freq = 1.0 / (\n            10000\n            ** (\n                torch.arange(0, self.config.d_model, 2).float()\n                / self.config.d_model\n            )\n        )\n        pos = torch.arange(self.config.max_seq_length).float()\n        sincos = torch.einsum(\"i,j->ij\", pos, inv_freq)\n        emb = torch.cat((sincos, sincos), dim=-1)\n        return emb.unsqueeze(0)\n\n    def _init_parameters(self):\n        \"\"\"Initialize parameters with specific CPU optimization.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                # Use Pytorch's native CPU optimized initialization\n                nn.init.xavier_uniform_(p)\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with optimized computation flow.\"\"\"\n        # Generate embeddings\n        x = self.embedding(x) * math.sqrt(self.config.d_model)\n\n        # Add rotary position embeddings\n        x = x + self.pos_embedding[:, : x.size(1)]\n        x = self.dropout(x)\n\n        # Apply transformer layers\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        return self.norm(x)\n\n```\n\n\n",
        "eval_script": "## main.py\nimport math\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\n\nfrom dataclasses import dataclass\n\nfrom loguru import logger\n\nimport psutil\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass CPUOptimizedConfig:\n    \"\"\"Configuration for CPU-optimized transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64  # Size of chunks for blocked operations\n    n_threads: int = psutil.cpu_count(logical=True)\n    use_fused_ops: bool = True\n    cache_size_mb: int = 32\n\nclass CPUOptimizedLinear(nn.Module):\n    \"\"\"Custom linear layer optimized for CPU execution with blocked matrix multiplication.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        config: CPUOptimizedConfig,\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.config = config\n\n        # Initialize weights in blocks for better cache utilization\n        self.n_blocks = math.ceil(out_features / config.chunk_size)\n        self.weight_blocks = nn.ParameterList(\n            [\n                nn.Parameter(\n                    torch.empty(\n                        min(\n                            config.chunk_size,\n                            out_features - i * config.chunk_size,\n                        ),\n                        in_features,\n                    )\n                )\n                for i in range(self.n_blocks)\n            ]\n        )\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n        # Operation cache\n        self.cache = {}\n        self.cache_size = 0\n        self.max_cache_size = (\n            config.cache_size_mb * 1024 * 1024\n        )  # Convert to bytes\n\n        logger.info(\n            f\"Initialized CPUOptimizedLinear with {self.n_blocks} blocks\"\n        )\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with blocked initialization.\"\"\"\n        for block in self.weight_blocks:\n            nn.init.kaiming_uniform_(block, a=math.sqrt(5))\n        nn.init.zeros_(self.bias)\n\n    def _blocked_matmul(\n        self, x: Tensor, weight_block: Tensor\n    ) -> Tensor:\n        \"\"\"Perform blocked matrix multiplication optimized for CPU cache.\"\"\"\n        batch_size, seq_len, _ = x.shape\n        out_features = weight_block.size(0)\n\n        # Reshape input for blocked multiplication\n        x_blocked = x.view(batch_size * seq_len, -1)\n\n        # Cache key for this operation\n        cache_key = (x_blocked.shape, weight_block.shape)\n\n        if cache_key in self.cache:\n            result = torch.matmul(x_blocked, weight_block.t())\n        else:\n            result = torch.matmul(x_blocked, weight_block.t())\n\n            # Cache management\n            if self.cache_size < self.max_cache_size:\n                self.cache[cache_key] = result\n                self.cache_size += (\n                    result.element_size() * result.nelement()\n                )\n\n        return result.view(batch_size, seq_len, -1)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with blocked computation.\"\"\"\n        outputs = []\n\n        # Process each block in parallel\n        with ThreadPoolExecutor(\n            max_workers=self.config.n_threads\n        ) as executor:\n            futures = [\n                executor.submit(self._blocked_matmul, x, block)\n                for block in self.weight_blocks\n            ]\n            outputs = [future.result() for future in futures]\n\n        # Concatenate results and add bias\n        output = torch.cat(outputs, dim=-1)\n        return output + self.bias\n\nclass NoamConfig:\n    \"\"\"Configuration for CPU-optimized Noam transformer.\"\"\"\n\n    d_model: int = 512\n    n_heads: int = 8\n    n_layers: int = 6\n    d_ff: int = 2048\n    dropout: float = 0.1\n    max_seq_length: int = 512\n    vocab_size: int = 30000\n    chunk_size: int = 64\n    n_threads: int = psutil.cpu_count(logical=True)\n    warmup_steps: int = 4000\n    epsilon: float = 1e-6\n    cache_size_mb: int = 32\n    use_mqa: bool = True\n\nclass CPUOptimizedRMSNorm(nn.Module):\n    \"\"\"RMSNorm implementation optimized for CPU execution.\"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\n            \"dummy\", torch.ones(1)\n        )  # For optimization hints\n\n    def _rms_norm(self, x: Tensor) -> Tensor:\n        \"\"\"Optimized RMSNorm computation.\"\"\"\n        # Compute norm in chunks for better cache utilization\n        norm_sq = x.pow(2).mean(dim=-1, keepdim=True)\n        return x * torch.rsqrt(norm_sq + self.eps) * self.g\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with vectorized operations.\"\"\"\n        return self._rms_norm(x.float()).type_as(x)\n\nclass CPUOptimizedMultiQueryAttention(nn.Module):\n    \"\"\"Multi-Query Attention optimized for CPU execution.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n        self.d_k = config.d_model // config.n_heads\n\n        # Single key and value projections for MQA\n        self.k_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n        self.v_proj = CPUOptimizedLinear(\n            config.d_model, self.d_k, config\n        )\n\n        # Multiple query projections\n        self.q_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n        self.o_proj = CPUOptimizedLinear(\n            config.d_model, config.d_model, config\n        )\n\n        self.scale = self.d_k**-0.5\n        self.cache = {}\n\n        logger.info(\"Initialized CPUOptimizedMultiQueryAttention\")\n\n    def _cached_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, chunk_size: int\n    ) -> Tensor:\n        \"\"\"Compute attention scores with caching and chunking.\"\"\"\n        batch_size, n_heads, seq_len, d_k = q.shape\n        outputs = []\n\n        for i in range(0, seq_len, chunk_size):\n            chunk_q = q[:, :, i : i + chunk_size]\n\n            # Use cached computations when possible\n            cache_key = (chunk_q.shape, k.shape)\n            if cache_key in self.cache:\n                chunk_output = self.cache[cache_key]\n            else:\n                scores = (\n                    torch.matmul(chunk_q, k.transpose(-2, -1))\n                    * self.scale\n                )\n                weights = F.softmax(scores, dim=-1)\n                chunk_output = torch.matmul(weights, v)\n\n                # Cache management\n                if len(self.cache) < 100:  # Limit cache size\n                    self.cache[cache_key] = chunk_output\n\n            outputs.append(chunk_output)\n\n        return torch.cat(outputs, dim=2)\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tensor:\n        \"\"\"Forward pass with Multi-Query Attention.\"\"\"\n        batch_size = q.size(0)\n\n        # Project queries (multiple heads)\n        q = self.q_proj(q).view(\n            batch_size, -1, self.config.n_heads, self.d_k\n        )\n\n        # Project keys and values (single head)\n        k = self.k_proj(k).unsqueeze(1)\n        v = self.v_proj(v).unsqueeze(1)\n\n        # Expand k and v for all heads\n        k = k.expand(-1, self.config.n_heads, -1, -1)\n        v = v.expand(-1, self.config.n_heads, -1, -1)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n\n        # Compute attention with caching and chunking\n        context = self._cached_attention(\n            q, k, v, self.config.chunk_size\n        )\n\n        # Reshape and project output\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(batch_size, -1, self.config.d_model)\n        )\n        return self.o_proj(context)\n\nclass CPUOptimizedFeedForward(nn.Module):\n    \"\"\"Feed-forward network with CPU optimizations and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        self.fc1 = CPUOptimizedLinear(\n            config.d_model, config.d_ff, config\n        )\n        self.fc2 = CPUOptimizedLinear(\n            config.d_ff, config.d_model, config\n        )\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        # Vectorized activation\n        self.activation = self._vectorized_swish\n\n        logger.info(\"Initialized CPUOptimizedFeedForward\")\n\n    def _vectorized_swish(self, x: Tensor) -> Tensor:\n        \"\"\"Vectorized SwiGLU activation.\"\"\"\n        return x * torch.sigmoid(x)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass with normalized feed-forward.\"\"\"\n        x = self.norm(x)\n        x = self.fc2(self.activation(self.fc1(x)))\n        return x\n\nclass CPUOptimizedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.attention = CPUOptimizedMultiQueryAttention(config)\n        self.feed_forward = CPUOptimizedFeedForward(config)\n\n        # RMSNorm for pre-normalization\n        self.norm1 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n        self.norm2 = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        logger.info(\"Initialized CPUOptimizedTransformerLayer\")\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with pre-normalization.\"\"\"\n        # Pre-norm architecture\n        x = x + self.attention(self.norm1(x), x, x, mask)\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\nclass CPUOptimizedNoamTransformer(nn.Module):\n    \"\"\"Complete Noam transformer with MQA and RMSNorm.\"\"\"\n\n    def __init__(self, config: NoamConfig):\n        super().__init__()\n        self.config = config\n\n        # Token embeddings with optimal memory layout\n        self.embedding = nn.Embedding(\n            config.vocab_size, config.d_model\n        )\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Pre-compute rotary position embeddings\n        self.register_buffer(\n            \"pos_embedding\", self._create_rotary_embedding()\n        )\n\n        # Transformer layers\n        self.layers = nn.ModuleList(\n            [\n                CPUOptimizedTransformerLayer(config)\n                for _ in range(config.n_layers)\n            ]\n        )\n\n        # Final normalization\n        self.norm = CPUOptimizedRMSNorm(\n            config.d_model, eps=config.epsilon\n        )\n\n        self._init_parameters()\n        logger.info(\"Initialized CPUOptimizedNoamTransformer\")\n\n    def _create_rotary_embedding(self) -> Tensor:\n        \"\"\"Create rotary position embeddings.\"\"\"\n        inv_freq = 1.0 / (\n            10000\n            ** (\n                torch.arange(0, self.config.d_model, 2).float()\n                / self.config.d_model\n            )\n        )\n        pos = torch.arange(self.config.max_seq_length).float()\n        sincos = torch.einsum(\"i,j->ij\", pos, inv_freq)\n        emb = torch.cat((sincos.sin(), sincos.cos()), dim=-1)\n        return emb.unsqueeze(0)\n\n    def _init_parameters(self):\n        \"\"\"Initialize parameters with specific CPU optimization.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                # Use Pytorch's native CPU optimized initialization\n                nn.init.xavier_uniform_(p)\n\n    def forward(\n        self, x: Tensor, mask: Optional[Tensor] = None\n    ) -> Tensor:\n        \"\"\"Forward pass with optimized computation flow.\"\"\"\n        # Generate embeddings\n        x = self.embedding(x) * math.sqrt(self.config.d_model)\n\n        # Add rotary position embeddings\n        pos_emb = self.pos_embedding[:, : x.size(1)].to(x.device)\n        x = x + pos_emb\n        x = self.dropout(x)\n\n        # Apply transformer layers\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        return self.norm(x)\n\n\ndef test_forward():\n    config = NoamConfig()\n    model = CPUOptimizedNoamTransformer(config)\n    \n    # Set the model to evaluation mode to disable dropout\n    model.eval()\n\n    # Create sample input\n    input_tensor = torch.randint(0, config.vocab_size, (1, config.max_seq_length), dtype=torch.long)\n    mask = torch.ones(1, 1, config.max_seq_length, config.max_seq_length)\n\n    # Get outputs from both methods\n    original_output = model.forward(input_tensor, mask)\n    new_output = model.forward_new_implementation(input_tensor, mask)\n\n    # Assert statements to ensure functionality is the same\n    assert torch.allclose(original_output, new_output, rtol=1e-5, atol=1e-8), \"Outputs from forward and forward_new_implementation do not match\"\n    assert torch.allclose(original_output, new_output, rtol=1e-5, atol=1e-8), \"Outputs are not numerically close\"\n    assert original_output.shape == new_output.shape, \"Output shapes are different\"\n\nif __name__ == \"__main__\":\n    test_forward()"
    },
    {
        "func_name": "ButtonStyles.get_classes",
        "idx": "234",
        "repo_name": "BaseKodu___MyRealEstate",
        "func_path": "myrealestate/common/forms.py",
        "orig_func": "@staticmethod\ndef get_classes(variant='primary', size=None, outline=False, additional_classes=None):\n    \"\"\"\n        Get DaisyUI button classes\n        variant: primary, secondary, accent, info, success, warning, error\n        size: lg, sm, xs\n        outline: True/False\n        \"\"\"\n    classes = ['btn']\n    if variant:\n        if outline:\n            classes.append(f'btn-outline btn-{variant}')\n        else:\n            classes.append(f'btn-{variant}')\n    if size:\n        classes.append(f'btn-{size}')\n    if additional_classes:\n        classes.extend(additional_classes)\n    return ' '.join(classes)",
        "orig_context": "```python\n## myrealestate/common/forms.py\nclass ButtonStyles:\n    \"\"\"Helper class for DaisyUI button styling\"\"\"\n    @staticmethod\n    def get_classes(variant='primary', size=None, outline=False, additional_classes=None):\n        \"\"\"\n        Get DaisyUI button classes\n        variant: primary, secondary, accent, info, success, warning, error\n        size: lg, sm, xs\n        outline: True/False\n        \"\"\"\n        classes = ['btn']\n        \n        # Add variant\n        if variant:\n            if outline:\n                classes.append(f'btn-outline btn-{variant}')\n            else:\n                classes.append(f'btn-{variant}')\n        \n        # Add size\n        if size:\n            classes.append(f'btn-{size}')\n            \n        # Add additional classes\n        if additional_classes:\n            classes.extend(additional_classes)\n            \n        return ' '.join(classes)\n\n```\n\n\n",
        "eval_script": "## myrealestate/common/forms.py\nclass ButtonStyles:\n    \"\"\"Helper class for DaisyUI button styling\"\"\"\n    @staticmethod\n    def get_classes(variant='primary', size=None, outline=False, additional_classes=None):\n        \"\"\"\n        Get DaisyUI button classes\n        variant: primary, secondary, accent, info, success, warning, error\n        size: lg, sm, xs\n        outline: True/False\n        \"\"\"\n        classes = ['btn']\n        \n        # Add variant\n        if variant:\n            if outline:\n                classes.append(f'btn-outline btn-{variant}')\n            else:\n                classes.append(f'btn-{variant}')\n        \n        # Add size\n        if size:\n            classes.append(f'btn-{size}')\n            \n        # Add additional classes\n        if additional_classes:\n            classes.extend(additional_classes)\n            \n        return ' '.join(classes)\n    \n\n\n\ndef test_get_classes():\n    assert ButtonStyles.get_classes(variant='primary', size='lg') == ButtonStyles.get_classes_new_implementation(variant='primary', size='lg'), \"Test case 1 failed\"\n    assert ButtonStyles.get_classes(variant='accent', outline=True) == ButtonStyles.get_classes_new_implementation(variant='accent', outline=True), \"Test case 2 failed\"\n    assert ButtonStyles.get_classes(size='xs', additional_classes=['custom-class']) == ButtonStyles.get_classes_new_implementation(size='xs', additional_classes=['custom-class']), \"Test case 3 failed\"\n    assert ButtonStyles.get_classes(variant='warning', size='sm', outline=False, additional_classes=None) == ButtonStyles.get_classes_new_implementation(variant='warning', size='sm', outline=False, additional_classes=None), \"Test case 4 failed\"\n    assert ButtonStyles.get_classes() == ButtonStyles.get_classes_new_implementation(), \"Test case 5 failed\"\n\nif __name__ == '__main__':\n    test_get_classes()"
    },
    {
        "func_name": "CustomUserManager.create_user",
        "idx": "235",
        "repo_name": "BaseKodu___MyRealEstate",
        "func_path": "myrealestate/accounts/models.py",
        "orig_func": "def create_user(self, email, password=None, **extra_fields):\n    if not email:\n        raise ValueError(_('Email is required'))\n    email = self.normalize_email(email)\n    user = self.model(email=email, **extra_fields)\n    user.set_password(password)\n    user.save(using=self._db)\n    return user",
        "orig_context": "```python\n## myrealestate/accounts/models.py\nfrom django.contrib.auth.models import AbstractUser, BaseUserManager\n\nfrom django.utils.translation import gettext_lazy as _\n\nclass CustomUserManager(BaseUserManager):\n    def create_user(self, email, password=None, **extra_fields):\n        if not email:\n            raise ValueError(_('Email is required'))\n        email = self.normalize_email(email)\n        user = self.model(email=email, **extra_fields)\n        user.set_password(password)\n        user.save(using=self._db)\n        return user\n\n    def create_superuser(self, email, password=None, **extra_fields):\n        extra_fields.setdefault('is_staff', True)\n        extra_fields.setdefault('is_superuser', True)\n        return self.create_user(email, password, **extra_fields)\n\n```\n\n\n",
        "eval_script": "## myrealestate/accounts/models.py\n# from django.contrib.auth.models import BaseUserManager  # Commented out\n\nfrom django.utils.translation import gettext_lazy as _\n\n\nclass MockUser:\n    \"\"\"\n    A mock User class to simulate saving and setting passwords,\n    since Django's real model functionality isn't available here.\n    \"\"\"\n\n    def __init__(self, email, **extra_fields):\n        self.email = email\n        self.extra_fields = extra_fields\n\n    def set_password(self, password):\n        # Mock setting a password\n        self.password = password\n\n    def save(self, using=None):\n        # Mock save just to simulate the action\n        print(\"User has been saved with email:\", self.email)\n        if self.password:\n            print(\"User password is set.\")\n        print(\"Extra fields:\", self.extra_fields)\n\n\nclass CustomUserManager:\n    model = MockUser  # Assign the mock user class as the model\n\n    def normalize_email(self, email):\n        \"\"\"Simplistic email normalization to mimic Django behavior\"\"\"\n        return email.lower()\n\n    def create_user(self, email, password=None, **extra_fields):\n        if not email:\n            raise ValueError(_('Email is required'))\n        email = self.normalize_email(email)\n        user = self.model(email=email, **extra_fields)\n        user.set_password(password)\n        user.save(using=None)  # No self._db needed\n        return user\n\n\n    def create_superuser(self, email, password=None, **extra_fields):\n        extra_fields.setdefault('is_staff', True)\n        extra_fields.setdefault('is_superuser', True)\n        return self.create_user(email, password, **extra_fields)\n\ndef test_create_user():\n    manager = CustomUserManager()\n\n    # Test case 1\n    user1 = manager.create_user(\"test@example.com\", \"password123\", first_name=\"John\")\n    user2 = manager.create_user_new_implementation(\"test@example.com\", \"password123\", first_name=\"John\")\n    assert user1.email == user2.email\n    assert user1.password == user2.password\n    assert user1.extra_fields == user2.extra_fields\n\n    # Test case 2\n    user1 = manager.create_user(\"foo@bar.com\", \"securepassword\", last_name=\"Doe\")\n    user2 = manager.create_user_new_implementation(\"foo@bar.com\", \"securepassword\", last_name=\"Doe\")\n    assert user1.email == user2.email\n    assert user1.password == user2.password\n    assert user1.extra_fields == user2.extra_fields\n\n    # Test case 3\n    user1 = manager.create_user(\"hello@world.com\", \"helloworld\")\n    user2 = manager.create_user_new_implementation(\"hello@world.com\", \"helloworld\")\n    assert user1.email == user2.email\n    assert user1.password == user2.password\n    assert user1.extra_fields == user2.extra_fields\n\nif __name__ == \"__main__\":\n    test_create_user()"
    },
    {
        "func_name": "messages_to_langchain_messages",
        "idx": "241",
        "repo_name": "Akumsk___pdflyx",
        "func_path": "helpers.py",
        "orig_func": "def messages_to_langchain_messages(chat_history_texts):\n    chat_history = []\n    for msg in chat_history_texts:\n        if msg.startswith('HumanMessage:'):\n            content = msg[len('HumanMessage:'):].strip()\n            chat_history.append(HumanMessage(content=content))\n        elif msg.startswith('AIMessage:'):\n            content = msg[len('AIMessage:'):].strip()\n            chat_history.append(AIMessage(content=content))\n    return chat_history",
        "orig_context": "```python\n## helpers.py\nfrom langchain.schema import Document, HumanMessage, AIMessage\n\ndef messages_to_langchain_messages(chat_history_texts):\n\n    # Convert chat_history_texts to list of HumanMessage and AIMessage\n    chat_history = []\n    for msg in chat_history_texts:\n        if msg.startswith(\"HumanMessage:\"):\n            content = msg[len(\"HumanMessage:\") :].strip()\n            chat_history.append(HumanMessage(content=content))\n        elif msg.startswith(\"AIMessage:\"):\n            content = msg[len(\"AIMessage:\") :].strip()\n            chat_history.append(AIMessage(content=content))\n\n    return chat_history\n\n```\n\n\n",
        "eval_script": "## helpers.py\nfrom langchain.schema import Document, HumanMessage, AIMessage\n\ndef messages_to_langchain_messages(chat_history_texts):\n    # Convert chat_history_texts to list of HumanMessage and AIMessage\n    chat_history = []\n    for msg in chat_history_texts:\n        if msg.startswith(\"HumanMessage:\"):\n            content = msg[len(\"HumanMessage:\") :].strip()\n            chat_history.append(HumanMessage(content=content))\n        elif msg.startswith(\"AIMessage:\"):\n            content = msg[len(\"AIMessage:\") :].strip()\n            chat_history.append(AIMessage(content=content))\n\n    return chat_history\n\n# Assume messages_to_langchain_messages_new_implementation is imported or defined elsewhere\n\n\ndef test_messages_to_langchain_messages():\n    # Test case 1\n    input_data = [\n        \"HumanMessage: Hello!\",\n        \"AIMessage: How can I assist you today?\"\n    ]\n    expected_output = messages_to_langchain_messages(input_data)\n    new_output = messages_to_langchain_messages_new_implementation(input_data)\n    assert new_output == expected_output, \"Test case 1 failed\"\n\n    # Test case 2\n    input_data = [\n        \"AIMessage: This is a test.\",\n        \"HumanMessage: Indeed it is.\"\n    ]\n    expected_output = messages_to_langchain_messages(input_data)\n    new_output = messages_to_langchain_messages_new_implementation(input_data)\n    assert new_output == expected_output, \"Test case 2 failed\"\n\n    # Test case 3\n    input_data = [\n        \"HumanMessage: What's the weather today?\",\n        \"AIMessage: It's sunny and warm.\"\n    ]\n    expected_output = messages_to_langchain_messages(input_data)\n    new_output = messages_to_langchain_messages_new_implementation(input_data)\n    assert new_output == expected_output, \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_messages_to_langchain_messages()"
    },
    {
        "func_name": "LLMService.is_prompt_relevant_to_documents",
        "idx": "242",
        "repo_name": "Akumsk___pdflyx",
        "func_path": "llm_service.py",
        "orig_func": "def is_prompt_relevant_to_documents(self, prompt, sources):\n    \"\"\"\n        Determine if the prompt is relevant to the retrieved documents.\n        Implement a similarity check or any other logic as needed.\n        For demonstration, we'll perform a simple keyword overlap.\n        \"\"\"\n    prompt_keywords = set(prompt.lower().split())\n    source_text = ' '.join([doc.page_content.lower() for doc in sources])\n    source_keywords = set(source_text.split())\n    overlap = prompt_keywords.intersection(source_keywords)\n    relevance_threshold = 0.1\n    if len(prompt_keywords) == 0:\n        return False\n    similarity_ratio = len(overlap) / len(prompt_keywords)\n    return similarity_ratio >= relevance_threshold",
        "orig_context": "```python\n## helpers.py\nfrom datetime import datetime\n\ndef current_timestamp():\n    date_time = (\n            datetime.now().date().strftime(\"%Y-%m-%d\")\n            + \", \"\n            + datetime.now().time().strftime(\"%H:%M:%S\")\n    )\n    return date_time\n\n```\n\n\n```python\n## settings.py\nimport os\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nMODEL_NAME = \"gpt-4o\"\n\nDOCS_IN_RETRIEVER=5\n\n```\n\n\n```python\n## llm_service.py\nimport os\n\nimport warnings\n\nimport datetime\n\nimport fitz\n\nfrom langchain.chains.history_aware_retriever import create_history_aware_retriever\n\nfrom langchain.chains.retrieval import create_retrieval_chain\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nfrom langchain_community.vectorstores import FAISS\n\nfrom langchain_community.document_loaders import PyMuPDFLoader\n\nfrom langchain.text_splitter import CharacterTextSplitter\n\nfrom langchain.schema import Document\n\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nfrom settings import OPENAI_API_KEY, MODEL_NAME, CHAT_HISTORY_LEVEL, DOCS_IN_RETRIEVER\n\nfrom helpers import current_timestamp\n\nclass LLMService:\n    vector_store = None  # Class variable to store the vector store\n    def __init__(self, model_name=MODEL_NAME):\n        self.llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=model_name)\n\n    def load_and_index_documents(self, folder_path):\n        documents = []\n        found_valid_file = False\n\n        for filename in os.listdir(folder_path):\n            file_path = os.path.join(folder_path, filename)\n\n            if filename.endswith(\".pdf\"):\n                loader = PyMuPDFLoader(file_path)\n                docs = loader.load()\n                for doc in docs:\n                    doc.metadata[\"source\"] = filename\n                    documents.append(doc)\n                found_valid_file = True\n\n            elif filename.endswith(\".docx\"):\n                content = self.load_word_file(file_path)\n                doc = Document(page_content=content, metadata={\"source\": filename})\n                documents.append(doc)\n                found_valid_file = True\n\n            elif filename.endswith(\".xlsx\"):\n                content = self.load_excel_file(file_path)\n                doc = Document(page_content=content, metadata={\"source\": filename})\n                documents.append(doc)\n                found_valid_file = True\n\n        if not found_valid_file:\n            return \"No valid files found in the folder. Please provide PDF, Word, or Excel files.\"\n\n        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        split_docs = text_splitter.split_documents(documents)\n\n        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n        LLMService.vector_store = FAISS.from_documents(split_docs, embeddings)\n        return \"Documents successfully indexed.\"\n\n    def generate_response(self, prompt, chat_history=None):\n        if not LLMService.vector_store:\n            return (\n                \"Please set the folder path using /folder and ensure documents are loaded.\",\n                None,\n            )\n\n        # Ensure chat_history is a list\n        if chat_history is None:\n            chat_history = []\n\n        # Create the retriever with k=DOCS_IN_RETRIEVER\n        retriever = LLMService.vector_store.as_retriever(search_kwargs={'k': DOCS_IN_RETRIEVER})\n\n        # Create the history-aware retriever\n        retriever_prompt = ChatPromptTemplate.from_messages(\n            [\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"user\", \"{input}\"),\n                (\n                    \"user\",\n                    \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n                ),\n            ]\n        )\n\n        history_aware_retriever = create_history_aware_retriever(\n            llm=self.llm, retriever=retriever, prompt=retriever_prompt\n        )\n\n        # Create the question-answering chain\n        system_prompt = (\n            \"You are a project assistant from consultant side on design and construction projects. \"\n            \"Use the following pieces of retrieved context to answer \"\n            \"the question. If you don't know the answer, say that you \"\n            \"don't know. Do not include references to the source documents in your answer. \"\n            f\"Don't know. If you need to use current date, today is {current_timestamp()}. \"\n            \"If Prompt includes a request to provide a link to documents in context, respond with: Please follow the link below:\"\n            \" \\n\\n{context}\"\n        )\n\n        prompt_template = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"user\", \"{input}\"),\n            ]\n        )\n\n        question_answer_chain = create_stuff_documents_chain(self.llm, prompt_template)\n\n        # Create the retrieval chain using create_retrieval_chain\n        rag_chain = create_retrieval_chain(\n            retriever=history_aware_retriever, combine_docs_chain=question_answer_chain\n        )\n\n        # Run the chain with the provided prompt and chat history\n        result = rag_chain.invoke({\"input\": prompt, \"chat_history\": chat_history})\n\n        answer = result.get(\"answer\", \"\")\n        sources = result.get(\"context\", [])\n\n        # Implement similarity threshold logic\n        # For demonstration, we'll assume that if sources are returned, they are relevant.\n        # You can enhance this by calculating similarity scores and setting a threshold.\n\n        if not sources:\n            return answer, None\n\n        source_files = set(\n            [doc.metadata[\"source\"] for doc in sources if \"source\" in doc.metadata]\n        )\n\n        # Build the references\n        references = {}\n        for doc in sources:\n            filename = doc.metadata.get('source', 'Unknown')\n            page = doc.metadata.get('page', 'Unknown')\n            if filename not in references:\n                references[filename] = set()\n            references[filename].add(page)\n\n        # Only append references if the prompt is likely related to documents\n        # For a more robust solution, implement similarity checks here\n        # For example, calculate cosine similarity between prompt and retrieved docs\n\n        # Placeholder for similarity check\n        # Implement your similarity logic here and set 'is_relevant' accordingly\n        is_relevant = self.is_prompt_relevant_to_documents(prompt, sources)\n\n        if is_relevant:\n            answer_with_references = answer + \"\\n\\n------------------\" + \"\\nReferences:\\n\"\n\n            for doc_name, pages in references.items():\n                pages_list = sorted(pages)\n                pages_str = ', '.join(str(page) for page in pages_list)\n                answer_with_references += f\"{doc_name}, pages: {pages_str}\\n\"\n\n            return answer_with_references, source_files\n        else:\n            return answer, None\n\n    def is_prompt_relevant_to_documents(self, prompt, sources):\n        \"\"\"\n        Determine if the prompt is relevant to the retrieved documents.\n        Implement a similarity check or any other logic as needed.\n        For demonstration, we'll perform a simple keyword overlap.\n        \"\"\"\n        # Extract keywords from the prompt\n        prompt_keywords = set(prompt.lower().split())\n\n        # Extract keywords from the sources\n        source_text = ' '.join([doc.page_content.lower() for doc in sources])\n        source_keywords = set(source_text.split())\n\n        # Calculate overlap\n        overlap = prompt_keywords.intersection(source_keywords)\n\n        # Define a threshold for relevance\n        relevance_threshold = 0.1  # 10% overlap\n\n        if len(prompt_keywords) == 0:\n            return False\n\n        similarity_ratio = len(overlap) / len(prompt_keywords)\n\n        return similarity_ratio >= relevance_threshold\n\n    def get_empty_docs(self, folder_path):\n        \"\"\"\n        Scan through all PDF files in the specified folder and return a list of filenames\n        that contain one or more empty pages.\n        \"\"\"\n        empty_docs = []\n\n        # Suppress warnings from PyMuPDFLoader if necessary\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n            for filename in os.listdir(folder_path):\n                if not filename.lower().endswith(\".pdf\"):\n                    continue  # Skip non-PDF files\n\n                file_path = os.path.join(folder_path, filename)\n\n                try:\n                    # Open the PDF using PyMuPDF\n                    with fitz.open(file_path) as doc:\n                        for page_num in range(len(doc)):\n                            page = doc.load_page(page_num)\n                            text = page.get_text().strip()\n\n                            if not text:\n                                # Empty page found\n                                print(f\"Empty content on page {page_num} of document {filename}\")\n                                empty_docs.append(filename)\n                                break  # No need to check further pages in this document\n\n                except Exception as e:\n                    print(f\"Error processing {filename}: {e}\")\n                    continue  # Skip to the next file in case of an error\n\n        return empty_docs\n\n    def get_metadata(self, folder_path, db_service):\n        from langchain.schema import HumanMessage\n        metadata_list = []\n        existing_file_paths = []\n\n        for filename in os.listdir(folder_path):\n            file_path = os.path.join(folder_path, filename)\n\n            # Skip directories\n            if os.path.isdir(file_path):\n                continue\n\n            # Get the last modified time of the file from filesystem\n            timestamp = os.path.getmtime(file_path)\n            file_date_modified = datetime.datetime.fromtimestamp(timestamp)\n            date_modify_str = file_date_modified.strftime('%Y-%m-%d %H:%M:%S')\n\n            # Get dates from database\n            db_date_modified, date_of_analysis = db_service.get_file_dates(file_path)\n\n            if date_of_analysis:\n                # Compare file's date_modified with date_of_analysis\n                if file_date_modified <= date_of_analysis:\n                    # File has not been modified since last analysis; skip processing\n                    print(f\"Skipping {filename}; no changes detected.\")\n                    continue\n                else:\n                    print(f\"Re-analyzing {filename}; file has been modified.\")\n            else:\n                print(f\"Analyzing new file: {filename}\")\n\n            # Load content based on file type\n            if filename.endswith(\".pdf\"):\n                loader = PyMuPDFLoader(file_path)\n                docs = loader.load()\n                content = ' '.join([doc.page_content for doc in docs])\n\n            elif filename.endswith(\".docx\"):\n                content = self.load_word_file(file_path)\n\n            elif filename.endswith(\".xlsx\"):\n                content = self.load_excel_file(file_path)\n\n            else:\n                continue  # Skip unsupported file types\n\n            # Limit content to first 2000 characters to avoid long prompts\n            content_sample = content[:2000]\n\n            # Generate AI description and document type\n            prompt = (\n                \"Please analyze the following document content and provide the document type and a brief description in the following format:\\n\\n\"\n                \"Document Type: [document type]\\n\"\n                \"Description: [description]\\n\\n\"\n                \"Content:\\n\"\n                f\"{content_sample}\"\n            )\n\n            response = self.llm.invoke([HumanMessage(content=prompt)]).content\n\n            # Extract description and document type from response\n            document_type = ''\n            description = ''\n            lines = response.strip().split('\\n')\n            for line in lines:\n                if line.lower().startswith('document type:'):\n                    document_type = line[len('document type:'):].strip()\n                elif line.lower().startswith('description:'):\n                    description = line[len('description:'):].strip()\n\n            # Append metadata as a dictionary to the list\n            metadata_list.append({\n                'filename': filename,\n                'path_file': file_path,\n                'document_type': document_type,\n                'date_modified': date_modify_str,\n                'description': description\n            })\n\n            # After processing all files, mark files as deleted if they are not in the folder\n            db_service.mark_files_as_deleted(existing_file_paths)\n\n        return metadata_list\n\n```\n\n\n",
        "eval_script": "import os\n\n# Ensure the 'static/' directory exists for the fitz import to execute successfully.\nif not os.path.exists('static'):\n    os.makedirs('static')\n\n# Integrate the content from `helpers.py`\nfrom datetime import datetime\n\ndef current_timestamp():\n    date_time = (\n            datetime.now().date().strftime(\"%Y-%m-%d\")\n            + \", \"\n            + datetime.now().time().strftime(\"%H:%M:%S\")\n    )\n    return date_time\n\n# Integrate the content from `settings.py`\nOPENAI_API_KEY = \"mock-api-key\"  # Mocked API key, replace with real one during actual use\nMODEL_NAME = \"gpt-4o\"\nDOCS_IN_RETRIEVER=5\n\n# Original LLMService Code\nimport warnings\nimport datetime\nimport fitz\nfrom langchain.chains.history_aware_retriever import create_history_aware_retriever\nfrom langchain.chains.retrieval import create_retrieval_chain\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.schema import Document\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nclass LLMService:\n    vector_store = None  # Class variable to store the vector store\n    def __init__(self, model_name=MODEL_NAME):\n        self.llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=model_name)\n\n    def load_and_index_documents(self, folder_path):\n        documents = []\n        found_valid_file = False\n\n        for filename in os.listdir(folder_path):\n            file_path = os.path.join(folder_path, filename)\n\n            if filename.endswith(\".pdf\"):\n                loader = PyMuPDFLoader(file_path)\n                docs = loader.load()\n                for doc in docs:\n                    doc.metadata[\"source\"] = filename\n                    documents.append(doc)\n                found_valid_file = True\n\n            elif filename.endswith(\".docx\"):\n                content = self.load_word_file(file_path)\n                doc = Document(page_content=content, metadata={\"source\": filename})\n                documents.append(doc)\n                found_valid_file = True\n\n            elif filename.endswith(\".xlsx\"):\n                content = self.load_excel_file(file_path)\n                doc = Document(page_content=content, metadata={\"source\": filename})\n                documents.append(doc)\n                found_valid_file = True\n\n        if not found_valid_file:\n            return \"No valid files found in the folder. Please provide PDF, Word, or Excel files.\"\n\n        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        split_docs = text_splitter.split_documents(documents)\n\n        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n        LLMService.vector_store = FAISS.from_documents(split_docs, embeddings)\n        return \"Documents successfully indexed.\"\n\n    def generate_response(self, prompt, chat_history=None):\n        if not LLMService.vector_store:\n            return (\n                \"Please set the folder path using /folder and ensure documents are loaded.\",\n                None,\n            )\n\n        # Ensure chat_history is a list\n        if chat_history is None:\n            chat_history = []\n\n        # Create the retriever with k=DOCS_IN_RETRIEVER\n        retriever = LLMService.vector_store.as_retriever(search_kwargs={'k': DOCS_IN_RETRIEVER})\n\n        # Create the history-aware retriever\n        retriever_prompt = ChatPromptTemplate.from_messages(\n            [\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"user\", \"{input}\"),\n                (\n                    \"user\",\n                    \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n                ),\n            ]\n        )\n\n        history_aware_retriever = create_history_aware_retriever(\n            llm=self.llm, retriever=retriever, prompt=retriever_prompt\n        )\n\n        # Create the question-answering chain\n        system_prompt = (\n            \"You are a project assistant from consultant side on design and construction projects. \"\n            \"Use the following pieces of retrieved context to answer \"\n            \"the question. If you don't know the answer, say that you \"\n            \"don't know. Do not include references to the source documents in your answer. \"\n            f\"Don't know. If you need to use current date, today is {current_timestamp()}. \"\n            \"If Prompt includes a request to provide a link to documents in context, respond with: Please follow the link below:\"\n            \" \\n\\n{context}\"\n        )\n\n        prompt_template = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                MessagesPlaceholder(variable_name=\"chat_history\"),\n                (\"user\", \"{input}\"),\n            ]\n        )\n\n        question_answer_chain = create_stuff_documents_chain(self.llm, prompt_template)\n\n        # Create the retrieval chain using create_retrieval_chain\n        rag_chain = create_retrieval_chain(\n            retriever=history_aware_retriever, combine_docs_chain=question_answer_chain\n        )\n\n        # Run the chain with the provided prompt and chat history\n        result = rag_chain.invoke({\"input\": prompt, \"chat_history\": chat_history})\n\n        answer = result.get(\"answer\", \"\")\n        sources = result.get(\"context\", [])\n\n        # Implement similarity threshold logic\n        # For demonstration, we'll assume that if sources are returned, they are relevant.\n        # You can enhance this by calculating similarity scores and setting a threshold.\n\n        if not sources:\n            return answer, None\n\n        source_files = set(\n            [doc.metadata[\"source\"] for doc in sources if \"source\" in doc.metadata]\n        )\n\n        # Build the references\n        references = {}\n        for doc in sources:\n            filename = doc.metadata.get('source', 'Unknown')\n            page = doc.metadata.get('page', 'Unknown')\n            if filename not in references:\n                references[filename] = set()\n            references[filename].add(page)\n\n        # Only append references if the prompt is likely related to documents\n        # For a more robust solution, implement similarity checks here\n        # For example, calculate cosine similarity between prompt and retrieved docs\n\n        # Placeholder for similarity check\n        # Implement your similarity logic here and set 'is_relevant' accordingly\n        is_relevant = self.is_prompt_relevant_to_documents(prompt, sources)\n\n        if is_relevant:\n            answer_with_references = answer + \"\\n\\n------------------\" + \"\\nReferences:\\n\"\n\n            for doc_name, pages in references.items():\n                pages_list = sorted(pages)\n                pages_str = ', '.join(str(page) for page in pages_list)\n                answer_with_references += f\"{doc_name}, pages: {pages_str}\\n\"\n\n            return answer_with_references, source_files\n        else:\n            return answer, None\n\n    def is_prompt_relevant_to_documents(self, prompt, sources):\n        \"\"\"\n        Determine if the prompt is relevant to the retrieved documents.\n        Implement a similarity check or any other logic as needed.\n        For demonstration, we'll perform a simple keyword overlap.\n        \"\"\"\n        # Extract keywords from the prompt\n        prompt_keywords = set(prompt.lower().split())\n\n        # Extract keywords from the sources\n        source_text = ' '.join([doc.page_content.lower() for doc in sources])\n        source_keywords = set(source_text.split())\n\n        # Calculate overlap\n        overlap = prompt_keywords.intersection(source_keywords)\n\n        # Define a threshold for relevance\n        relevance_threshold = 0.1  # 10% overlap\n\n        if len(prompt_keywords) == 0:\n            return False\n\n        similarity_ratio = len(overlap) / len(prompt_keywords)\n\n        return similarity_ratio >= relevance_threshold\n\n\n    def get_empty_docs(self, folder_path):\n        \"\"\"\n        Scan through all PDF files in the specified folder and return a list of filenames\n        that contain one or more empty pages.\n        \"\"\"\n        empty_docs = []\n\n        # Suppress warnings from PyMuPDFLoader if necessary\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n            for filename in os.listdir(folder_path):\n                if not filename.lower().endswith(\".pdf\"):\n                    continue  # Skip non-PDF files\n\n                file_path = os.path.join(folder_path, filename)\n\n                try:\n                    # Open the PDF using PyMuPDF\n                    with fitz.open(file_path) as doc:\n                        for page_num in range(len(doc)):\n                            page = doc.load_page(page_num)\n                            text = page.get_text().strip()\n\n                            if not text:\n                                # Empty page found\n                                print(f\"Empty content on page {page_num} of document {filename}\")\n                                empty_docs.append(filename)\n                                break  # No need to check further pages in this document\n\n                except Exception as e:\n                    print(f\"Error processing {filename}: {e}\")\n                    continue  # Skip to the next file in case of an error\n\n        return empty_docs\n\n    def get_metadata(self, folder_path, db_service):\n        from langchain.schema import HumanMessage\n        metadata_list = []\n        existing_file_paths = []\n\n        for filename in os.listdir(folder_path):\n            file_path = os.path.join(folder_path, filename)\n\n            # Skip directories\n            if os.path.isdir(file_path):\n                continue\n\n            # Get the last modified time of the file from filesystem\n            timestamp = os.path.getmtime(file_path)\n            file_date_modified = datetime.datetime.fromtimestamp(timestamp)\n            date_modify_str = file_date_modified.strftime('%Y-%m-%d %H:%M:%S')\n\n            # Get dates from database\n            db_date_modified, date_of_analysis = db_service.get_file_dates(file_path)\n\n            if date_of_analysis:\n                # Compare file's date_modified with date_of_analysis\n                if file_date_modified <= date_of_analysis:\n                    # File has not been modified since last analysis; skip processing\n                    print(f\"Skipping {filename}; no changes detected.\")\n                    continue\n                else:\n                    print(f\"Re-analyzing {filename}; file has been modified.\")\n            else:\n                print(f\"Analyzing new file: {filename}\")\n\n            # Load content based on file type\n            if filename.endswith(\".pdf\"):\n                loader = PyMuPDFLoader(file_path)\n                docs = loader.load()\n                content = ' '.join([doc.page_content for doc in docs])\n\n            elif filename.endswith(\".docx\"):\n                content = self.load_word_file(file_path)\n\n            elif filename.endswith(\".xlsx\"):\n                content = self.load_excel_file(file_path)\n\n            else:\n                continue  # Skip unsupported file types\n\n            # Limit content to first 2000 characters to avoid long prompts\n            content_sample = content[:2000]\n\n            # Generate AI description and document type\n            prompt = (\n                \"Please analyze the following document content and provide the document type and a brief description in the following format:\\n\\n\"\n                \"Document Type: [document type]\\n\"\n                \"Description: [description]\\n\\n\"\n                \"Content:\\n\"\n                f\"{content_sample}\"\n            )\n\n            response = self.llm.invoke([HumanMessage(content=prompt)]).content\n\n            # Extract description and document type from response\n            document_type = ''\n            description = ''\n            lines = response.strip().split('\\n')\n            for line in lines:\n                if line.lower().startswith('document type:'):\n                    document_type = line[len('document type:'):].strip()\n                elif line.lower().startswith('description:'):\n                    description = line[len('description:'):].strip()\n\n            # Append metadata as a dictionary to the list\n            metadata_list.append({\n                'filename': filename,\n                'path_file': file_path,\n                'document_type': document_type,\n                'date_modified': date_modify_str,\n                'description': description\n            })\n\n            # After processing all files, mark files as deleted if they are not in the folder\n            db_service.mark_files_as_deleted(existing_file_paths)\n\n        return metadata_list\n\n# Test function to compare the old and new implementations\ndef test_is_prompt_relevant_to_documents():\n    llm_service = LLMService()\n\n    # Test case 1: No overlap\n    prompt1 = \"This is a test prompt\"\n    sources1 = [Document(page_content=\"Different content here\")]\n    assert llm_service.is_prompt_relevant_to_documents(prompt1, sources1) == llm_service.is_prompt_relevant_to_documents_new_implementation(prompt1, sources1)\n\n    # Test case 2: Some overlap\n    prompt2 = \"Relevant test document\"\n    sources2 = [Document(page_content=\"This is a relevant test document\")]\n    assert llm_service.is_prompt_relevant_to_documents(prompt2, sources2) == llm_service.is_prompt_relevant_to_documents_new_implementation(prompt2, sources2)\n\n    # Test case 3: Full overlap\n    prompt3 = \"Same content\"\n    sources3 = [Document(page_content=\"Same content\")]\n    assert llm_service.is_prompt_relevant_to_documents(prompt3, sources3) == llm_service.is_prompt_relevant_to_documents_new_implementation(prompt3, sources3)\n\n# Main function to call the test\nif __name__ == \"__main__\":\n    test_is_prompt_relevant_to_documents()\n    print(\"All tests passed successfully.\")"
    },
    {
        "func_name": "Status.folder_set",
        "idx": "243",
        "repo_name": "Akumsk___pdflyx",
        "func_path": "text.py",
        "orig_func": "@staticmethod\ndef folder_set(user_name, folder_path, file_list, empty_list=None):\n    response = f'Status Information:\\n\\nDear {user_name},\\nThe folder path is currently set to: {folder_path}\\n\\nValid Files:\\n{file_list}\\n\\n'\n    if empty_list:\n        empty_files = '\\n'.join(empty_list)\n        response += f'\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\nPlease review these files to ensure they are correctly formatted.'\n    return response",
        "orig_context": "```python\n## text.py\nclass Status:\n    @staticmethod\n    def folder_set(user_name, folder_path, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def folder_no_files(user_name, folder_path):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}, but no valid files were found.\\n\"\n        )\n\n    def upload_set(user_name, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context.\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def upload_no_files(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context, but no valid files were found.\\n\"\n        )\n\n    @staticmethod\n    def no_context(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            \"No context has been set yet. You can set it using the /knowledge_base command or by sending documents directly to the chat.\\n\"\n        )\n\n```\n\n\n",
        "eval_script": "## text.py\nclass Status:\n    @staticmethod\n    def folder_set(user_name, folder_path, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n\n\n    @staticmethod\n    def folder_no_files(user_name, folder_path):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}, but no valid files were found.\\n\"\n        )\n\n    def upload_set(user_name, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context.\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def upload_no_files(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context, but no valid files were found.\\n\"\n        )\n\n    @staticmethod\n    def no_context(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            \"No context has been set yet. You can set it using the /knowledge_base command or by sending documents directly to the chat.\\n\"\n        )\n\ndef test_folder_set():\n    # Test with no empty_list\n    assert Status.folder_set(\"Alice\", \"/path/to/folder\", [\"file1.txt\", \"file2.txt\"]) == \\\n           Status.folder_set_new_implementation(\"Alice\", \"/path/to/folder\", [\"file1.txt\", \"file2.txt\"])\n    \n    # Test with an empty empty_list\n    assert Status.folder_set(\"Bob\", \"/another/path\", [\"file3.txt\"], []) == \\\n           Status.folder_set_new_implementation(\"Bob\", \"/another/path\", [\"file3.txt\"], [])\n    \n    # Test with a populated empty_list\n    assert Status.folder_set(\"Charlie\", \"/third/path\", [\"file4.txt\"], [\"file5.txt\", \"file6.txt\"]) == \\\n           Status.folder_set_new_implementation(\"Charlie\", \"/third/path\", [\"file4.txt\"], [\"file5.txt\", \"file6.txt\"])\n\nif __name__ == \"__main__\":\n    test_folder_set()"
    },
    {
        "func_name": "Status.upload_set",
        "idx": "244",
        "repo_name": "Akumsk___pdflyx",
        "func_path": "text.py",
        "orig_func": "def upload_set(user_name, file_list, empty_list=None):\n    response = f'Status Information:\\n\\nDear {user_name},\\nDocuments sent to the chat are used as context.\\n\\nValid Files:\\n{file_list}\\n\\n'\n    if empty_list:\n        empty_files = '\\n'.join(empty_list)\n        response += f'\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\nPlease review these files to ensure they are correctly formatted.'\n    return response",
        "orig_context": "```python\n## text.py\nclass Status:\n    @staticmethod\n    def folder_set(user_name, folder_path, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def folder_no_files(user_name, folder_path):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}, but no valid files were found.\\n\"\n        )\n\n    def upload_set(user_name, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context.\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def upload_no_files(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context, but no valid files were found.\\n\"\n        )\n\n    @staticmethod\n    def no_context(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            \"No context has been set yet. You can set it using the /knowledge_base command or by sending documents directly to the chat.\\n\"\n        )\n\n```\n\n\n",
        "eval_script": "## text.py\nclass Status:\n    @staticmethod\n    def folder_set(user_name, folder_path, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def folder_no_files(user_name, folder_path):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"The folder path is currently set to: {folder_path}, but no valid files were found.\\n\"\n        )\n\n    def upload_set(user_name, file_list, empty_list=None):\n        response = (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context.\\n\\n\"\n            f\"Valid Files:\\n{file_list}\\n\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\u26a0\ufe0f **Note:** Some uploaded files are corrupted or contain unreadable pages:\\n{empty_files}\\n\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\"\n            )\n        return response\n\n    @staticmethod\n    def upload_no_files(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            f\"Documents sent to the chat are used as context, but no valid files were found.\\n\"\n        )\n\n    @staticmethod\n    def no_context(user_name):\n        return (\n            f\"Status Information:\\n\\n\"\n            f\"Dear {user_name},\\n\"\n            \"No context has been set yet. You can set it using the /knowledge_base command or by sending documents directly to the chat.\\n\"\n        )\n\ndef test_upload_set():\n    # Test case 1: No empty files\n    result1_old = Status.upload_set(\"Alice\", [\"file1.txt\", \"file2.txt\"])\n    result1_new = Status.upload_set_new_implementation(\"Alice\", [\"file1.txt\", \"file2.txt\"])\n    assert result1_old == result1_new, \"Test case 1 failed\"\n\n    # Test case 2: With some empty files\n    result2_old = Status.upload_set(\"Bob\", [\"file3.txt\"], [\"file3.txt\"])\n    result2_new = Status.upload_set_new_implementation(\"Bob\", [\"file3.txt\"], [\"file3.txt\"])\n    assert result2_old == result2_new, \"Test case 2 failed\"\n\n    # Test case 3: No valid files, a user with no empty list\n    result3_old = Status.upload_set(\"Charlie\", [])\n    result3_new = Status.upload_set_new_implementation(\"Charlie\", [])\n    assert result3_old == result3_new, \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_upload_set()\n"
    },
    {
        "func_name": "Responses.folder_is_set",
        "idx": "245",
        "repo_name": "Akumsk___pdflyx",
        "func_path": "text.py",
        "orig_func": "@staticmethod\ndef folder_is_set(folder_path, empty_list=None):\n    response = f'\ud83d\udcc1 **Folder Path Set Successfully:**\\n\\nFolder path: {folder_path}\\nValid files have been indexed.\\n'\n    if empty_list:\n        empty_files = '\\n'.join(empty_list)\n        response += f'\\n\u26a0\ufe0f **Note:** Some files are corrupted or contain empty pages:\\n{empty_files}\\nPlease review these files to ensure they are correctly formatted.\\nUse the /status command to view detailed information.'\n    else:\n        response += '\\nAll files have been successfully indexed without any issues.'\n    return response",
        "orig_context": "```python\n## text.py\nclass Responses:\n    @staticmethod\n    def request_access():\n        return \"Please provide the folder path for your documents:\"\n\n    @staticmethod\n    def grant_access_success(user_id):\n        return f\"User {user_id} has been granted access.\"\n\n    @staticmethod\n    def grant_access_usage():\n        return \"Usage: /grant_access <user_id>\"\n\n    @staticmethod\n    def access_denied():\n        return \"You do not have access, please use /request_access.\"\n\n    @staticmethod\n    def access_requested():\n        return \"Your access request has been sent to the admin.\"\n\n    @staticmethod\n    def unauthorized_action():\n        return \"You are not authorized to perform this action.\"\n\n    @staticmethod\n    def invalid_folder_path():\n        return \"Invalid folder path. Please provide a valid path.\"\n\n    @staticmethod\n    def no_valid_files():\n        return \"No valid files found in the folder. Please provide a folder containing valid documents.\"\n\n    @staticmethod\n    def documents_indexed():\n        return \"Documents successfully indexed.\"\n\n    @staticmethod\n    def folder_is_set(folder_path, empty_list=None):\n        response = (\n            f\"\ud83d\udcc1 **Folder Path Set Successfully:**\\n\\n\"\n            f\"Folder path: {folder_path}\\n\"\n            f\"Valid files have been indexed.\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\\n\u26a0\ufe0f **Note:** Some files are corrupted or contain empty pages:\\n{empty_files}\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\\n\"\n                f\"Use the /status command to view detailed information.\"\n            )\n        else:\n            response += \"\\nAll files have been successfully indexed without any issues.\"\n        return response\n\n\n    @staticmethod\n    def indexing_error():\n        return \"An error occurred while loading and indexing your documents. Please try again later.\"\n\n    @staticmethod\n    def upload_success():\n        return (\n            \"Your documents have been uploaded and indexed successfully. \"\n            \"You may now ask any questions related to these documents.\"\n        )\n\n    @staticmethod\n    def upload_partial_success():\n        return (\n            \"You have selected different file types. Only the PDF files have been uploaded and indexed. \"\n            \"You may now ask any questions related to these PDF documents.\"\n        )\n\n    @staticmethod\n    def unsupported_files():\n        return (\n            \"I'm sorry, but only PDF files are supported. Please upload your documents in PDF format.\"\n        )\n\n    @staticmethod\n    def processing_error():\n        return (\n            \"I'm sorry, but I couldn't process the files you sent. \"\n            \"Please ensure they are in PDF format and try again.\"\n        )\n\n    @staticmethod\n    def generic_error():\n        return (\n            \"An unexpected error occurred while processing your files. Please try again later.\"\n        )\n\n    @staticmethod\n    def no_files_received():\n        return (\n            \"I'm sorry, but I didn't receive any files. Please attach your documents and try again.\"\n        )\n\n    @staticmethod\n    def file_too_large():\n        return \"I'm sorry, but I couldn't process the file more than 20Mb. Please upload PDF files less than 20Mb.\"\n\n```\n\n\n",
        "eval_script": "## text.py\nclass Responses:\n    @staticmethod\n    def request_access():\n        return \"Please provide the folder path for your documents:\"\n\n    @staticmethod\n    def grant_access_success(user_id):\n        return f\"User {user_id} has been granted access.\"\n\n    @staticmethod\n    def grant_access_usage():\n        return \"Usage: /grant_access <user_id>\"\n\n    @staticmethod\n    def access_denied():\n        return \"You do not have access, please use /request_access.\"\n\n    @staticmethod\n    def access_requested():\n        return \"Your access request has been sent to the admin.\"\n\n    @staticmethod\n    def unauthorized_action():\n        return \"You are not authorized to perform this action.\"\n\n    @staticmethod\n    def invalid_folder_path():\n        return \"Invalid folder path. Please provide a valid path.\"\n\n    @staticmethod\n    def no_valid_files():\n        return \"No valid files found in the folder. Please provide a folder containing valid documents.\"\n\n    @staticmethod\n    def documents_indexed():\n        return \"Documents successfully indexed.\"\n\n    @staticmethod\n    def folder_is_set(folder_path, empty_list=None):\n        response = (\n            f\"\ud83d\udcc1 **Folder Path Set Successfully:**\\n\\n\"\n            f\"Folder path: {folder_path}\\n\"\n            f\"Valid files have been indexed.\\n\"\n        )\n        if empty_list:\n            empty_files = \"\\n\".join(empty_list)\n            response += (\n                f\"\\n\u26a0\ufe0f **Note:** Some files are corrupted or contain empty pages:\\n{empty_files}\\n\"\n                f\"Please review these files to ensure they are correctly formatted.\\n\"\n                f\"Use the /status command to view detailed information.\"\n            )\n        else:\n            response += \"\\nAll files have been successfully indexed without any issues.\"\n        return response\n\n    @staticmethod\n\n\n    @staticmethod\n    def indexing_error():\n        return \"An error occurred while loading and indexing your documents. Please try again later.\"\n\n    @staticmethod\n    def upload_success():\n        return (\n            \"Your documents have been uploaded and indexed successfully. \"\n            \"You may now ask any questions related to these documents.\"\n        )\n\n    @staticmethod\n    def upload_partial_success():\n        return (\n            \"You have selected different file types. Only the PDF files have been uploaded and indexed. \"\n            \"You may now ask any questions related to these PDF documents.\"\n        )\n\n    @staticmethod\n    def unsupported_files():\n        return (\n            \"I'm sorry, but only PDF files are supported. Please upload your documents in PDF format.\"\n        )\n\n    @staticmethod\n    def processing_error():\n        return (\n            \"I'm sorry, but I couldn't process the files you sent. \"\n            \"Please ensure they are in PDF format and try again.\"\n        )\n\n    @staticmethod\n    def generic_error():\n        return (\n            \"An unexpected error occurred while processing your files. Please try again later.\"\n        )\n\n    @staticmethod\n    def no_files_received():\n        return (\n            \"I'm sorry, but I didn't receive any files. Please attach your documents and try again.\"\n        )\n\n    @staticmethod\n    def file_too_large():\n        return \"I'm sorry, but I couldn't process the file more than 20Mb. Please upload PDF files less than 20Mb.\"\n\n\ndef test_folder_is_set():\n    # Test case 1: Without empty_list\n    folder_path = \"/home/user/tmp\"\n    assert Responses.folder_is_set(folder_path) == Responses.folder_is_set_new_implementation(folder_path)\n    \n    # Test case 2: With empty_list containing some files\n    empty_list = [\"file1.txt\", \"file2.pdf\"]\n    assert Responses.folder_is_set(folder_path, empty_list) == Responses.folder_is_set_new_implementation(folder_path, empty_list)\n    \n    # Test case 3: With empty_list as an empty list\n    empty_list = []\n    assert Responses.folder_is_set(folder_path, empty_list) == Responses.folder_is_set_new_implementation(folder_path, empty_list)\n\n\nif __name__ == \"__main__\":\n    test_folder_is_set()"
    },
    {
        "func_name": "EditTool.validate_path",
        "idx": "248",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-windows-streamlit/computer_use/tools/edit.py",
        "orig_func": "def validate_path(self, command: str, path: Path):\n    \"\"\"\n        Check that the path/command combination is valid.\n        \"\"\"\n    if not path.is_absolute():\n        suggested_path = Path(os.getcwd()) / path\n        raise ToolError(f'The path {path} is not an absolute path. Maybe you meant {suggested_path}?')\n    if command != 'create' and (not path.exists()):\n        raise ToolError(f'The path {path} does not exist')\n    if command == 'create' and path.exists():\n        raise ToolError(f'File already exists at {path}')\n    if command != 'view' and path.is_dir():\n        raise ToolError(f'The path {path} is a directory')",
        "orig_context": "```python\n## computer-use-windows-streamlit/computer_use/tools/base.py\nfrom abc import ABCMeta, abstractmethod\n\nfrom dataclasses import dataclass, fields, replace\n\nfrom typing import Any\n\nfrom anthropic.types.beta import BetaToolUnionParam\n\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"Executes the tool with the given arguments.\"\"\"\n        ...\n\n    @abstractmethod\n    def to_params(\n        self,\n    ) -> BetaToolUnionParam:\n        raise NotImplementedError\n\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\nclass CLIResult(ToolResult):\n    \"\"\"A ToolResult that can be rendered as a CLI output.\"\"\"\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n```\n\n\n```python\n## computer-use-windows-streamlit/computer_use/tools/run.py\nimport asyncio\n\nTRUNCATED_MESSAGE: str = \"<response clipped><NOTE>To save on context, only part of this file has been shown. You should use appropriate search/filter commands for your platform to find specific content.</NOTE>\"\n\nMAX_RESPONSE_LEN: int = 16000\n\nasync def run(cmd: str, timeout: float = 30.0):\n    \"\"\"Run a shell command asynchronously with a timeout.\"\"\"\n    process = await asyncio.create_subprocess_shell(\n        cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n    )\n\n    try:\n        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)\n    except asyncio.TimeoutError:\n        process.kill()\n        return (\n            process.returncode,\n            \"\",\n            f\"Command timed out after {timeout} seconds\",\n        )\n\n    stdout_str = stdout.decode() if stdout else \"\"\n    stderr_str = stderr.decode() if stderr else \"\"\n\n    if len(stdout_str) > MAX_RESPONSE_LEN:\n        stdout_str = stdout_str[:MAX_RESPONSE_LEN] + \"\\n\" + TRUNCATED_MESSAGE\n\n    return process.returncode, stdout_str, stderr_str\n\n```\n\n\n```python\n## computer-use-windows-streamlit/computer_use/tools/edit.py\nimport os\n\nfrom pathlib import Path\n\nfrom typing import ClassVar, Literal, Optional, TypedDict\n\nfrom .base import BaseAnthropicTool, CLIResult, ToolError\n\nfrom .run import run\n\nSNIPPET_LINES = 3\n\nclass ToolEditParam(TypedDict):\n    type: str\n    name: str\n\nclass EditTool(BaseAnthropicTool):\n    \"\"\"\n    A tool that allows the agent to edit files.\n    The tool parameters are defined by Anthropic and are not editable.\n    \"\"\"\n\n    name: ClassVar[Literal[\"str_replace_editor\"]] = \"str_replace_editor\"\n    api_type: ClassVar[Literal[\"text_editor_20241022\"]] = \"text_editor_20241022\"  # Updated to match expected type\n    _file_history: dict[Path, list[str]]\n\n    def __init__(self):\n        self._file_history = {}\n        super().__init__()\n\n    def validate_path(self, command: str, path: Path):\n        \"\"\"\n        Check that the path/command combination is valid.\n        \"\"\"\n        if not path.is_absolute():\n            suggested_path = Path(os.getcwd()) / path\n            raise ToolError(\n                f\"The path {path} is not an absolute path. Maybe you meant {suggested_path}?\"\n            )\n\n        if command != \"create\" and not path.exists():\n            raise ToolError(f\"The path {path} does not exist\")\n\n        if command == \"create\" and path.exists():\n            raise ToolError(f\"File already exists at {path}\")\n\n        if command != \"view\" and path.is_dir():\n            raise ToolError(f\"The path {path} is a directory\")\n\n    async def __call__(\n        self,\n        *,\n        command: str,\n        path: str,\n        old_str: str | None = None,\n        new_str: str | None = None,\n        file_text: str | None = None,\n        insert_line: int | None = None,\n        view_range: list[int] | None = None,\n        **kwargs,\n    ):\n        path = Path(path)\n        self.validate_path(command, path)\n\n        if command == \"view\":\n            if path.is_dir():\n                # For directories, list contents using platform-appropriate method\n                if os.name == 'nt':  # Windows\n                    _, stdout, stderr = await run(\n                        f'dir /B \"{path}\"'\n                    )\n                else:  # Unix\n                    _, stdout, stderr = await run(\n                        rf\"find {path} -maxdepth 2 -not -path '*/\\.*'\"\n                    )\n                if not stderr:\n                    stdout = f\"Here's the files and directories in {path}:\\n{stdout}\\n\"\n                return CLIResult(output=stdout, error=stderr)\n\n            file_content = path.read_text()\n\n            if view_range:\n                if len(view_range) != 2:\n                    raise ToolError(\n                        \"The `view_range` parameter must be a list of two integers\"\n                    )\n                init_line, final_line = view_range\n                if init_line > final_line:\n                    raise ToolError(\n                        \"Invalid `view_range`: first number must be less than or equal to second number\"\n                    )\n                file_lines = file_content.split(\"\\n\")\n                n_lines_file = len(file_lines)\n                if final_line == -1:\n                    file_content = \"\\n\".join(file_lines[init_line - 1 :])\n                else:\n                    file_content = \"\\n\".join(file_lines[init_line - 1 : final_line])\n\n            return self._format_file_content(path, file_content, init_line=view_range[0] if view_range else 1)\n\n        if command == \"create\":\n            if file_text is None:\n                raise ToolError(\"Parameter `file_text` is required for create command\")\n\n            path.write_text(file_text)\n            return CLIResult(output=f\"File {path} has been created.\")\n\n        if command == \"str_replace\":\n            if old_str is None or new_str is None:\n                raise ToolError(\n                    \"Parameters `old_str` and `new_str` are required for str_replace command\"\n                )\n\n            file_content = path.read_text()\n            occurrences = file_content.count(old_str)\n\n            if occurrences == 0:\n                raise ToolError(f\"String '{old_str}' not found in {path}\")\n\n            if occurrences > 1:\n                file_content_lines = file_content.split(\"\\n\")\n                lines = [\n                    f\"{i+1}: {line}\"\n                    for i, line in enumerate(file_content_lines)\n                    if old_str in line\n                ]\n                raise ToolError(\n                    f\"Found {occurrences} occurrences of '{old_str}' in {path}. Please be more specific.\\nLines containing the string:\\n\"\n                    + \"\\n\".join(lines)\n                )\n\n            new_file_content = file_content.replace(old_str, new_str, 1)\n            self._file_history.setdefault(path, []).append(file_content)\n            path.write_text(new_file_content)\n\n            # Create a snippet of the edited section\n            replacement_line = file_content.split(old_str)[0].count(\"\\n\")\n            start_line = max(0, replacement_line - SNIPPET_LINES)\n            end_line = replacement_line + SNIPPET_LINES + new_str.count(\"\\n\")\n            snippet = \"\\n\".join(new_file_content.split(\"\\n\")[start_line : end_line + 1])\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"insert\":\n            if new_str is None:\n                raise ToolError(\"Parameter `new_str` is required for insert command\")\n            if insert_line is None:\n                raise ToolError(\"Parameter `insert_line` is required for insert command\")\n\n            new_str = new_str.expandtabs()\n            file_text = path.read_text()\n            file_text_lines = file_text.split(\"\\n\")\n            n_lines_file = len(file_text_lines)\n\n            if insert_line < 0 or (\n                insert_line > n_lines_file and insert_line != n_lines_file\n            ):\n                raise ToolError(\n                    f\"Invalid `insert_line` parameter: {insert_line}. File has {n_lines_file} lines.\"\n                )\n\n            new_str_lines = new_str.split(\"\\n\")\n            new_file_text_lines = (\n                file_text_lines[:insert_line]\n                + new_str_lines\n                + file_text_lines[insert_line:]\n            )\n            new_file_text = \"\\n\".join(new_file_text_lines)\n\n            self._file_history.setdefault(path, []).append(file_text)\n            path.write_text(new_file_text)\n\n            # Create a snippet of the edited section\n            start_line = max(0, insert_line - SNIPPET_LINES)\n            end_line = min(\n                len(new_file_text_lines),\n                insert_line + len(new_str_lines) + SNIPPET_LINES,\n            )\n            snippet_lines = new_file_text_lines[start_line:end_line]\n            snippet = \"\\n\".join(snippet_lines)\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"undo_edit\":\n            if path not in self._file_history or not self._file_history[path]:\n                raise ToolError(f\"No edit history found for {path}\")\n\n            last_content = self._file_history[path].pop()\n            path.write_text(last_content)\n\n            return CLIResult(output=f\"Last edit to {path} undone successfully\")\n\n        raise ToolError(f\"Invalid command: {command}\")\n\n    def _format_file_content(\n        self, path: Path, file_content: str, init_line: int = 1\n    ) -> CLIResult:\n        \"\"\"Format file content with line numbers.\"\"\"\n        if not file_content:\n            return CLIResult(output=f\"File {path} is empty\")\n\n        file_descriptor = f\"lines {init_line}-{init_line + file_content.count(chr(10))} of {path}\"\n        if init_line == 1 and file_content.count(chr(10)) == path.read_text().count(\n            chr(10)\n        ):\n            file_descriptor = str(path)\n\n        if file_content:\n            file_content = file_content.expandtabs()\n            file_content = \"\\n\".join(\n                [\n                    f\"{i + init_line:6}\\t{line}\"\n                    for i, line in enumerate(file_content.split(\"\\n\"))\n                ]\n            )\n\n        return CLIResult(\n            output=(\n                f\"Here's the content of {file_descriptor}:\\n\"\n                + file_content\n                + \"\\n\"\n            )\n        )\n\n    def to_params(self) -> ToolEditParam:\n        return {\n            \"type\": self.api_type,\n            \"name\": self.name,\n        }\n\n```\n\n\n",
        "eval_script": "# Import necessary modules and classes\nimport asyncio\nimport os\nfrom pathlib import Path\nfrom abc import ABCMeta, abstractmethod\nfrom dataclasses import dataclass, fields, replace\nfrom typing import Any, ClassVar, Literal, Optional, TypedDict\n\n# Define the run function from computer-use-windows-streamlit/computer_use/tools/run.py\nTRUNCATED_MESSAGE: str = \"<response clipped><NOTE>To save on context, only part of this file has been shown. You should use appropriate search/filter commands for your platform to find specific content.</NOTE>\"\nMAX_RESPONSE_LEN: int = 16000\n\nasync def run(cmd: str, timeout: float = 30.0):\n    \"\"\"Run a shell command asynchronously with a timeout.\"\"\"\n    process = await asyncio.create_subprocess_shell(\n        cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n    )\n    try:\n        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)\n    except asyncio.TimeoutError:\n        process.kill()\n        return (\n            process.returncode,\n            \"\",\n            f\"Command timed out after {timeout} seconds\",\n        )\n    stdout_str = stdout.decode() if stdout else \"\"\n    stderr_str = stderr.decode() if stderr else \"\"\n    if len(stdout_str) > MAX_RESPONSE_LEN:\n        stdout_str = stdout_str[:MAX_RESPONSE_LEN] + \"\\n\" + TRUNCATED_MESSAGE\n    return process.returncode, stdout_str, stderr_str\n\n# Define base classes and error handling from computer-use-windows-streamlit/computer_use/tools/base.py\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"Executes the tool with the given arguments.\"\"\"\n        ...\n\n    @abstractmethod\n    def to_params(\n        self,\n    ) -> Any:\n        raise NotImplementedError\n\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\nclass CLIResult(ToolResult):\n    \"\"\"A ToolResult that can be rendered as a CLI output.\"\"\"\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n# Define the EditTool and related classes from computer-use-windows-streamlit/computer_use/tools/edit.py\nSNIPPET_LINES = 3\n\nclass ToolEditParam(TypedDict):\n    type: str\n    name: str\n\nclass EditTool(BaseAnthropicTool):\n    \"\"\"A tool that allows the agent to edit files.\"\"\"\n\n    name: ClassVar[Literal[\"str_replace_editor\"]] = \"str_replace_editor\"\n    api_type: ClassVar[Literal[\"text_editor_20241022\"]] = \"text_editor_20241022\"\n    _file_history: dict[Path, list[str]]\n\n    def __init__(self):\n        self._file_history = {}\n        super().__init__()\n\n    def validate_path(self, command: str, path: Path):\n        \"\"\"Check that the path/command combination is valid.\"\"\"\n        if not path.is_absolute():\n            suggested_path = Path(os.getcwd()) / path\n            raise ToolError(\n                f\"The path {path} is not an absolute path. Maybe you meant {suggested_path}?\"\n            )\n\n        if command != \"view\" and path.is_dir():\n            raise ToolError(f\"The path {path} is a directory\")\n\n        if command != \"create\" and not path.exists():\n            raise ToolError(f\"The path {path} does not exist\")\n\n        if command == \"create\" and path.exists():\n            raise ToolError(f\"File already exists at {path}\")\n\n\n    async def __call__(\n        self,\n        *,\n        command: str,\n        path: str,\n        old_str: str | None = None,\n        new_str: str | None = None,\n        file_text: str | None = None,\n        insert_line: int | None = None,\n        view_range: list[int] | None = None,\n        **kwargs,\n    ) -> CLIResult:\n        path = Path(path)\n        self.validate_path(command, path)\n\n        if command == \"view\":\n            if path.is_dir():\n                # For directories, list contents using platform-appropriate method\n                if os.name == 'nt':  # Windows\n                    _, stdout, stderr = await run(\n                        f'dir /B \"{path}\"'\n                    )\n                else:  # Unix\n                    _, stdout, stderr = await run(\n                        rf\"find {path} -maxdepth 2 -not -path '*/\\\\.*'\"\n                    )\n                if not stderr:\n                    stdout = f\"Here's the files and directories in {path}:\\n{stdout}\\n\"\n                return CLIResult(output=stdout, error=stderr)\n\n            file_content = path.read_text()\n\n            if view_range:\n                if len(view_range) != 2:\n                    raise ToolError(\n                        \"The `view_range` parameter must be a list of two integers\"\n                    )\n                init_line, final_line = view_range\n                if init_line > final_line:\n                    raise ToolError(\n                        \"Invalid `view_range`: first number must be less than or equal to second number\"\n                    )\n                file_lines = file_content.split(\"\\n\")\n                if final_line == -1:\n                    file_content = \"\\n\".join(file_lines[init_line - 1:])\n                else:\n                    file_content = \"\\n\".join(file_lines[init_line - 1: final_line])\n\n            return self._format_file_content(path, file_content, init_line=view_range[0] if view_range else 1)\n\n        if command == \"create\":\n            if file_text is None:\n                raise ToolError(\"Parameter `file_text` is required for create command\")\n\n            path.write_text(file_text)\n            return CLIResult(output=f\"File {path} has been created.\")\n\n        if command == \"str_replace\":\n            if old_str is None or new_str is None:\n                raise ToolError(\n                    \"Parameters `old_str` and `new_str` are required for str_replace command\"\n                )\n\n            file_content = path.read_text()\n            occurrences = file_content.count(old_str)\n\n            if occurrences == 0:\n                raise ToolError(f\"String '{old_str}' not found in {path}\")\n\n            if occurrences > 1:\n                file_content_lines = file_content.split(\"\\n\")\n                lines = [\n                    f\"{i+1}: {line}\"\n                    for i, line in enumerate(file_content_lines)\n                    if old_str in line\n                ]\n                raise ToolError(\n                    f\"Found {occurrences} occurrences of '{old_str}' in {path}. Please be more specific.\\nLines containing the string:\\n\"\n                    + \"\\n\".join(lines)\n                )\n\n            new_file_content = file_content.replace(old_str, new_str, 1)\n            self._file_history.setdefault(path, []).append(file_content)\n            path.write_text(new_file_content)\n\n            # Create a snippet of the edited section\n            replacement_line = file_content.split(old_str)[0].count(\"\\n\")\n            start_line = max(0, replacement_line - SNIPPET_LINES)\n            end_line = replacement_line + SNIPPET_LINES + new_str.count(\"\\n\")\n            snippet = \"\\n\".join(new_file_content.split(\"\\n\")[start_line: end_line + 1])\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"insert\":\n            if new_str is None:\n                raise ToolError(\"Parameter `new_str` is required for insert command\")\n            if insert_line is None:\n                raise ToolError(\"Parameter `insert_line` is required for insert command\")\n\n            new_str = new_str.expandtabs()\n            file_text = path.read_text()\n            file_text_lines = file_text.split(\"\\n\")\n            n_lines_file = len(file_text_lines)\n\n            if insert_line < 0 or (\n                insert_line > n_lines_file and insert_line != n_lines_file\n            ):\n                raise ToolError(\n                    f\"Invalid `insert_line` parameter: {insert_line}. File has {n_lines_file} lines.\"\n                )\n\n            new_str_lines = new_str.split(\"\\n\")\n            new_file_text_lines = (\n                file_text_lines[:insert_line]\n                + new_str_lines\n                + file_text_lines[insert_line:]\n            )\n            new_file_text = \"\\n\".join(new_file_text_lines)\n\n            self._file_history.setdefault(path, []).append(file_text)\n            path.write_text(new_file_text)\n\n            # Create a snippet of the edited section\n            start_line = max(0, insert_line - SNIPPET_LINES)\n            end_line = min(\n                len(new_file_text_lines),\n                insert_line + len(new_str_lines) + SNIPPET_LINES,\n            )\n            snippet_lines = new_file_text_lines[start_line:end_line]\n            snippet = \"\\n\".join(snippet_lines)\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"undo_edit\":\n            if path not in self._file_history or not self._file_history[path]:\n                raise ToolError(f\"No edit history found for {path}\")\n\n            last_content = self._file_history[path].pop()\n            path.write_text(last_content)\n\n            return CLIResult(output=f\"Last edit to {path} undone successfully\")\n\n        raise ToolError(f\"Invalid command: {command}\")\n\n    def _format_file_content(\n        self, path: Path, file_content: str, init_line: int = 1\n    ) -> CLIResult:\n        \"\"\"Format file content with line numbers.\"\"\"\n        if not file_content:\n            return CLIResult(output=f\"File {path} is empty\")\n\n        file_descriptor = f\"lines {init_line}-{init_line + file_content.count(chr(10))} of {path}\"\n        if init_line == 1 and file_content.count(chr(10)) == path.read_text().count(\n            chr(10)\n        ):\n            file_descriptor = str(path)\n\n        if file_content:\n            file_content = file_content.expandtabs()\n            file_content = \"\\n\".join(\n                [\n                    f\"{i + init_line:6}\\t{line}\"\n                    for i, line in enumerate(file_content.split(\"\\n\"))\n                ]\n            )\n\n        return CLIResult(\n            output=(\n                f\"Here's the content of {file_descriptor}:\\n\"\n                + file_content\n                + \"\\n\"\n            )\n        )\n\n    def to_params(self) -> ToolEditParam:\n        return {\n            \"type\": self.api_type,\n            \"name\": self.name,\n        }\n\n\ndef test_validate_path():\n    tool = EditTool()\n    \n    # Test 1: Validating a non-absolute path\n    try:\n        tool.validate_path('create', Path('relative/path.txt'))\n    except ToolError as e:\n        assert str(e) == f\"The path relative/path.txt is not an absolute path. Maybe you meant {Path(os.getcwd()) / 'relative/path.txt'}?\"\n      \n    try:\n        tool.validate_path_new_implementation('create', Path('relative/path.txt'))\n    except ToolError as e:\n        assert str(e) == f\"The path relative/path.txt is not an absolute path. Maybe you meant {Path(os.getcwd()) / 'relative/path.txt'}?\"\n\n    # Test 2: Command is 'create' but file already exists\n    temp_path = Path('/home/user/tmp/already_exists.txt')\n    temp_path.touch()\n    try:\n        tool.validate_path('create', temp_path)\n    except ToolError as e:\n        assert str(e) == f\"File already exists at {temp_path}\"\n\n    try:\n        tool.validate_path_new_implementation('create', temp_path)\n    except ToolError as e:\n        assert str(e) == f\"File already exists at {temp_path}\"\n\n    # Test 3: Given path is a directory and command is not 'view'\n    temp_dir = Path('/home/user/tmp/test_dir')\n    temp_dir.mkdir(exist_ok=True)\n    try:\n        tool.validate_path('create', temp_dir)\n    except ToolError as e:\n        assert str(e) == f\"The path {temp_dir} is a directory\"\n\n    try:\n        tool.validate_path_new_implementation('create', temp_dir)\n    except ToolError as e:\n        assert str(e) == f\"The path {temp_dir} is a directory\"\n    \n    # Clean up\n    temp_path.unlink()\n    temp_dir.rmdir()\n\nif __name__ == \"__main__\":\n    test_validate_path()"
    },
    {
        "func_name": "ToolResult.replace",
        "idx": "249",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-windows-streamlit/computer_use/tools/base.py",
        "orig_func": "def replace(self, **kwargs):\n    \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n    return replace(self, **kwargs)",
        "orig_context": "```python\n## computer-use-windows-streamlit/computer_use/tools/base.py\nfrom dataclasses import dataclass, fields, replace\n\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\n```\n\n\n",
        "eval_script": "from dataclasses import dataclass, fields, replace\n\n@dataclass\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\n\ndef test_replace():\n    # Create tool result instances for testing\n    tr_original = ToolResult(output=\"output1\", error=\"error1\", base64_image=\"image1\", system=\"system1\")\n    tr_new = ToolResult(output=\"output1\", error=\"error1\", base64_image=\"image1\", system=\"system1\")\n\n    # Test case 1: Replace output\n    tr_replaced_original = tr_original.replace(output=\"new_output\")\n    tr_replaced_new = tr_new.replace_new_implementation(output=\"new_output\")\n    assert tr_replaced_original == tr_replaced_new, \"Mismatch in replace output functionality\"\n\n    # Test case 2: Replace error\n    tr_replaced_original = tr_original.replace(error=\"new_error\")\n    tr_replaced_new = tr_new.replace_new_implementation(error=\"new_error\")\n    assert tr_replaced_original == tr_replaced_new, \"Mismatch in replace error functionality\"\n\n    # Test case 3: Replace multiple fields\n    tr_replaced_original = tr_original.replace(output=\"new_output\", base64_image=\"new_image\")\n    tr_replaced_new = tr_new.replace_new_implementation(output=\"new_output\", base64_image=\"new_image\")\n    assert tr_replaced_original == tr_replaced_new, \"Mismatch in replacing multiple fields functionality\"\n\nif __name__ == \"__main__\":\n    test_replace()"
    },
    {
        "func_name": "BaseAnthropicTool.handle_error",
        "idx": "251",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-langchain-agent/Agent/Tools/base.py",
        "orig_func": "def handle_error(self, e: Exception) -> ToolResult:\n    \"\"\"Convert any exception to a ToolResult.\"\"\"\n    if isinstance(e, ToolError):\n        return e.to_result()\n    return ToolResult(output=None, error=str(e), base64_image=None, system=None)",
        "orig_context": "```python\n## computer-use-langchain-agent/Agent/Tools/base.py\nfrom typing import Optional, TypedDict\n\nclass ToolResult(TypedDict):\n    \"\"\"Result from a tool execution.\"\"\"\n    output: Optional[str]\n    error: Optional[str]\n    base64_image: Optional[str]\n    system: Optional[str]\n\nclass ToolError(Exception):\n    \"\"\"Exception raised for tool errors.\"\"\"\n    def __init__(self, message: str = \"\", output: str = \"\"):\n        self.message = message\n        self.output = output\n        super().__init__(message or output)\n\n    def to_result(self) -> ToolResult:\n        \"\"\"Convert error to a ToolResult.\"\"\"\n        return ToolResult(\n            output=self.output,\n            error=self.message or self.output,\n            base64_image=None,\n            system=None\n        )\n\nclass BaseAnthropicTool:\n    \"\"\"Base class for Anthropic tools.\"\"\"\n    name: str\n    api_type: str\n\n    def __init__(self):\n        self.__name__ = self.name  # Required for LangChain compatibility\n\n    def to_params(self):\n        \"\"\"Convert tool parameters to Anthropic API format.\"\"\"\n        return {\"name\": self.name, \"type\": self.api_type}\n\n    def handle_error(self, e: Exception) -> ToolResult:\n        \"\"\"Convert any exception to a ToolResult.\"\"\"\n        if isinstance(e, ToolError):\n            return e.to_result()\n        return ToolResult(\n            output=None,\n            error=str(e),\n            base64_image=None,\n            system=None\n        )\n\n```\n\n\n",
        "eval_script": "## computer-use-langchain-agent/Agent/Tools/base.py\nfrom typing import Optional, TypedDict\n\nclass ToolResult(TypedDict):\n    \"\"\"Result from a tool execution.\"\"\"\n    output: Optional[str]\n    error: Optional[str]\n    base64_image: Optional[str]\n    system: Optional[str]\n\nclass ToolError(Exception):\n    \"\"\"Exception raised for tool errors.\"\"\"\n    def __init__(self, message: str = \"\", output: str = \"\"):\n        self.message = message\n        self.output = output\n        super().__init__(message or output)\n\n    def to_result(self) -> ToolResult:\n        \"\"\"Convert error to a ToolResult.\"\"\"\n        return ToolResult(\n            output=self.output,\n            error=self.message or self.output,\n            base64_image=None,\n            system=None\n        )\n\nclass BaseAnthropicTool:\n    \"\"\"Base class for Anthropic tools.\"\"\"\n    name: str\n    api_type: str\n\n    def __init__(self, name: str = \"default_name\", api_type: str = \"default_type\"):\n        self.name = name\n        self.api_type = api_type\n        self.__name__ = self.name  # Required for LangChain compatibility\n\n    def to_params(self):\n        \"\"\"Convert tool parameters to Anthropic API format.\"\"\"\n        return {\"name\": self.name, \"type\": self.api_type}\n\n    def handle_error(self, e: Exception) -> ToolResult:\n        \"\"\"Convert any exception to a ToolResult.\"\"\"\n        if isinstance(e, ToolError):\n            return e.to_result()\n        return ToolResult(\n            output=None,\n            error=str(e),\n            base64_image=None,\n            system=None\n        )\n\n\ndef test_handle_error():\n    tool = BaseAnthropicTool()\n    \n    # Test with ToolError\n    e1 = ToolError(message=\"Test error\", output=\"Some output\")\n    result_old = tool.handle_error(e1)\n    result_new = tool.handle_error_new_implementation(e1)\n    assert result_old == result_new, \"Mismatch with ToolError\"\n\n    # Test with ValueError\n    e2 = ValueError(\"Value error occurred\")\n    result_old = tool.handle_error(e2)\n    result_new = tool.handle_error_new_implementation(e2)\n    assert result_old == result_new, \"Mismatch with ValueError\"\n\n    # Test with KeyError\n    e3 = KeyError(\"Key not found\")\n    result_old = tool.handle_error(e3)\n    result_new = tool.handle_error_new_implementation(e3)\n    assert result_old == result_new, \"Mismatch with KeyError\"\n\nif __name__ == \"__main__\":\n    test_handle_error()"
    },
    {
        "func_name": "BashTool.convert_to_windows_command",
        "idx": "254",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-langchain-agent/Agent/Tools/bash.py",
        "orig_func": "def convert_to_windows_command(self, command: str) -> str:\n    \"\"\"Convert Unix-style commands to Windows equivalents.\"\"\"\n    command_map = {'ls': 'dir', 'rm': 'del', 'cp': 'copy', 'mv': 'move', 'cat': 'type', 'clear': 'cls', 'touch': 'echo.>', 'mkdir': 'md', 'rmdir': 'rd', 'pwd': 'cd', 'grep': 'findstr'}\n    parts = command.split()\n    if not parts:\n        return command\n    base_cmd = parts[0]\n    if base_cmd in command_map:\n        parts[0] = command_map[base_cmd]\n        command = ' '.join(parts)\n    command = command.replace('/', '\\\\')\n    return command",
        "orig_context": "```python\n## computer-use-langchain-agent/Agent/Tools/base.py\nfrom typing import Optional, TypedDict\n\nclass ToolResult(TypedDict):\n    \"\"\"Result from a tool execution.\"\"\"\n    output: Optional[str]\n    error: Optional[str]\n    base64_image: Optional[str]\n    system: Optional[str]\n\n```\n\n\n```python\n## computer-use-langchain-agent/Agent/Tools/bash.py\nimport asyncio\n\nimport subprocess\n\nfrom typing import Literal, Optional\n\nfrom langchain.tools import BaseTool\n\nfrom pydantic import BaseModel, Field\n\nfrom .base import ToolResult\n\nclass BashInput(BaseModel):\n    command: str = Field(description=\"The command to execute\")\n    restart: Optional[bool] = Field(None, description=\"Whether to restart the tool\")\n\nclass BashTool(BaseTool):\n    \"\"\"Windows-compatible tool for executing system commands.\"\"\"\n    name: str = \"bash\"\n    description: str = \"Run commands in a bash shell\"\n    args_schema: type[BaseModel] = BashInput\n\n    def _run(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command synchronously\n            process = subprocess.run(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True,\n                text=True,\n                encoding='utf-8',\n                errors='ignore'\n            )\n            \n            if process.returncode != 0 and process.stderr:\n                return ToolResult(\n                    output=process.stdout,\n                    error=process.stderr,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=process.stdout,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    async def _arun(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command\n            process = await asyncio.create_subprocess_shell(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True\n            )\n            \n            stdout, stderr = await process.communicate()\n            \n            # Decode output using UTF-8, ignoring errors\n            stdout_str = stdout.decode('utf-8', errors='ignore') if stdout else None\n            stderr_str = stderr.decode('utf-8', errors='ignore') if stderr else None\n            \n            if process.returncode != 0 and stderr_str:\n                return ToolResult(\n                    output=stdout_str,\n                    error=stderr_str,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=stdout_str,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    def convert_to_windows_command(self, command: str) -> str:\n        \"\"\"Convert Unix-style commands to Windows equivalents.\"\"\"\n        # Common Unix to Windows command mappings\n        command_map = {\n            'ls': 'dir',\n            'rm': 'del',\n            'cp': 'copy',\n            'mv': 'move',\n            'cat': 'type',\n            'clear': 'cls',\n            'touch': 'echo.>',\n            'mkdir': 'md',\n            'rmdir': 'rd',\n            'pwd': 'cd',\n            'grep': 'findstr',\n        }\n\n        # Split the command into parts\n        parts = command.split()\n        if not parts:\n            return command\n\n        # Replace the command if it exists in the mapping\n        base_cmd = parts[0]\n        if base_cmd in command_map:\n            parts[0] = command_map[base_cmd]\n            command = ' '.join(parts)\n\n        # Handle path separators\n        command = command.replace('/', '\\\\')\n        \n        return command\n\n    def validate_command(self, command: str) -> bool:\n        \"\"\"Validate that the command is safe to execute.\"\"\"\n        # List of forbidden commands\n        forbidden = [\n            'format',\n            'del /f',\n            'rmdir /s',\n            'rd /s',\n            'del /q',\n            'format',\n            'shutdown',\n            'taskkill',\n        ]\n        \n        # Check if any forbidden command is present\n        command_lower = command.lower()\n        return not any(cmd in command_lower for cmd in forbidden)\n\n```\n\n\n",
        "eval_script": "import asyncio\nimport subprocess\nfrom typing import Literal, Optional, TypedDict\nfrom langchain.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass ToolResult(TypedDict):\n    \"\"\"Result from a tool execution.\"\"\"\n    output: Optional[str]\n    error: Optional[str]\n    base64_image: Optional[str]\n    system: Optional[str]\n\nclass BashInput(BaseModel):\n    command: str = Field(description=\"The command to execute\")\n    restart: Optional[bool] = Field(None, description=\"Whether to restart the tool\")\n\nclass BashTool(BaseTool):\n    \"\"\"Windows-compatible tool for executing system commands.\"\"\"\n    name: str = \"bash\"\n    description: str = \"Run commands in a bash shell\"\n    args_schema: type[BaseModel] = BashInput\n\n    def _run(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command synchronously\n            process = subprocess.run(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True,\n                text=True,\n                encoding='utf-8',\n                errors='ignore'\n            )\n            \n            if process.returncode != 0 and process.stderr:\n                return ToolResult(\n                    output=process.stdout,\n                    error=process.stderr,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=process.stdout,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    async def _arun(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command\n            process = await asyncio.create_subprocess_shell(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True\n            )\n            \n            stdout, stderr = await process.communicate()\n            \n            # Decode output using UTF-8, ignoring errors\n            stdout_str = stdout.decode('utf-8', errors='ignore') if stdout else None\n            stderr_str = stderr.decode('utf-8', errors='ignore') if stderr else None\n            \n            if process.returncode != 0 and stderr_str:\n                return ToolResult(\n                    output=stdout_str,\n                    error=stderr_str,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=stdout_str,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    def convert_to_windows_command(self, command: str) -> str:\n        \"\"\"Convert Unix-style commands to Windows equivalents.\"\"\"\n        # Common Unix to Windows command mappings\n        command_map = {\n            'ls': 'dir',\n            'rm': 'del',\n            'cp': 'copy',\n            'mv': 'move',\n            'cat': 'type',\n            'clear': 'cls',\n            'touch': 'echo.>',\n            'mkdir': 'md',\n            'rmdir': 'rd',\n            'pwd': 'cd',\n            'grep': 'findstr',\n        }\n\n        # Split the command into parts\n        parts = command.split()\n        if not parts:\n            return command\n\n        # Replace the command if it exists in the mapping\n        base_cmd = parts[0]\n        if base_cmd in command_map:\n            parts[0] = command_map[base_cmd]\n            command = ' '.join(parts)\n\n        # Handle path separators\n        command = command.replace('/', '\\\\')\n        \n        return command\n\n    def validate_command(self, command: str) -> bool:\n        \"\"\"Validate that the command is safe to execute.\"\"\"\n        # List of forbidden commands\n        forbidden = [\n            'format',\n            'del /f',\n            'rmdir /s',\n            'rd /s',\n            'del /q',\n            'format',\n            'shutdown',\n            'taskkill',\n        ]\n        \n        # Check if any forbidden command is present\n        command_lower = command.lower()\n        return not any(cmd in command_lower for cmd in forbidden)\n\n\n\ndef test_convert_to_windows_command():\n    tool = BashTool()\n    \n    # Test cases\n    assert tool.convert_to_windows_command('ls /home') == tool.convert_to_windows_command_new_implementation('ls /home')\n    assert tool.convert_to_windows_command('rm file.txt') == tool.convert_to_windows_command_new_implementation('rm file.txt')\n    assert tool.convert_to_windows_command('mv /path/to/source /path/to/dest') == tool.convert_to_windows_command_new_implementation('mv /path/to/source /path/to/dest')\n\nif __name__ == \"__main__\":\n    test_convert_to_windows_command()"
    },
    {
        "func_name": "BashTool.validate_command",
        "idx": "255",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-langchain-agent/Agent/Tools/bash.py",
        "orig_func": "def validate_command(self, command: str) -> bool:\n    \"\"\"Validate that the command is safe to execute.\"\"\"\n    forbidden = ['format', 'del /f', 'rmdir /s', 'rd /s', 'del /q', 'format', 'shutdown', 'taskkill']\n    command_lower = command.lower()\n    return not any((cmd in command_lower for cmd in forbidden))",
        "orig_context": "```python\n## computer-use-langchain-agent/Agent/Tools/base.py\nfrom typing import Optional, TypedDict\n\nclass ToolResult(TypedDict):\n    \"\"\"Result from a tool execution.\"\"\"\n    output: Optional[str]\n    error: Optional[str]\n    base64_image: Optional[str]\n    system: Optional[str]\n\n```\n\n\n```python\n## computer-use-langchain-agent/Agent/Tools/bash.py\nimport asyncio\n\nimport subprocess\n\nfrom typing import Literal, Optional\n\nfrom langchain.tools import BaseTool\n\nfrom pydantic import BaseModel, Field\n\nfrom .base import ToolResult\n\nclass BashInput(BaseModel):\n    command: str = Field(description=\"The command to execute\")\n    restart: Optional[bool] = Field(None, description=\"Whether to restart the tool\")\n\nclass BashTool(BaseTool):\n    \"\"\"Windows-compatible tool for executing system commands.\"\"\"\n    name: str = \"bash\"\n    description: str = \"Run commands in a bash shell\"\n    args_schema: type[BaseModel] = BashInput\n\n    def _run(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command synchronously\n            process = subprocess.run(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True,\n                text=True,\n                encoding='utf-8',\n                errors='ignore'\n            )\n            \n            if process.returncode != 0 and process.stderr:\n                return ToolResult(\n                    output=process.stdout,\n                    error=process.stderr,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=process.stdout,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    async def _arun(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command\n            process = await asyncio.create_subprocess_shell(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True\n            )\n            \n            stdout, stderr = await process.communicate()\n            \n            # Decode output using UTF-8, ignoring errors\n            stdout_str = stdout.decode('utf-8', errors='ignore') if stdout else None\n            stderr_str = stderr.decode('utf-8', errors='ignore') if stderr else None\n            \n            if process.returncode != 0 and stderr_str:\n                return ToolResult(\n                    output=stdout_str,\n                    error=stderr_str,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=stdout_str,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    def convert_to_windows_command(self, command: str) -> str:\n        \"\"\"Convert Unix-style commands to Windows equivalents.\"\"\"\n        # Common Unix to Windows command mappings\n        command_map = {\n            'ls': 'dir',\n            'rm': 'del',\n            'cp': 'copy',\n            'mv': 'move',\n            'cat': 'type',\n            'clear': 'cls',\n            'touch': 'echo.>',\n            'mkdir': 'md',\n            'rmdir': 'rd',\n            'pwd': 'cd',\n            'grep': 'findstr',\n        }\n\n        # Split the command into parts\n        parts = command.split()\n        if not parts:\n            return command\n\n        # Replace the command if it exists in the mapping\n        base_cmd = parts[0]\n        if base_cmd in command_map:\n            parts[0] = command_map[base_cmd]\n            command = ' '.join(parts)\n\n        # Handle path separators\n        command = command.replace('/', '\\\\')\n        \n        return command\n\n    def validate_command(self, command: str) -> bool:\n        \"\"\"Validate that the command is safe to execute.\"\"\"\n        # List of forbidden commands\n        forbidden = [\n            'format',\n            'del /f',\n            'rmdir /s',\n            'rd /s',\n            'del /q',\n            'format',\n            'shutdown',\n            'taskkill',\n        ]\n        \n        # Check if any forbidden command is present\n        command_lower = command.lower()\n        return not any(cmd in command_lower for cmd in forbidden)\n\n```\n\n\n",
        "eval_script": "import asyncio\nimport subprocess\nfrom typing import Optional, Tuple, Dict, Any, Literal, TypedDict\nfrom langchain.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass ToolResult(TypedDict):\n    \"\"\"Result from a tool execution.\"\"\"\n    output: Optional[str]\n    error: Optional[str]\n    base64_image: Optional[str]\n    system: Optional[str]\n\nclass BashInput(BaseModel):\n    command: str = Field(description=\"The command to execute\")\n    restart: Optional[bool] = Field(None, description=\"Whether to restart the tool\")\n\nclass BashTool(BaseTool):\n    \"\"\"Windows-compatible tool for executing system commands.\"\"\"\n    name: str = \"bash\"\n    description: str = \"Run commands in a bash shell\"\n    args_schema: type[BaseModel] = BashInput\n\n    def _run(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command synchronously\n            process = subprocess.run(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True,\n                text=True,\n                encoding='utf-8',\n                errors='ignore'\n            )\n            \n            if process.returncode != 0 and process.stderr:\n                return ToolResult(\n                    output=process.stdout,\n                    error=process.stderr,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=process.stdout,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    async def _arun(\n        self,\n        command: str,\n        restart: Optional[bool] = None,\n    ) -> ToolResult:\n        try:\n            # Validate command\n            if not self.validate_command(command):\n                return ToolResult(\n                    output=None,\n                    error=\"Command contains forbidden operations\",\n                    base64_image=None,\n                    system=None\n                )\n\n            # Convert Unix-style commands to Windows equivalents\n            windows_command = self.convert_to_windows_command(command)\n            \n            # Run the command\n            process = await asyncio.create_subprocess_shell(\n                windows_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                shell=True\n            )\n            \n            stdout, stderr = await process.communicate()\n            \n            # Decode output using UTF-8, ignoring errors\n            stdout_str = stdout.decode('utf-8', errors='ignore') if stdout else None\n            stderr_str = stderr.decode('utf-8', errors='ignore') if stderr else None\n            \n            if process.returncode != 0 and stderr_str:\n                return ToolResult(\n                    output=stdout_str,\n                    error=stderr_str,\n                    base64_image=None,\n                    system=None\n                )\n            \n            return ToolResult(\n                output=stdout_str,\n                error=None,\n                base64_image=None,\n                system=None\n            )\n\n        except Exception as e:\n            return ToolResult(\n                output=None,\n                error=f\"Command execution failed: {str(e)}\",\n                base64_image=None,\n                system=None\n            )\n\n    def convert_to_windows_command(self, command: str) -> str:\n        \"\"\"Convert Unix-style commands to Windows equivalents.\"\"\"\n        # Common Unix to Windows command mappings\n        command_map = {\n            'ls': 'dir',\n            'rm': 'del',\n            'cp': 'copy',\n            'mv': 'move',\n            'cat': 'type',\n            'clear': 'cls',\n            'touch': 'echo.>',\n            'mkdir': 'md',\n            'rmdir': 'rd',\n            'pwd': 'cd',\n            'grep': 'findstr',\n        }\n\n        # Split the command into parts\n        parts = command.split()\n        if not parts:\n            return command\n\n        # Replace the command if it exists in the mapping\n        base_cmd = parts[0]\n        if base_cmd in command_map:\n            parts[0] = command_map[base_cmd]\n            command = ' '.join(parts)\n\n        # Handle path separators\n        command = command.replace('/', '\\\\')\n        \n        return command\n\n    def validate_command(self, command: str) -> bool:\n        \"\"\"Validate that the command is safe to execute.\"\"\"\n        # List of forbidden commands\n        forbidden = [\n            'format',\n            'del /f',\n            'rmdir /s',\n            'rd /s',\n            'del /q',\n            'format',\n            'shutdown',\n            'taskkill',\n        ]\n        \n        # Check if any forbidden command is present\n        command_lower = command.lower()\n        return not any(cmd in command_lower for cmd in forbidden)\n\n\n\ndef test_validate_command():\n    bash_tool = BashTool()\n\n    # Test cases\n    command1 = \"ls\"\n    command2 = \"shutdown now\"\n    command3 = \"mkdir new_folder\"\n    \n    # Assert that both implementations return the same results\n    assert bash_tool.validate_command(command1) == bash_tool.validate_command_new_implementation(command1), \"Test case 1 failed\"\n    assert bash_tool.validate_command(command2) == bash_tool.validate_command_new_implementation(command2), \"Test case 2 failed\"\n    assert bash_tool.validate_command(command3) == bash_tool.validate_command_new_implementation(command3), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_validate_command()"
    },
    {
        "func_name": "EditTool._format_file_content",
        "idx": "256",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-windows-streamlit/computer_use/tools/edit.py",
        "orig_func": "def _format_file_content(self, path: Path, file_content: str, init_line: int=1) -> CLIResult:\n    \"\"\"Format file content with line numbers.\"\"\"\n    if not file_content:\n        return CLIResult(output=f'File {path} is empty')\n    file_descriptor = f'lines {init_line}-{init_line + file_content.count(chr(10))} of {path}'\n    if init_line == 1 and file_content.count(chr(10)) == path.read_text().count(chr(10)):\n        file_descriptor = str(path)\n    if file_content:\n        file_content = file_content.expandtabs()\n        file_content = '\\n'.join([f'{i + init_line:6}\\t{line}' for (i, line) in enumerate(file_content.split('\\n'))])\n    return CLIResult(output=f\"Here's the content of {file_descriptor}:\\n\" + file_content + '\\n')",
        "orig_context": "```python\n## computer-use-windows-streamlit/computer_use/tools/base.py\nfrom abc import ABCMeta, abstractmethod\n\nfrom dataclasses import dataclass, fields, replace\n\nfrom typing import Any\n\nfrom anthropic.types.beta import BetaToolUnionParam\n\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"Executes the tool with the given arguments.\"\"\"\n        ...\n\n    @abstractmethod\n    def to_params(\n        self,\n    ) -> BetaToolUnionParam:\n        raise NotImplementedError\n\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\nclass CLIResult(ToolResult):\n    \"\"\"A ToolResult that can be rendered as a CLI output.\"\"\"\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n```\n\n\n```python\n## computer-use-windows-streamlit/computer_use/tools/run.py\nimport asyncio\n\nTRUNCATED_MESSAGE: str = \"<response clipped><NOTE>To save on context, only part of this file has been shown. You should use appropriate search/filter commands for your platform to find specific content.</NOTE>\"\n\nMAX_RESPONSE_LEN: int = 16000\n\nasync def run(cmd: str, timeout: float = 30.0):\n    \"\"\"Run a shell command asynchronously with a timeout.\"\"\"\n    process = await asyncio.create_subprocess_shell(\n        cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n    )\n\n    try:\n        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)\n    except asyncio.TimeoutError:\n        process.kill()\n        return (\n            process.returncode,\n            \"\",\n            f\"Command timed out after {timeout} seconds\",\n        )\n\n    stdout_str = stdout.decode() if stdout else \"\"\n    stderr_str = stderr.decode() if stderr else \"\"\n\n    if len(stdout_str) > MAX_RESPONSE_LEN:\n        stdout_str = stdout_str[:MAX_RESPONSE_LEN] + \"\\n\" + TRUNCATED_MESSAGE\n\n    return process.returncode, stdout_str, stderr_str\n\n```\n\n\n```python\n## computer-use-windows-streamlit/computer_use/tools/edit.py\nimport os\n\nfrom pathlib import Path\n\nfrom typing import ClassVar, Literal, Optional, TypedDict\n\nfrom .base import BaseAnthropicTool, CLIResult, ToolError\n\nfrom .run import run\n\nSNIPPET_LINES = 3\n\nclass ToolEditParam(TypedDict):\n    type: str\n    name: str\n\nclass EditTool(BaseAnthropicTool):\n    \"\"\"\n    A tool that allows the agent to edit files.\n    The tool parameters are defined by Anthropic and are not editable.\n    \"\"\"\n\n    name: ClassVar[Literal[\"str_replace_editor\"]] = \"str_replace_editor\"\n    api_type: ClassVar[Literal[\"text_editor_20241022\"]] = \"text_editor_20241022\"  # Updated to match expected type\n    _file_history: dict[Path, list[str]]\n\n    def __init__(self):\n        self._file_history = {}\n        super().__init__()\n\n    def validate_path(self, command: str, path: Path):\n        \"\"\"\n        Check that the path/command combination is valid.\n        \"\"\"\n        if not path.is_absolute():\n            suggested_path = Path(os.getcwd()) / path\n            raise ToolError(\n                f\"The path {path} is not an absolute path. Maybe you meant {suggested_path}?\"\n            )\n\n        if command != \"create\" and not path.exists():\n            raise ToolError(f\"The path {path} does not exist\")\n\n        if command == \"create\" and path.exists():\n            raise ToolError(f\"File already exists at {path}\")\n\n        if command != \"view\" and path.is_dir():\n            raise ToolError(f\"The path {path} is a directory\")\n\n    async def __call__(\n        self,\n        *,\n        command: str,\n        path: str,\n        old_str: str | None = None,\n        new_str: str | None = None,\n        file_text: str | None = None,\n        insert_line: int | None = None,\n        view_range: list[int] | None = None,\n        **kwargs,\n    ):\n        path = Path(path)\n        self.validate_path(command, path)\n\n        if command == \"view\":\n            if path.is_dir():\n                # For directories, list contents using platform-appropriate method\n                if os.name == 'nt':  # Windows\n                    _, stdout, stderr = await run(\n                        f'dir /B \"{path}\"'\n                    )\n                else:  # Unix\n                    _, stdout, stderr = await run(\n                        rf\"find {path} -maxdepth 2 -not -path '*/\\.*'\"\n                    )\n                if not stderr:\n                    stdout = f\"Here's the files and directories in {path}:\\n{stdout}\\n\"\n                return CLIResult(output=stdout, error=stderr)\n\n            file_content = path.read_text()\n\n            if view_range:\n                if len(view_range) != 2:\n                    raise ToolError(\n                        \"The `view_range` parameter must be a list of two integers\"\n                    )\n                init_line, final_line = view_range\n                if init_line > final_line:\n                    raise ToolError(\n                        \"Invalid `view_range`: first number must be less than or equal to second number\"\n                    )\n                file_lines = file_content.split(\"\\n\")\n                n_lines_file = len(file_lines)\n                if final_line == -1:\n                    file_content = \"\\n\".join(file_lines[init_line - 1 :])\n                else:\n                    file_content = \"\\n\".join(file_lines[init_line - 1 : final_line])\n\n            return self._format_file_content(path, file_content, init_line=view_range[0] if view_range else 1)\n\n        if command == \"create\":\n            if file_text is None:\n                raise ToolError(\"Parameter `file_text` is required for create command\")\n\n            path.write_text(file_text)\n            return CLIResult(output=f\"File {path} has been created.\")\n\n        if command == \"str_replace\":\n            if old_str is None or new_str is None:\n                raise ToolError(\n                    \"Parameters `old_str` and `new_str` are required for str_replace command\"\n                )\n\n            file_content = path.read_text()\n            occurrences = file_content.count(old_str)\n\n            if occurrences == 0:\n                raise ToolError(f\"String '{old_str}' not found in {path}\")\n\n            if occurrences > 1:\n                file_content_lines = file_content.split(\"\\n\")\n                lines = [\n                    f\"{i+1}: {line}\"\n                    for i, line in enumerate(file_content_lines)\n                    if old_str in line\n                ]\n                raise ToolError(\n                    f\"Found {occurrences} occurrences of '{old_str}' in {path}. Please be more specific.\\nLines containing the string:\\n\"\n                    + \"\\n\".join(lines)\n                )\n\n            new_file_content = file_content.replace(old_str, new_str, 1)\n            self._file_history.setdefault(path, []).append(file_content)\n            path.write_text(new_file_content)\n\n            # Create a snippet of the edited section\n            replacement_line = file_content.split(old_str)[0].count(\"\\n\")\n            start_line = max(0, replacement_line - SNIPPET_LINES)\n            end_line = replacement_line + SNIPPET_LINES + new_str.count(\"\\n\")\n            snippet = \"\\n\".join(new_file_content.split(\"\\n\")[start_line : end_line + 1])\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"insert\":\n            if new_str is None:\n                raise ToolError(\"Parameter `new_str` is required for insert command\")\n            if insert_line is None:\n                raise ToolError(\"Parameter `insert_line` is required for insert command\")\n\n            new_str = new_str.expandtabs()\n            file_text = path.read_text()\n            file_text_lines = file_text.split(\"\\n\")\n            n_lines_file = len(file_text_lines)\n\n            if insert_line < 0 or (\n                insert_line > n_lines_file and insert_line != n_lines_file\n            ):\n                raise ToolError(\n                    f\"Invalid `insert_line` parameter: {insert_line}. File has {n_lines_file} lines.\"\n                )\n\n            new_str_lines = new_str.split(\"\\n\")\n            new_file_text_lines = (\n                file_text_lines[:insert_line]\n                + new_str_lines\n                + file_text_lines[insert_line:]\n            )\n            new_file_text = \"\\n\".join(new_file_text_lines)\n\n            self._file_history.setdefault(path, []).append(file_text)\n            path.write_text(new_file_text)\n\n            # Create a snippet of the edited section\n            start_line = max(0, insert_line - SNIPPET_LINES)\n            end_line = min(\n                len(new_file_text_lines),\n                insert_line + len(new_str_lines) + SNIPPET_LINES,\n            )\n            snippet_lines = new_file_text_lines[start_line:end_line]\n            snippet = \"\\n\".join(snippet_lines)\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"undo_edit\":\n            if path not in self._file_history or not self._file_history[path]:\n                raise ToolError(f\"No edit history found for {path}\")\n\n            last_content = self._file_history[path].pop()\n            path.write_text(last_content)\n\n            return CLIResult(output=f\"Last edit to {path} undone successfully\")\n\n        raise ToolError(f\"Invalid command: {command}\")\n\n    def _format_file_content(\n        self, path: Path, file_content: str, init_line: int = 1\n    ) -> CLIResult:\n        \"\"\"Format file content with line numbers.\"\"\"\n        if not file_content:\n            return CLIResult(output=f\"File {path} is empty\")\n\n        file_descriptor = f\"lines {init_line}-{init_line + file_content.count(chr(10))} of {path}\"\n        if init_line == 1 and file_content.count(chr(10)) == path.read_text().count(\n            chr(10)\n        ):\n            file_descriptor = str(path)\n\n        if file_content:\n            file_content = file_content.expandtabs()\n            file_content = \"\\n\".join(\n                [\n                    f\"{i + init_line:6}\\t{line}\"\n                    for i, line in enumerate(file_content.split(\"\\n\"))\n                ]\n            )\n\n        return CLIResult(\n            output=(\n                f\"Here's the content of {file_descriptor}:\\n\"\n                + file_content\n                + \"\\n\"\n            )\n        )\n\n    def to_params(self) -> ToolEditParam:\n        return {\n            \"type\": self.api_type,\n            \"name\": self.name,\n        }\n\n```\n\n\n",
        "eval_script": "import os\nimport asyncio\nfrom pathlib import Path\nfrom typing import ClassVar, Literal, TypedDict, Any\nfrom abc import ABCMeta, abstractmethod\nfrom dataclasses import dataclass, fields, replace\n\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        ...\n\n    @abstractmethod\n    def to_params(self) -> dict:\n        raise NotImplementedError\n\n@dataclass\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(field: str | None, other_field: str | None, concatenate: bool = True):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\nclass CLIResult(ToolResult):\n    \"\"\"A ToolResult that can be rendered as a CLI output.\"\"\"\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\nasync def run(cmd: str, timeout: float = 30.0):\n    \"\"\"Run a shell command asynchronously with a timeout.\"\"\"\n    process = await asyncio.create_subprocess_shell(\n        cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n    )\n\n    try:\n        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout)\n    except asyncio.TimeoutError:\n        process.kill()\n        return (\n            process.returncode,\n            \"\",\n            f\"Command timed out after {timeout} seconds\",\n        )\n\n    stdout_str = stdout.decode() if stdout else \"\"\n    stderr_str = stderr.decode() if stderr else \"\"\n\n    if len(stdout_str) > MAX_RESPONSE_LEN:\n        stdout_str = stdout_str[:MAX_RESPONSE_LEN] + \"\\n\" + TRUNCATED_MESSAGE\n\n    return process.returncode, stdout_str, stderr_str\n\nclass ToolEditParam(TypedDict):\n    type: str\n    name: str\n\nclass EditTool(BaseAnthropicTool):\n    name: ClassVar[Literal[\"str_replace_editor\"]] = \"str_replace_editor\"\n    api_type: ClassVar[Literal[\"text_editor_20241022\"]] = \"text_editor_20241022\"\n    _file_history: dict[Path, list[str]]\n\n    def __init__(self):\n        self._file_history = {}\n        super().__init__()\n\n    def validate_path(self, command: str, path: Path):\n        if not path.is_absolute():\n            suggested_path = Path(os.getcwd()) / path\n            raise ToolError(\n                f\"The path {path} is not an absolute path. Maybe you meant {suggested_path}?\"\n            )\n\n        if command != \"create\" and not path.exists():\n            raise ToolError(f\"The path {path} does not exist\")\n\n        if command == \"create\" and path.exists():\n            raise ToolError(f\"File already exists at {path}\")\n\n        if command != \"view\" and path.is_dir():\n            raise ToolError(f\"The path {path} is a directory\")\n\n    async def __call__(\n        self,\n        *,\n        command: str,\n        path: str,\n        old_str: str | None = None,\n        new_str: str | None = None,\n        file_text: str | None = None,\n        insert_line: int | None = None,\n        view_range: list[int] | None = None,\n        **kwargs,\n    ):\n        path = Path(path)\n        self.validate_path(command, path)\n\n        if command == \"view\":\n            if path.is_dir():\n                if os.name == 'nt':\n                    _, stdout, stderr = await run(\n                        f'dir /B \"{path}\"'\n                    )\n                else:\n                    _, stdout, stderr = await run(\n                        rf\"find {path} -maxdepth 2 -not -path '*/\\.*'\"\n                    )\n                if not stderr:\n                    stdout = f\"Here's the files and directories in {path}:\\n{stdout}\\n\"\n                return CLIResult(output=stdout, error=stderr)\n\n            file_content = path.read_text()\n\n            if view_range:\n                if len(view_range) != 2:\n                    raise ToolError(\n                        \"The `view_range` parameter must be a list of two integers\"\n                    )\n                init_line, final_line = view_range\n                if init_line > final_line:\n                    raise ToolError(\n                        \"Invalid `view_range`: first number must be less than or equal to second number\"\n                    )\n                file_lines = file_content.split(\"\\n\")\n                n_lines_file = len(file_lines)\n                if final_line == -1:\n                    file_content = \"\\n\".join(file_lines[init_line - 1 :])\n                else:\n                    file_content = \"\\n\".join(file_lines[init_line - 1 : final_line])\n\n            return self._format_file_content(path, file_content, init_line=view_range[0] if view_range else 1)\n\n        if command == \"create\":\n            if file_text is None:\n                raise ToolError(\"Parameter `file_text` is required for create command\")\n\n            path.write_text(file_text)\n            return CLIResult(output=f\"File {path} has been created.\")\n\n        if command == \"str_replace\":\n            if old_str is None or new_str is None:\n                raise ToolError(\n                    \"Parameters `old_str` and `new_str` are required for str_replace command\"\n                )\n\n            file_content = path.read_text()\n            occurrences = file_content.count(old_str)\n\n            if occurrences == 0:\n                raise ToolError(f\"String '{old_str}' not found in {path}\")\n\n            if occurrences > 1:\n                file_content_lines = file_content.split(\"\\n\")\n                lines = [\n                    f\"{i+1}: {line}\"\n                    for i, line in enumerate(file_content_lines)\n                    if old_str in line\n                ]\n                raise ToolError(\n                    f\"Found {occurrences} occurrences of '{old_str}' in {path}. Please be more specific.\\nLines containing the string:\\n\"\n                    + \"\\n\".join(lines)\n                )\n\n            new_file_content = file_content.replace(old_str, new_str, 1)\n            self._file_history.setdefault(path, []).append(file_content)\n            path.write_text(new_file_content)\n\n            replacement_line = file_content.split(old_str)[0].count(\"\\n\")\n            start_line = max(0, replacement_line - SNIPPET_LINES)\n            end_line = replacement_line + SNIPPET_LINES + new_str.count(\"\\n\")\n            snippet = \"\\n\".join(new_file_content.split(\"\\n\")[start_line : end_line + 1])\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"insert\":\n            if new_str is None:\n                raise ToolError(\"Parameter `new_str` is required for insert command\")\n            if insert_line is None:\n                raise ToolError(\"Parameter `insert_line` is required for insert command\")\n\n            new_str = new_str.expandtabs()\n            file_text = path.read_text()\n            file_text_lines = file_text.split(\"\\n\")\n            n_lines_file = len(file_text_lines)\n\n            if insert_line < 0 or (\n                insert_line > n_lines_file and insert_line != n_lines_file\n            ):\n                raise ToolError(\n                    f\"Invalid `insert_line` parameter: {insert_line}. File has {n_lines_file} lines.\"\n                )\n\n            new_str_lines = new_str.split(\"\\n\")\n            new_file_text_lines = (\n                file_text_lines[:insert_line]\n                + new_str_lines\n                + file_text_lines[insert_line:]\n            )\n            new_file_text = \"\\n\".join(new_file_text_lines)\n\n            self._file_history.setdefault(path, []).append(file_text)\n            path.write_text(new_file_text)\n\n            start_line = max(0, insert_line - SNIPPET_LINES)\n            end_line = min(\n                len(new_file_text_lines),\n                insert_line + len(new_str_lines) + SNIPPET_LINES,\n            )\n            snippet_lines = new_file_text_lines[start_line:end_line]\n            snippet = \"\\n\".join(snippet_lines)\n\n            return CLIResult(\n                output=f\"File {path} has been edited. Here's the affected section:\\n{snippet}\"\n            )\n\n        if command == \"undo_edit\":\n            if path not in self._file_history or not self._file_history[path]:\n                raise ToolError(f\"No edit history found for {path}\")\n\n            last_content = self._file_history[path].pop()\n            path.write_text(last_content)\n\n            return CLIResult(output=f\"Last edit to {path} undone successfully\")\n\n        raise ToolError(f\"Invalid command: {command}\")\n\n    def _format_file_content(self, path: Path, file_content: str, init_line: int = 1) -> CLIResult:\n        \"\"\"Format file content with line numbers.\"\"\"\n        if not file_content:\n            return CLIResult(output=f\"File {path} is empty\")\n\n        file_descriptor = f\"lines {init_line}-{init_line + file_content.count(chr(10))} of {path}\"\n        if init_line == 1 and file_content.count(chr(10)) == path.read_text().count(chr(10)):\n            file_descriptor = str(path)\n\n        if file_content:\n            file_content = file_content.expandtabs()\n            file_content = \"\\n\".join(\n                [f\"{i + init_line:6}\\t{line}\" for i, line in enumerate(file_content.split(\"\\n\"))]\n            )\n\n        return CLIResult(\n            output=(\n                f\"Here's the content of {file_descriptor}:\\n\" + file_content + \"\\n\"\n            )\n        )\n\n\n    def to_params(self) -> ToolEditParam:\n        return {\n            \"type\": self.api_type,\n            \"name\": self.name,\n        }\n\ndef test__format_file_content():\n    tool = EditTool()\n\n    # Ensure the test directory exists\n    os.makedirs(\"/home/user/tmp\", exist_ok=True)\n\n    # Test with empty content\n    path = Path(\"/home/user/tmp/test_file_1.txt\")\n    path.write_text(\"\")  # Create an empty file\n    assert tool._format_file_content(path, \"\", 1).output == tool._format_file_content_new_implementation(path, \"\", 1).output\n\n    # Test with simple content\n    simple_content = \"Hello, World!\\nThis is a test.\\nAnother line.\"\n    path.write_text(simple_content)  # Write simple content to the file\n    assert tool._format_file_content(path, simple_content, 3).output == tool._format_file_content_new_implementation(path, simple_content, 3).output\n\n    # Test with complex content\n    complex_content = \"Line 1\\nLine 2\\n\\tIndented line\\nAnother line\\nFinal line\"\n    path.write_text(complex_content)  # Write complex content to the file\n    assert tool._format_file_content(path, complex_content, 1).output == tool._format_file_content_new_implementation(path, complex_content, 1).output\n\nif __name__ == \"__main__\":\n    test__format_file_content()"
    },
    {
        "func_name": "ComputerTool.scale_coordinates",
        "idx": "257",
        "repo_name": "Cam10001110101___computer-use-windows",
        "func_path": "computer-use-windows-streamlit/computer_use/tools/computer.py",
        "orig_func": "def scale_coordinates(self, source: ScalingSource, x: int, y: int):\n    \"\"\"Scale coordinates to a target maximum resolution.\"\"\"\n    if not self._scaling_enabled:\n        return (x, y)\n    ratio = self.width / self.height\n    target_dimension = None\n    for dimension in MAX_SCALING_TARGETS.values():\n        if abs(dimension['width'] / dimension['height'] - ratio) < 0.02:\n            if dimension['width'] < self.width:\n                target_dimension = dimension\n            break\n    if target_dimension is None:\n        return (x, y)\n    x_scaling_factor = target_dimension['width'] / self.width\n    y_scaling_factor = target_dimension['height'] / self.height\n    if source == ScalingSource.API:\n        if x > self.width or y > self.height:\n            raise ToolError(f'Coordinates {x}, {y} are out of bounds')\n        return (round(x / x_scaling_factor), round(y / y_scaling_factor))\n    return (round(x * x_scaling_factor), round(y * y_scaling_factor))",
        "orig_context": "```python\n## computer-use-windows-streamlit/computer_use/tools/base.py\nfrom abc import ABCMeta, abstractmethod\n\nfrom dataclasses import dataclass, fields, replace\n\nfrom typing import Any\n\nfrom anthropic.types.beta import BetaToolUnionParam\n\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"Executes the tool with the given arguments.\"\"\"\n        ...\n\n    @abstractmethod\n    def to_params(\n        self,\n    ) -> BetaToolUnionParam:\n        raise NotImplementedError\n\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\n```\n\n\n```python\n## computer-use-windows-streamlit/computer_use/tools/computer.py\nimport asyncio\n\nimport base64\n\nimport os\n\nfrom enum import StrEnum\n\nfrom pathlib import Path\n\nfrom typing import Literal, TypedDict\n\nfrom uuid import uuid4\n\nimport pyautogui\n\nimport win32api\n\nimport win32con\n\nfrom PIL import Image\n\nfrom anthropic.types.beta import BetaToolParam\n\nfrom .base import BaseAnthropicTool, ToolError, ToolResult\n\npyautogui.FAILSAFE = True\n\npyautogui.PAUSE = 0.1\n\nOUTPUT_DIR = os.path.join(os.environ.get('TEMP', '.'), 'outputs')\n\nTYPING_DELAY_MS = 12\n\nTYPING_GROUP_SIZE = 50\n\nAction = Literal[\n    \"key\",\n    \"type\",\n    \"mouse_move\",\n    \"left_click\",\n    \"left_click_drag\",\n    \"right_click\",\n    \"middle_click\",\n    \"double_click\",\n    \"screenshot\",\n    \"cursor_position\",\n]\n\nclass Resolution(TypedDict):\n    width: int\n    height: int\n\nMAX_SCALING_TARGETS: dict[str, Resolution] = {\n    \"XGA\": Resolution(width=1024, height=768),  # 4:3\n    \"WXGA\": Resolution(width=1280, height=800),  # 16:10\n    \"FWXGA\": Resolution(width=1366, height=768),  # ~16:9\n}\n\nclass ScalingSource(StrEnum):\n    COMPUTER = \"computer\"\n    API = \"api\"\n\nclass ComputerToolOptions(TypedDict):\n    display_height_px: int\n    display_width_px: int\n    display_number: int | None\n\ndef chunks(s: str, chunk_size: int) -> list[str]:\n    return [s[i : i + chunk_size] for i in range(0, len(s), chunk_size)]\n\nclass ComputerTool(BaseAnthropicTool):\n    \"\"\"\n    A tool that allows the agent to interact with the screen, keyboard, and mouse of the current computer.\n    Windows-compatible implementation using pyautogui and win32api.\n    \"\"\"\n\n    name: Literal[\"computer\"] = \"computer\"\n    api_type: Literal[\"computer_20241022\"] = \"computer_20241022\"  # Updated to match expected type\n    width: int\n    height: int\n    display_num: int | None\n\n    _screenshot_delay = 2.0\n    _scaling_enabled = True\n\n    @property\n    def options(self) -> ComputerToolOptions:\n        width, height = self.scale_coordinates(\n            ScalingSource.COMPUTER, self.width, self.height\n        )\n        return {\n            \"display_width_px\": width,\n            \"display_height_px\": height,\n            \"display_number\": self.display_num,\n        }\n\n    def to_params(self) -> BetaToolParam:\n        return {\"name\": self.name, \"type\": self.api_type, **self.options}\n\n    def __init__(self):\n        super().__init__()\n        # Get primary monitor resolution\n        self.width = win32api.GetSystemMetrics(win32con.SM_CXSCREEN)\n        self.height = win32api.GetSystemMetrics(win32con.SM_CYSCREEN)\n        self.display_num = None  # Windows handles multiple displays differently\n\n    async def __call__(\n        self,\n        *,\n        action: Action,\n        text: str | None = None,\n        coordinate: tuple[int, int] | None = None,\n        **kwargs,\n    ):\n        try:\n            if action in (\"mouse_move\", \"left_click_drag\"):\n                if coordinate is None:\n                    raise ToolError(f\"coordinate is required for {action}\")\n                if text is not None:\n                    raise ToolError(f\"text is not accepted for {action}\")\n                if not isinstance(coordinate, list) or len(coordinate) != 2:\n                    raise ToolError(f\"{coordinate} must be a tuple of length 2\")\n                if not all(isinstance(i, int) and i >= 0 for i in coordinate):\n                    raise ToolError(f\"{coordinate} must be a tuple of non-negative ints\")\n\n                x, y = self.scale_coordinates(\n                    ScalingSource.API, coordinate[0], coordinate[1]\n                )\n\n                if action == \"mouse_move\":\n                    await asyncio.to_thread(pyautogui.moveTo, x, y)\n                elif action == \"left_click_drag\":\n                    await asyncio.to_thread(pyautogui.dragTo, x, y)\n\n                return await self.screenshot()\n\n            if action in (\"key\", \"type\"):\n                if text is None:\n                    raise ToolError(f\"text is required for {action}\")\n                if coordinate is not None:\n                    raise ToolError(f\"coordinate is not accepted for {action}\")\n                if not isinstance(text, str):\n                    raise ToolError(output=f\"{text} must be a string\")\n\n                if action == \"key\":\n                    await asyncio.to_thread(pyautogui.press, text)\n                elif action == \"type\":\n                    for chunk in chunks(text, TYPING_GROUP_SIZE):\n                        await asyncio.to_thread(\n                            pyautogui.write, chunk, interval=TYPING_DELAY_MS/1000\n                        )\n\n                return await self.screenshot()\n\n            if action in (\n                \"left_click\",\n                \"right_click\",\n                \"double_click\",\n                \"middle_click\",\n                \"screenshot\",\n                \"cursor_position\",\n            ):\n                if text is not None:\n                    raise ToolError(f\"text is not accepted for {action}\")\n                if coordinate is not None:\n                    raise ToolError(f\"coordinate is not accepted for {action}\")\n\n                if action == \"screenshot\":\n                    return await self.screenshot()\n                elif action == \"cursor_position\":\n                    x, y = win32api.GetCursorPos()\n                    scaled_x, scaled_y = self.scale_coordinates(\n                        ScalingSource.COMPUTER, x, y\n                    )\n                    return ToolResult(\n                        output=f\"X={scaled_x},Y={scaled_y}\",\n                        error=None,\n                        base64_image=None\n                    )\n                else:\n                    click_funcs = {\n                        \"left_click\": lambda: pyautogui.click(button='left'),\n                        \"right_click\": lambda: pyautogui.click(button='right'),\n                        \"middle_click\": lambda: pyautogui.click(button='middle'),\n                        \"double_click\": lambda: pyautogui.doubleClick(),\n                    }\n                    await asyncio.to_thread(click_funcs[action])\n                    return await self.screenshot()\n\n            raise ToolError(f\"Invalid action: {action}\")\n\n        except pyautogui.FailSafeException as e:\n            raise ToolError(f\"Mouse movement failed (hit screen edge): {str(e)}\")\n        except Exception as e:\n            raise ToolError(f\"Action failed: {str(e)}\")\n\n    async def screenshot(self) -> ToolResult:\n        \"\"\"Take a screenshot of the current screen and return the base64 encoded image.\"\"\"\n        output_dir = Path(OUTPUT_DIR)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        path = output_dir / f\"screenshot_{uuid4().hex}.png\"\n\n        try:\n            # Take screenshot using pyautogui\n            screenshot = await asyncio.to_thread(pyautogui.screenshot)\n            \n            if self._scaling_enabled:\n                x, y = self.scale_coordinates(\n                    ScalingSource.COMPUTER, self.width, self.height\n                )\n                screenshot = screenshot.resize((x, y), Image.Resampling.LANCZOS)\n\n            # Save the screenshot\n            screenshot.save(str(path))\n            \n            # Add delay to let things settle\n            await asyncio.sleep(self._screenshot_delay)\n            \n            # Return the result with base64 encoded image\n            return ToolResult(\n                output=None,\n                error=None,\n                base64_image=base64.b64encode(path.read_bytes()).decode()\n            )\n        except Exception as e:\n            raise ToolError(f\"Failed to take screenshot: {str(e)}\")\n        finally:\n            # Clean up the temporary file\n            try:\n                if path.exists():\n                    path.unlink()\n            except Exception:\n                pass\n\n    def scale_coordinates(self, source: ScalingSource, x: int, y: int):\n        \"\"\"Scale coordinates to a target maximum resolution.\"\"\"\n        if not self._scaling_enabled:\n            return x, y\n        ratio = self.width / self.height\n        target_dimension = None\n        for dimension in MAX_SCALING_TARGETS.values():\n            # allow some error in the aspect ratio - not ratios are exactly 16:9\n            if abs(dimension[\"width\"] / dimension[\"height\"] - ratio) < 0.02:\n                if dimension[\"width\"] < self.width:\n                    target_dimension = dimension\n                break\n        if target_dimension is None:\n            return x, y\n        # should be less than 1\n        x_scaling_factor = target_dimension[\"width\"] / self.width\n        y_scaling_factor = target_dimension[\"height\"] / self.height\n        if source == ScalingSource.API:\n            if x > self.width or y > self.height:\n                raise ToolError(f\"Coordinates {x}, {y} are out of bounds\")\n            # scale up\n            return round(x / x_scaling_factor), round(y / y_scaling_factor)\n        # scale down\n        return round(x * x_scaling_factor), round(y * y_scaling_factor)\n\n```\n\n\n",
        "eval_script": "import asyncio\nimport base64\nimport os\nimport sys # Added to perform platform check\nfrom enum import StrEnum\nfrom pathlib import Path\nfrom typing import Literal, TypedDict, Any\nfrom uuid import uuid4\nfrom PIL import Image\nfrom abc import ABCMeta, abstractmethod\nfrom dataclasses import dataclass, fields, replace\n\nif sys.platform.startswith('win'):\n    import pyautogui\n    import win32api\n    import win32con\n\n    pyautogui.FAILSAFE = True\n    pyautogui.PAUSE = 0.1\n\nTYPING_DELAY_MS = 12\nTYPING_GROUP_SIZE = 50\n\nOUTPUT_DIR = os.path.join(os.environ.get('TEMP', '.'), 'outputs')\n\nAction = Literal[\n    \"key\",\n    \"type\",\n    \"mouse_move\",\n    \"left_click\",\n    \"left_click_drag\",\n    \"right_click\",\n    \"middle_click\",\n    \"double_click\",\n    \"screenshot\",\n    \"cursor_position\",\n]\n\nclass Resolution(TypedDict):\n    width: int\n    height: int\n\nMAX_SCALING_TARGETS: dict[str, Resolution] = {\n    \"XGA\": Resolution(width=1024, height=768),  # 4:3\n    \"WXGA\": Resolution(width=1280, height=800),  # 16:10\n    \"FWXGA\": Resolution(width=1366, height=768),  # ~16:9\n}\n\nclass ScalingSource(StrEnum):\n    COMPUTER = \"computer\"\n    API = \"api\"\n\nclass ComputerToolOptions(TypedDict):\n    display_height_px: int\n    display_width_px: int\n    display_number: int | None\n\ndef chunks(s: str, chunk_size: int) -> list[str]:\n    return [s[i : i + chunk_size] for i in range(0, len(s), chunk_size)]\n\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"Executes the tool with the given arguments.\"\"\"\n        ...\n\n    @abstractmethod\n    def to_params(self):\n        raise NotImplementedError\n\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n\nclass ComputerTool(BaseAnthropicTool):\n    \"\"\"\n    A tool that allows the agent to interact with the screen, keyboard, and mouse of the current computer.\n    Windows-compatible implementation using pyautogui and win32api.\n    \"\"\"\n\n    name: Literal[\"computer\"] = \"computer\"\n    api_type: Literal[\"computer_20241022\"] = \"computer_20241022\"  # Updated to match expected type\n    width: int\n    height: int\n    display_num: int | None\n\n    _screenshot_delay = 2.0\n    _scaling_enabled = True\n\n    @property\n    def options(self) -> ComputerToolOptions:\n        width, height = self.scale_coordinates(\n            ScalingSource.COMPUTER, self.width, self.height\n        )\n        return {\n            \"display_width_px\": width,\n            \"display_height_px\": height,\n            \"display_number\": self.display_num,\n        }\n\n    def to_params(self):\n        return {\"name\": self.name, \"type\": self.api_type, **self.options}\n\n    def __init__(self):\n        super().__init__()\n        if sys.platform.startswith('win'):\n            # Get primary monitor resolution\n            self.width = win32api.GetSystemMetrics(win32con.SM_CXSCREEN)\n            self.height = win32api.GetSystemMetrics(win32con.SM_CYSCREEN)\n        else:\n            # Fallback resolution for non-Windows environment\n            self.width = 1920\n            self.height = 1080\n        self.display_num = None  # Windows handles multiple displays differently\n\n    async def __call__(\n        self,\n        *,\n        action: Action,\n        text: str | None = None,\n        coordinate: tuple[int, int] | None = None,\n        **kwargs,\n    ):\n        try:\n            if action in (\"mouse_move\", \"left_click_drag\"):\n                if coordinate is None:\n                    raise ToolError(f\"coordinate is required for {action}\")\n                if text is not None:\n                    raise ToolError(f\"text is not accepted for {action}\")\n                if not isinstance(coordinate, list) or len(coordinate) != 2:\n                    raise ToolError(f\"{coordinate} must be a tuple of length 2\")\n                if not all(isinstance(i, int) and i >= 0 for i in coordinate):\n                    raise ToolError(f\"{coordinate} must be a tuple of non-negative ints\")\n\n                x, y = self.scale_coordinates(\n                    ScalingSource.API, coordinate[0], coordinate[1]\n                )\n\n                if sys.platform.startswith('win'):\n                    if action == \"mouse_move\":\n                        await asyncio.to_thread(pyautogui.moveTo, x, y)\n                    elif action == \"left_click_drag\":\n                        await asyncio.to_thread(pyautogui.dragTo, x, y)\n\n                return await self.screenshot()\n\n            if action in (\"key\", \"type\"):\n                if text is None:\n                    raise ToolError(f\"text is required for {action}\")\n                if coordinate is not None:\n                    raise ToolError(f\"coordinate is not accepted for {action}\")\n                if not isinstance(text, str):\n                    raise ToolError(output=f\"{text} must be a string\")\n\n                if sys.platform.startswith('win'):\n                    if action == \"key\":\n                        await asyncio.to_thread(pyautogui.press, text)\n                    elif action == \"type\":\n                        for chunk in chunks(text, TYPING_GROUP_SIZE):\n                            await asyncio.to_thread(\n                                pyautogui.write, chunk, interval=TYPING_DELAY_MS/1000\n                            )\n\n                return await self.screenshot()\n\n            if action in (\n                \"left_click\",\n                \"right_click\",\n                \"double_click\",\n                \"middle_click\",\n                \"screenshot\",\n                \"cursor_position\",\n            ):\n                if text is not None:\n                    raise ToolError(f\"text is not accepted for {action}\")\n                if coordinate is not None:\n                    raise ToolError(f\"coordinate is not accepted for {action}\")\n\n                if action == \"screenshot\":\n                    return await self.screenshot()\n                elif action == \"cursor_position\" and sys.platform.startswith('win'):\n                    x, y = win32api.GetCursorPos()\n                    scaled_x, scaled_y = self.scale_coordinates(\n                        ScalingSource.COMPUTER, x, y\n                    )\n                    return ToolResult(\n                        output=f\"X={scaled_x},Y={scaled_y}\",\n                        error=None,\n                        base64_image=None\n                    )\n                elif sys.platform.startswith('win'):\n                    click_funcs = {\n                        \"left_click\": lambda: pyautogui.click(button='left'),\n                        \"right_click\": lambda: pyautogui.click(button='right'),\n                        \"middle_click\": lambda: pyautogui.click(button='middle'),\n                        \"double_click\": lambda: pyautogui.doubleClick(),\n                    }\n                    await asyncio.to_thread(click_funcs[action])\n                    return await self.screenshot()\n\n            raise ToolError(f\"Invalid action: {action}\")\n\n        except pyautogui.FailSafeException as e:\n            raise ToolError(f\"Mouse movement failed (hit screen edge): {str(e)}\")\n        except Exception as e:\n            raise ToolError(f\"Action failed: {str(e)}\")\n\n    async def screenshot(self) -> ToolResult:\n        \"\"\"Take a screenshot of the current screen and return the base64 encoded image.\"\"\"\n        output_dir = Path(OUTPUT_DIR)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        path = output_dir / f\"screenshot_{uuid4().hex}.png\"\n\n        try:\n            if sys.platform.startswith('win'):\n                # Take screenshot using pyautogui\n                screenshot = await asyncio.to_thread(pyautogui.screenshot)\n\n                if self._scaling_enabled:\n                    x, y = self.scale_coordinates(\n                        ScalingSource.COMPUTER, self.width, self.height\n                    )\n                    screenshot = screenshot.resize((x, y), Image.Resampling.LANCZOS)\n\n                # Save the screenshot\n                screenshot.save(str(path))\n            \n            # Return the result with base64 encoded image\n            return ToolResult(\n                output=None,\n                error=None,\n                base64_image=base64.b64encode(path.read_bytes()).decode()\n            )\n        except Exception as e:\n            raise ToolError(f\"Failed to take screenshot: {str(e)}\")\n        finally:\n            # Clean up the temporary file\n            try:\n                if path.exists():\n                    path.unlink()\n            except Exception:\n                pass\n\n    def scale_coordinates(self, source: ScalingSource, x: int, y: int):\n        \"\"\"Scale coordinates to a target maximum resolution.\"\"\"\n        if not self._scaling_enabled:\n            return x, y\n        ratio = self.width / self.height\n        target_dimension = None\n        for dimension in MAX_SCALING_TARGETS.values():\n            # allow some error in the aspect ratio - not ratios are exactly 16:9\n            if abs(dimension[\"width\"] / dimension[\"height\"] - ratio) < 0.02:\n                if dimension[\"width\"] < self.width:\n                    target_dimension = dimension\n                break\n        if target_dimension is None:\n            return x, y\n        # should be less than 1\n        x_scaling_factor = target_dimension[\"width\"] / self.width\n        y_scaling_factor = target_dimension[\"height\"] / self.height\n        if source == ScalingSource.API:\n            if x > self.width or y > self.height:\n                raise ToolError(f\"Coordinates {x}, {y} are out of bounds\")\n            # scale up\n            return round(x / x_scaling_factor), round(y / y_scaling_factor)\n        # scale down\n        return round(x * x_scaling_factor), round(y * y_scaling_factor)\n\n\ndef test_scale_coordinates():\n    tool = ComputerTool()\n    \n    # Test 1\n    assert tool.scale_coordinates(ScalingSource.COMPUTER, 640, 480) == tool.scale_coordinates_new_implementation(ScalingSource.COMPUTER, 640, 480)\n    \n    # Test 2\n    assert tool.scale_coordinates(ScalingSource.API, 800, 600) == tool.scale_coordinates_new_implementation(ScalingSource.API, 800, 600)\n    \n    # Test 3\n    assert tool.scale_coordinates(ScalingSource.COMPUTER, 1024, 768) == tool.scale_coordinates_new_implementation(ScalingSource.COMPUTER, 1024, 768)\n\nif __name__ == \"__main__\":\n    test_scale_coordinates()"
    },
    {
        "func_name": "collate",
        "idx": "258",
        "repo_name": "charlesap___gyre",
        "func_path": "parsing.py",
        "orig_func": "def collate(thecst):\n    ok = True\n    current = ''\n    for (assrt, things) in thecst.items():\n        if ok:\n            if current == '':\n                current = things[2]\n            if current != things[2]:\n                print(assrt, 'mixed use of for statements')\n                ok = False\n            if things[1] == False:\n                print(assrt, 'refereced but not defined')\n                ok = False\n    return (ok, current)",
        "orig_context": "```python\n## parsing.py\ndef collate(thecst):\n    ok = True\n    current=\"\"\n    for assrt,things in thecst.items():\n        if ok:\n            if current == \"\":\n                current = things[2]\n            if current != things[2]:\n                print(assrt,\"mixed use of for statements\")\n                ok = False;\n            if things[1]==False:\n                print(assrt,\"refereced but not defined\")\n                ok = False\n#            else:\n#                print(assrt,things)\n    return ok,current\n\n```\n\n\n",
        "eval_script": "## parsing.py\ndef collate(thecst):\n    ok = True\n    current=\"\"\n    for assrt,things in thecst.items():\n        if ok:\n            if current == \"\":\n                current = things[2]\n            if current != things[2]:\n                print(assrt,\"mixed use of for statements\")\n                ok = False;\n            if things[1]==False:\n                print(assrt,\"refereced but not defined\")\n                ok = False\n#            else:\n#                print(assrt,things)\n    return ok,current\n\n# Assuming the placeholder for the new implementation. In practice, this should be an actual function.\n\n\ndef test_collate():\n    # Test case 1: consistent 'for' usage and all references defined\n    input_data_1 = {'assert1': ('A', True, 'loop1'), 'assert2': ('B', True, 'loop1')}\n    assert collate(input_data_1) == collate_new_implementation(input_data_1)\n\n    # Test case 2: mixed 'for' usage\n    input_data_2 = {'assert1': ('A', True, 'loop1'), 'assert2': ('B', True, 'loop2')}\n    assert collate(input_data_2) == collate_new_implementation(input_data_2)\n\n    # Test case 3: undefined reference\n    input_data_3 = {'assert1': ('A', False, 'loop1')}\n    assert collate(input_data_3) == collate_new_implementation(input_data_3)\n\nif __name__ == \"__main__\":\n    test_collate()"
    },
    {
        "func_name": "GyreTransformer.qual",
        "idx": "260",
        "repo_name": "charlesap___gyre",
        "func_path": "parsing.py",
        "orig_func": "def qual(self, args):\n    rv = []\n    for subtree in args[0].children:\n        rv.append(subtree)\n    return list(args)",
        "orig_context": "```python\n## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\n```\n\n\n",
        "eval_script": "## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n\n        \n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\ndef test_qual():\n    transformer = GyreTransformer()\n    \n    # Test with empty input\n    args = [Tree(\"root\", [])]\n    assert transformer.qual(args) == transformer.qual_new_implementation(args)\n\n    # Test with single child\n    args = [Tree(\"root\", [Tree(\"child\", [])])]\n    assert transformer.qual(args) == transformer.qual_new_implementation(args)\n    \n    # Test with multiple children\n    args = [Tree(\"root\", [Tree(\"child1\", []), Tree(\"child2\", [])])]\n    assert transformer.qual(args) == transformer.qual_new_implementation(args)\n\nif __name__ == \"__main__\":\n    test_qual()"
    },
    {
        "func_name": "GyreTransformer.layout",
        "idx": "261",
        "repo_name": "charlesap___gyre",
        "func_path": "parsing.py",
        "orig_func": "def layout(self, args):\n    (nm0, nm1) = args[0]\n    return ('BORDERS', nm0, nm1, args[1], None)",
        "orig_context": "```python\n## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\n```\n\n\n",
        "eval_script": "## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\ndef test_layout():\n    transformer = GyreTransformer()\n    \n    # Test 1\n    args1 = [(\"name0\", \"name1\"), \"argument1\"]\n    assert transformer.layout(args1) == transformer.layout_new_implementation(args1)\n    \n    # Test 2\n    args2 = [(\"border0\", \"border1\"), \"argument2\"]\n    assert transformer.layout(args2) == transformer.layout_new_implementation(args2)\n    \n    # Test 3\n    args3 = [(\"edge0\", \"edge1\"), \"argument3\"]\n    assert transformer.layout(args3) == transformer.layout_new_implementation(args3)\n\nif __name__ == \"__main__\":\n    test_layout()"
    },
    {
        "func_name": "GyreTransformer.identity",
        "idx": "262",
        "repo_name": "charlesap___gyre",
        "func_path": "parsing.py",
        "orig_func": "def identity(self, args):\n    (nm0, nm1) = args[0]\n    return ('IS', nm0, nm1, args[1], None)",
        "orig_context": "```python\n## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\n```\n\n\n",
        "eval_script": "## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\ndef test_identity():\n    transformer = GyreTransformer()\n    \n    # Test case 1\n    args1 = [(\"name1\", \"name2\"), \"some_argument\"]\n    assert transformer.identity(args1) == transformer.identity_new_implementation(args1)\n    \n    # Test case 2\n    args2 = [(\"another_name1\", \"another_name2\"), \"different_argument\"]\n    assert transformer.identity(args2) == transformer.identity_new_implementation(args2)\n    \n    # Test case 3\n    args3 = [(\"\", \"\"), \"empty_names\"]\n    assert transformer.identity(args3) == transformer.identity_new_implementation(args3)\n\nif __name__ == \"__main__\":\n    test_identity()"
    },
    {
        "func_name": "GyreTransformer.projection",
        "idx": "263",
        "repo_name": "charlesap___gyre",
        "func_path": "parsing.py",
        "orig_func": "def projection(self, args):\n    if len(args) == 4:\n        how = args[2].upper()\n        rest = args[3]\n    else:\n        how = args[2].upper()\n        mod = args[3]\n        rest = args[4]\n    name = args[0][0]\n    more = args[0][1]\n    return (args[1].upper(), name, more, how, rest)",
        "orig_context": "```python\n## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\n```\n\n\n",
        "eval_script": "## parsing.py\nfrom lark import Lark, Transformer, Tree\n\nclass GyreTransformer(Transformer):\n\n    def scope(self, args):\n        return \"FOR\",\"\"+args[0],None,None,None\n\n    def qname(self, args):\n        return \"\"+args[0],args[1]\n\n    def mname(self, args):\n        return \"\"+args[0],args[1]\n\n    def qual(self, args):\n        rv = []\n        for subtree in args[0].children:\n            rv.append(subtree)\n        return list(args)\n\n    def identx(self, args):\n        return args\n\n    def namelist(self, args):\n        return args\n\n    def pairlist(self, args):\n        return args[0],args[1]\n\n    def orlist(self, args):\n        return args\n\n    def projector(self, args):\n        return \"\"+args[0]\n\n    def layout(self, args):\n        nm0, nm1 = args[0]\n        return \"BORDERS\",nm0,nm1,args[1],None\n\n    def modifier(self, args):\n        return \"\"+args[0]\n\n    def identity(self, args):\n        nm0, nm1 = args[0]\n        return \"IS\",nm0,nm1,args[1],None\n\n    def composition(self, args):\n        return \"HAS\",\"\"+args[0],None,args[1],None\n\n    def qualification(self, args):\n        return \"IN\",\"\"+args[1],args[0],None,None\n\n    def projection(self, args):\n        if len(args) == 4:\n            how = args[2].upper()\n            rest = args[3]\n        else:\n            how = args[2].upper()\n            mod = args[3]\n            rest = args[4]\n        name=args[0][0]\n        more=args[0][1]\n        return args[1].upper(),name,more,how,rest\n\n    def subletter(self, args):\n        return Tree(\"s\",\"\"+args[0])\n\n    def notsubletter(self, args):\n        return Tree(\"n\",\"\"+args[0])\n\n    def subdigit(self, args):\n        return Tree(\"d\",\"\"+args[0])\n\n    def subnumber(self, args):\n        return Tree(\"n\",args[0])\n\n    def subrange(self, args):\n        return Tree(\"r\",args)\n\n\n\ndef test_projection():\n    transformer = GyreTransformer()\n\n    # Test case 1\n    args1 = [(\"name1\", \"more1\"), \"direction\", \"how1\", \"rest1\"]\n    result1_orig = transformer.projection(args1)\n    result1_new = transformer.projection_new_implementation(args1)\n    assert result1_orig == result1_new, f\"Test case 1 failed: {result1_orig} != {result1_new}\"\n\n    # Test case 2\n    args2 = [(\"name2\", \"more2\"), \"direction\", \"how2\", \"mod2\", \"rest2\"]\n    result2_orig = transformer.projection(args2)\n    result2_new = transformer.projection_new_implementation(args2)\n    assert result2_orig == result2_new, f\"Test case 2 failed: {result2_orig} != {result2_new}\"\n\n    # Test case 3\n    args3 = [(\"name3\", \"\"), \"forward\", \"path\", \"result\"]\n    result3_orig = transformer.projection(args3)\n    result3_new = transformer.projection_new_implementation(args3)\n    assert result3_orig == result3_new, f\"Test case 3 failed: {result3_orig} != {result3_new}\"\n\nif __name__ == \"__main__\":\n    test_projection()"
    },
    {
        "func_name": "analyze_sentiment",
        "idx": "266",
        "repo_name": "hk-kumawat___Aura-Smart-Assistant",
        "func_path": "app.py",
        "orig_func": "def analyze_sentiment(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sentiment_score = analyzer.polarity_scores(text)\n    return sentiment_score",
        "orig_context": "```python\n## app.py\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\ndef analyze_sentiment(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sentiment_score = analyzer.polarity_scores(text)\n    return sentiment_score\n\n```\n\n\n",
        "eval_script": "## app.py\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\ndef analyze_sentiment(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sentiment_score = analyzer.polarity_scores(text)\n    return sentiment_score\n\ndef test_analyze_sentiment():\n    text_positive = \"I love this beautiful sunny day!\"\n    text_negative = \"I hate rainy days. They are awful!\"\n    text_neutral = \"Today is a day.\"\n\n    result_original = analyze_sentiment(text_positive)\n    result_new = analyze_sentiment_new_implementation(text_positive)\n    assert result_original == result_new, f\"Positive sentiment test failed: {result_original} != {result_new}\"\n    \n    result_original = analyze_sentiment(text_negative)\n    result_new = analyze_sentiment_new_implementation(text_negative)\n    assert result_original == result_new, f\"Negative sentiment test failed: {result_original} != {result_new}\"\n    \n    result_original = analyze_sentiment(text_neutral)\n    result_new = analyze_sentiment_new_implementation(text_neutral)\n    assert result_original == result_new, f\"Neutral sentiment test failed: {result_original} != {result_new}\"\n\nif __name__ == \"__main__\":\n    test_analyze_sentiment()"
    },
    {
        "func_name": "converter_dolar_para_reais",
        "idx": "267",
        "repo_name": "lucasaccioly___ExPython06",
        "func_path": "ExPython06/ex01.py",
        "orig_func": "def converter_dolar_para_reais(valor_dolar):\n    taxa_conversao = 5.09\n    valor_reais = valor_dolar * taxa_conversao\n    return valor_reais",
        "orig_context": "```python\n## ExPython06/ex01.py\ndef converter_dolar_para_reais(valor_dolar):\n    taxa_conversao = 5.09\n    valor_reais = valor_dolar * taxa_conversao\n    return valor_reais\n\n```\n\n\n",
        "eval_script": "def converter_dolar_para_reais(valor_dolar):\n    taxa_conversao = 5.09\n    valor_reais = valor_dolar * taxa_conversao\n    return valor_reais\n\n# Assume this new implementation exists\n\n\ndef test_converter_dolar_para_reais():\n    # Test 1: Simple case\n    assert converter_dolar_para_reais(1) == converter_dolar_para_reais_new_implementation(1)\n    # Test 2: Zero case\n    assert converter_dolar_para_reais(0) == converter_dolar_para_reais_new_implementation(0)\n    # Test 3: Fractional dollar case\n    assert converter_dolar_para_reais(1.5) == converter_dolar_para_reais_new_implementation(1.5)\n\nif __name__ == \"__main__\":\n    test_converter_dolar_para_reais()"
    },
    {
        "func_name": "calcular_frete",
        "idx": "268",
        "repo_name": "lucasaccioly___ExPython06",
        "func_path": "ExPython06/ex01.py",
        "orig_func": "def calcular_frete(peso_em_gramas):\n    taxa_frete_por_100g = 1.99\n    peso_em_kg = peso_em_gramas / 1000\n    valor_frete = peso_em_kg * taxa_frete_por_100g\n    return valor_frete",
        "orig_context": "```python\n## ExPython06/ex01.py\ndef calcular_frete(peso_em_gramas):\n    taxa_frete_por_100g = 1.99\n    peso_em_kg = peso_em_gramas / 1000\n    valor_frete = peso_em_kg * taxa_frete_por_100g\n    return valor_frete\n\n```\n\n\n",
        "eval_script": "## ExPython06/ex01.py\ndef calcular_frete(peso_em_gramas):\n    taxa_frete_por_100g = 1.99\n    peso_em_kg = peso_em_gramas / 1000\n    valor_frete = peso_em_kg * taxa_frete_por_100g\n    return valor_frete\n\n\n\ndef test_calcular_frete():\n    # Test case 1\n    assert calcular_frete(1000) == calcular_frete_new_implementation(1000), \"Test case 1 failed\"\n    \n    # Test case 2\n    assert calcular_frete(500) == calcular_frete_new_implementation(500), \"Test case 2 failed\"\n    \n    # Test case 3\n    assert calcular_frete(2500) == calcular_frete_new_implementation(2500), \"Test case 3 failed\"\n\nif __name__ == \"__main__\":\n    test_calcular_frete()"
    },
    {
        "func_name": "limit_by_key",
        "idx": "269",
        "repo_name": "deadlock-api___deadlock-api-analytics",
        "func_path": "deadlock_analytics_api/rate_limiter/limiter.py",
        "orig_func": "def limit_by_key(key: str, rate_limit: RateLimit) -> RateLimitStatus:\n    LOGGER.debug(f'Checking rate limit: key={key!r} rate_limit={rate_limit!r}')\n    current_time = float(time.time())\n    result = redis_conn().zrange(key, current_time - rate_limit.period, current_time, byscore=True)\n    times = list(map(float, result))\n    return RateLimitStatus(key=key, count=len(times), limit=rate_limit.limit, period=rate_limit.period, oldest_request_time=times[0] if times else 0)",
        "orig_context": "```python\n## deadlock_analytics_api/rate_limiter/models.py\nimport time\n\nfrom fastapi import HTTPException\n\nfrom pydantic import BaseModel\n\nclass RateLimitStatus(BaseModel):\n    key: str\n    count: int\n    limit: int\n    period: int\n    oldest_request_time: float\n\n    @property\n    def remaining(self) -> int:\n        return max(self.limit - self.count, 0)\n\n    @property\n    def is_limited(self) -> bool:\n        return self.count > self.limit\n\n    @property\n    def next_request_in(self) -> float:\n        if self.count < self.limit:\n            return 0\n        age_of_last_request = time.time() - self.oldest_request_time\n        return max(self.period - age_of_last_request, 0)\n\n    @property\n    def headers(self) -> dict[str, str]:\n        return {\n            \"RateLimit-Limit\": str(self.limit),\n            \"RateLimit-Period\": str(self.period),\n            \"RateLimit-Remaining\": str(self.remaining),\n            \"RateLimit-Reset\": str(self.next_request_in),\n            \"Retry-After\": str(self.next_request_in),\n        }\n\n    def raise_for_limit(self):\n        if self.is_limited:\n            raise HTTPException(\n                status_code=429,\n                detail={\n                    \"error\": \"rate_limit_exceeded\",\n                    \"message\": \"Rate limit exceeded, please check the headers for more information.\",\n                },\n                headers=self.headers,\n            )\n\nclass RateLimit(BaseModel):\n    limit: int\n    period: int\n    path: str | None = None\n\n```\n\n\n```python\n## deadlock_analytics_api/globs.py\nimport os\n\nimport redis\n\nREDIS_HOST = os.environ.get(\"REDIS_HOST\", \"redis\")\n\nREDIS_PASS = os.environ.get(\"REDIS_PASS\")\n\ndef redis_conn():\n    return redis.Redis(\n        host=REDIS_HOST, port=6379, password=REDIS_PASS, db=0, decode_responses=True\n    )\n\n```\n\n\n```python\n## deadlock_analytics_api/rate_limiter/limiter.py\nimport logging\n\nimport time\n\nfrom deadlock_analytics_api.globs import ENFORCE_RATE_LIMITS, postgres_conn, redis_conn\n\nfrom deadlock_analytics_api.rate_limiter.models import RateLimit, RateLimitStatus\n\nLOGGER = logging.getLogger(__name__)\n\ndef limit_by_key(key: str, rate_limit: RateLimit) -> RateLimitStatus:\n    LOGGER.debug(f\"Checking rate limit: {key=} {rate_limit=}\")\n    current_time = float(time.time())\n    result = redis_conn().zrange(\n        key, current_time - rate_limit.period, current_time, byscore=True\n    )\n    times = list(map(float, result))\n    return RateLimitStatus(\n        key=key,\n        count=len(times),\n        limit=rate_limit.limit,\n        period=rate_limit.period,\n        oldest_request_time=times[0] if times else 0,\n    )\n\n```\n\n\n",
        "eval_script": "# Import necessary modules and classes\nimport logging\nimport time\nfrom typing import List\n\n# Mocking the Redis functionality using a simple in-memory list\nclass MockRedis:\n    def __init__(self):\n        self.storage = {}\n\n    def zrange(self, key, start, end, byscore=False) -> List[str]:\n        if key not in self.storage:\n            return []\n        # Extract elements within the range defined by start and end\n        return [str(score) for score in self.storage[key] if start <= score <= end]\n\nmock_redis_instance = MockRedis()\n\ndef redis_conn():\n    return mock_redis_instance\n\n# Define the RateLimit and RateLimitStatus model classes\nfrom pydantic import BaseModel\nfrom fastapi import HTTPException\n\nclass RateLimitStatus(BaseModel):\n    key: str\n    count: int\n    limit: int\n    period: int\n    oldest_request_time: float\n\n    @property\n    def remaining(self) -> int:\n        return max(self.limit - self.count, 0)\n\n    @property\n    def is_limited(self) -> bool:\n        return self.count > self.limit\n\n    @property\n    def next_request_in(self) -> float:\n        if self.count < self.limit:\n            return 0\n        age_of_last_request = time.time() - self.oldest_request_time\n        return max(self.period - age_of_last_request, 0)\n\n    @property\n    def headers(self) -> dict[str, str]:\n        return {\n            \"RateLimit-Limit\": str(self.limit),\n            \"RateLimit-Period\": str(self.period),\n            \"RateLimit-Remaining\": str(self.remaining),\n            \"RateLimit-Reset\": str(self.next_request_in),\n            \"Retry-After\": str(self.next_request_in),\n        }\n\n    def raise_for_limit(self):\n        if self.is_limited:\n            raise HTTPException(\n                status_code=429,\n                detail={\n                    \"error\": \"rate_limit_exceeded\",\n                    \"message\": \"Rate limit exceeded, please check the headers for more information.\",\n                },\n                headers=self.headers,\n            )\n\nclass RateLimit(BaseModel):\n    limit: int\n    period: int\n    path: str | None = None\n\n# Actual function implementation\nLOGGER = logging.getLogger(__name__)\n\ndef limit_by_key(key: str, rate_limit: RateLimit) -> RateLimitStatus:\n    LOGGER.debug(f\"Checking rate limit: {key=} {rate_limit=}\")\n    current_time = float(time.time())\n    result = redis_conn().zrange(\n        key, current_time - rate_limit.period, current_time, byscore=True\n    )\n    times = list(map(float, result))\n    return RateLimitStatus(\n        key=key,\n        count=len(times),\n        limit=rate_limit.limit,\n        period=rate_limit.period,\n        oldest_request_time=times[0] if times else 0,\n    )\n\n# Assume that this is the new implementation that needs validation\n\n\n# Test function\ndef test_limit_by_key():\n    mock_redis_instance.storage = {\n        \"user_123\": [time.time() - 10, time.time() - 8, time.time() - 5]\n    }\n    rate_limit = RateLimit(limit=3, period=60)\n    result_old = limit_by_key(\"user_123\", rate_limit)\n    result_new = limit_by_key_new_implementation(\"user_123\", rate_limit)\n    assert result_old.count == result_new.count, \"Counts do not match\"\n    assert result_old.remaining == result_new.remaining, \"Remaining requests do not match\"\n    assert result_old.is_limited == result_new.is_limited, \"Limit status does not match\"\n\n# Main function to execute tests\nif __name__ == \"__main__\":\n    test_limit_by_key()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "MatchMetadataPlayer.from_dict",
        "idx": "270",
        "repo_name": "deadlock-api___deadlock-api-analytics",
        "func_path": "deadlock_analytics_api/models/match_metadata.py",
        "orig_func": "@classmethod\ndef from_dict(cls, row: dict[str, any]):\n    book_reward = {k.replace('book_reward.', ''): v for (k, v) in row.items() if k.startswith('book_reward.')}\n    book_reward = [dict(zip(book_reward, t)) for t in zip(*book_reward.values())]\n    items = {k.replace('items.', ''): v for (k, v) in row.items() if k.startswith('items.')}\n    items = [dict(zip(items, t)) for t in zip(*items.values())]\n    death_details = {k.replace('death_details.', ''): v for (k, v) in row.items() if k.startswith('death_details.')}\n    death_details = [dict(zip(death_details, t)) for t in zip(*death_details.values())]\n    return cls(account_id=row['account_id'], player_slot=row['player_slot'], team=row['team'], kills=row['kills'], deaths=row['deaths'], assists=row['assists'], net_worth=row['net_worth'], hero_id=row['hero_id'], last_hits=row['last_hits'], denies=row['denies'], ability_points=row['ability_points'], party=row['party'], assigned_lane=row['assigned_lane'], player_level=row['player_level'], abandon_match_time_s=row['abandon_match_time_s'], ability_stats=row['ability_stats'], stats_type_stat=row['stats_type_stat'], book_reward=[MatchMetadataPlayerBookReward.from_dict(row) for row in book_reward], death_details=[MatchMetadataPlayerDeathDetails.from_dict(row) for row in death_details], items=[MatchMetadataPlayerItems.from_dict(row) for row in items])",
        "orig_context": "```python\n## deadlock_analytics_api/models/match_metadata.py\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass MatchMetadataPlayerBookReward(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    book_id: int\n    xp_amount: int\n    starting_xp: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerDeathDetails(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    killer_player_slot: int\n    death_pos: list[float]\n    killer_pos: list[float]\n    death_duration_s: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerItems(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    item_id: int\n    upgrade_id: int\n    sold_time_s: int\n    flags: int\n    imbued_ability_id: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayer(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    account_id: int\n    player_slot: int\n    team: str\n    kills: int\n    deaths: int\n    assists: int\n    net_worth: int\n    hero_id: int\n    last_hits: int\n    denies: int\n    ability_points: int\n    party: int\n    assigned_lane: int\n    player_level: int\n    abandon_match_time_s: int\n    ability_stats: dict[int, int]\n    stats_type_stat: list[float]\n    book_reward: list[MatchMetadataPlayerBookReward]\n    death_details: list[MatchMetadataPlayerDeathDetails]\n    items: list[MatchMetadataPlayerItems]\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        book_reward = {\n            k.replace(\"book_reward.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"book_reward.\")\n        }\n        book_reward = [dict(zip(book_reward, t)) for t in zip(*book_reward.values())]\n        items = {\n            k.replace(\"items.\", \"\"): v for k, v in row.items() if k.startswith(\"items.\")\n        }\n        items = [dict(zip(items, t)) for t in zip(*items.values())]\n        death_details = {\n            k.replace(\"death_details.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"death_details.\")\n        }\n        death_details = [\n            dict(zip(death_details, t)) for t in zip(*death_details.values())\n        ]\n        return cls(\n            account_id=row[\"account_id\"],\n            player_slot=row[\"player_slot\"],\n            team=row[\"team\"],\n            kills=row[\"kills\"],\n            deaths=row[\"deaths\"],\n            assists=row[\"assists\"],\n            net_worth=row[\"net_worth\"],\n            hero_id=row[\"hero_id\"],\n            last_hits=row[\"last_hits\"],\n            denies=row[\"denies\"],\n            ability_points=row[\"ability_points\"],\n            party=row[\"party\"],\n            assigned_lane=row[\"assigned_lane\"],\n            player_level=row[\"player_level\"],\n            abandon_match_time_s=row[\"abandon_match_time_s\"],\n            ability_stats=row[\"ability_stats\"],\n            stats_type_stat=row[\"stats_type_stat\"],\n            book_reward=[\n                MatchMetadataPlayerBookReward.from_dict(row) for row in book_reward\n            ],\n            death_details=[\n                MatchMetadataPlayerDeathDetails.from_dict(row) for row in death_details\n            ],\n            items=[MatchMetadataPlayerItems.from_dict(row) for row in items],\n        )\n\n```\n\n\n",
        "eval_script": "## deadlock_analytics_api/models/match_metadata.py\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass MatchMetadataPlayerBookReward(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    book_id: int\n    xp_amount: int\n    starting_xp: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerDeathDetails(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    killer_player_slot: int\n    death_pos: list[float]\n    killer_pos: list[float]\n    death_duration_s: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerItems(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    item_id: int\n    upgrade_id: int\n    sold_time_s: int\n    flags: int\n    imbued_ability_id: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayer(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    account_id: int\n    player_slot: int\n    team: str\n    kills: int\n    deaths: int\n    assists: int\n    net_worth: int\n    hero_id: int\n    last_hits: int\n    denies: int\n    ability_points: int\n    party: int\n    assigned_lane: int\n    player_level: int\n    abandon_match_time_s: int\n    ability_stats: dict[int, int]\n    stats_type_stat: list[float]\n    book_reward: list[MatchMetadataPlayerBookReward]\n    death_details: list[MatchMetadataPlayerDeathDetails]\n    items: list[MatchMetadataPlayerItems]\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        book_reward = {\n            k.replace(\"book_reward.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"book_reward.\")\n        }\n        book_reward = [dict(zip(book_reward, t)) for t in zip(*book_reward.values())]\n        items = {\n            k.replace(\"items.\", \"\"): v for k, v in row.items() if k.startswith(\"items.\")\n        }\n        items = [dict(zip(items, t)) for t in zip(*items.values())]\n        death_details = {\n            k.replace(\"death_details.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"death_details.\")\n        }\n        death_details = [\n            dict(zip(death_details, t)) for t in zip(*death_details.values())\n        ]\n        return cls(\n            account_id=row[\"account_id\"],\n            player_slot=row[\"player_slot\"],\n            team=row[\"team\"],\n            kills=row[\"kills\"],\n            deaths=row[\"deaths\"],\n            assists=row[\"assists\"],\n            net_worth=row[\"net_worth\"],\n            hero_id=row[\"hero_id\"],\n            last_hits=row[\"last_hits\"],\n            denies=row[\"denies\"],\n            ability_points=row[\"ability_points\"],\n            party=row[\"party\"],\n            assigned_lane=row[\"assigned_lane\"],\n            player_level=row[\"player_level\"],\n            abandon_match_time_s=row[\"abandon_match_time_s\"],\n            ability_stats=row[\"ability_stats\"],\n            stats_type_stat=row[\"stats_type_stat\"],\n            book_reward=[\n                MatchMetadataPlayerBookReward.from_dict(row) for row in book_reward\n            ],\n            death_details=[\n                MatchMetadataPlayerDeathDetails.from_dict(row) for row in death_details\n            ],\n            items=[MatchMetadataPlayerItems.from_dict(row) for row in items],\n        )\n\n\n\n\ndef test_from_dict():\n    # Test case 1\n    test_data_1 = {\n        \"account_id\": 1,\n        \"player_slot\": 2,\n        \"team\": \"Radiant\",\n        \"kills\": 10,\n        \"deaths\": 2,\n        \"assists\": 8,\n        \"net_worth\": 5000,\n        \"hero_id\": 102,\n        \"last_hits\": 100,\n        \"denies\": 10,\n        \"ability_points\": 5,\n        \"party\": 1,\n        \"assigned_lane\": 2,\n        \"player_level\": 15,\n        \"abandon_match_time_s\": 0,\n        \"ability_stats\": {1: 3, 2: 2},\n        \"stats_type_stat\": [1.0, 2.0],\n        \"book_reward\": [],\n        \"death_details\": [],\n        \"items\": []\n    }\n\n    assert MatchMetadataPlayer.from_dict(test_data_1) == MatchMetadataPlayer.from_dict_new_implementation(test_data_1)\n\n    # Test case 2\n    test_data_2 = {\n        \"account_id\": 3,\n        \"player_slot\": 4,\n        \"team\": \"Dire\",\n        \"kills\": 5,\n        \"deaths\": 12,\n        \"assists\": 15,\n        \"net_worth\": 3000,\n        \"hero_id\": 203,\n        \"last_hits\": 50,\n        \"denies\": 5,\n        \"ability_points\": 7,\n        \"party\": 0,\n        \"assigned_lane\": 1,\n        \"player_level\": 10,\n        \"abandon_match_time_s\": 0,\n        \"ability_stats\": {3: 1},\n        \"stats_type_stat\": [1.5],\n        \"book_reward\": [],\n        \"death_details\": [],\n        \"items\": []\n    }\n\n    assert MatchMetadataPlayer.from_dict(test_data_2) == MatchMetadataPlayer.from_dict_new_implementation(test_data_2)\n\n    # Test case 3\n    test_data_3 = {\n        \"account_id\": 5,\n        \"player_slot\": 1,\n        \"team\": \"Radiant\",\n        \"kills\": 20,\n        \"deaths\": 5,\n        \"assists\": 25,\n        \"net_worth\": 8000,\n        \"hero_id\": 300,\n        \"last_hits\": 200,\n        \"denies\": 15,\n        \"ability_points\": 10,\n        \"party\": 1,\n        \"assigned_lane\": 3,\n        \"player_level\": 20,\n        \"abandon_match_time_s\": 0,\n        \"ability_stats\": {5: 4},\n        \"stats_type_stat\": [2.0],\n        \"book_reward\": [],\n        \"death_details\": [],\n        \"items\": []\n    }\n\n    assert MatchMetadataPlayer.from_dict(test_data_3) == MatchMetadataPlayer.from_dict_new_implementation(test_data_3)\n\nif __name__ == \"__main__\":\n    test_from_dict()"
    },
    {
        "func_name": "MatchMetadata.from_rows",
        "idx": "271",
        "repo_name": "deadlock-api___deadlock-api-analytics",
        "func_path": "deadlock_analytics_api/models/match_metadata.py",
        "orig_func": "@classmethod\ndef from_rows(cls, match_info: dict[str, any], match_players: list[dict[str, any]]):\n    objectives = {k.replace('objectives.', ''): v for (k, v) in match_info.items() if k.startswith('objectives.')}\n    objectives = [dict(zip(objectives, t)) for t in zip(*objectives.values())]\n    mid_boss = {k.replace('mid_boss.', ''): v for (k, v) in match_info.items() if k.startswith('mid_boss.')}\n    mid_boss = [dict(zip(mid_boss, t)) for t in zip(*mid_boss.values())]\n    region = next((r.get('region_mode') for r in match_players if r.get('region_mode')))\n    return cls(match_id=match_info['match_id'], region=region, start_time=match_info['start_time'], winning_team=match_info['winning_team'], duration_s=match_info['duration_s'], match_outcome=match_info['match_outcome'], match_mode=match_info['match_mode'], game_mode=match_info['game_mode'], sample_time_s=match_info['sample_time_s'], stat_type=match_info['stat_type'], source_name=match_info['source_name'], objectives_mask_team0=match_info['objectives_mask_team0'], objectives_mask_team1=match_info['objectives_mask_team1'], is_high_skill_range_parties=match_info.get('is_high_skill_range_parties'), low_pri_pool=match_info.get('low_pri_pool'), new_player_pool=match_info.get('new_player_pool'), objectives=[MatchMetadataObjectives.from_dict(row) for row in objectives], mid_boss=[MatchMetadataMidBoss.from_dict(row) for row in mid_boss], players=[MatchMetadataPlayer.from_dict(row) for row in match_players])",
        "orig_context": "```python\n## deadlock_analytics_api/models/match_metadata.py\nimport datetime\n\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass MatchMetadataObjectives(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    destroyed_time_s: int\n    creep_damage: int\n    creep_damage_mitigated: int\n    player_damage: int\n    player_damage_mitigated: int\n    first_damage_time_s: int\n    team_objective: str\n    team: str\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataMidBoss(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    team_killed: str\n    team_claimed: str\n    destroyed_time_s: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerBookReward(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    book_id: int\n    xp_amount: int\n    starting_xp: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerDeathDetails(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    killer_player_slot: int\n    death_pos: list[float]\n    killer_pos: list[float]\n    death_duration_s: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerItems(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    item_id: int\n    upgrade_id: int\n    sold_time_s: int\n    flags: int\n    imbued_ability_id: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayer(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    account_id: int\n    player_slot: int\n    team: str\n    kills: int\n    deaths: int\n    assists: int\n    net_worth: int\n    hero_id: int\n    last_hits: int\n    denies: int\n    ability_points: int\n    party: int\n    assigned_lane: int\n    player_level: int\n    abandon_match_time_s: int\n    ability_stats: dict[int, int]\n    stats_type_stat: list[float]\n    book_reward: list[MatchMetadataPlayerBookReward]\n    death_details: list[MatchMetadataPlayerDeathDetails]\n    items: list[MatchMetadataPlayerItems]\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        book_reward = {\n            k.replace(\"book_reward.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"book_reward.\")\n        }\n        book_reward = [dict(zip(book_reward, t)) for t in zip(*book_reward.values())]\n        items = {\n            k.replace(\"items.\", \"\"): v for k, v in row.items() if k.startswith(\"items.\")\n        }\n        items = [dict(zip(items, t)) for t in zip(*items.values())]\n        death_details = {\n            k.replace(\"death_details.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"death_details.\")\n        }\n        death_details = [\n            dict(zip(death_details, t)) for t in zip(*death_details.values())\n        ]\n        return cls(\n            account_id=row[\"account_id\"],\n            player_slot=row[\"player_slot\"],\n            team=row[\"team\"],\n            kills=row[\"kills\"],\n            deaths=row[\"deaths\"],\n            assists=row[\"assists\"],\n            net_worth=row[\"net_worth\"],\n            hero_id=row[\"hero_id\"],\n            last_hits=row[\"last_hits\"],\n            denies=row[\"denies\"],\n            ability_points=row[\"ability_points\"],\n            party=row[\"party\"],\n            assigned_lane=row[\"assigned_lane\"],\n            player_level=row[\"player_level\"],\n            abandon_match_time_s=row[\"abandon_match_time_s\"],\n            ability_stats=row[\"ability_stats\"],\n            stats_type_stat=row[\"stats_type_stat\"],\n            book_reward=[\n                MatchMetadataPlayerBookReward.from_dict(row) for row in book_reward\n            ],\n            death_details=[\n                MatchMetadataPlayerDeathDetails.from_dict(row) for row in death_details\n            ],\n            items=[MatchMetadataPlayerItems.from_dict(row) for row in items],\n        )\n\nclass MatchMetadata(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    match_id: int\n    region: str\n    start_time: datetime.datetime\n    winning_team: str\n    duration_s: int\n    match_outcome: str\n    match_mode: str\n    game_mode: str\n    sample_time_s: list[int]\n    stat_type: list[int]\n    source_name: list[str]\n    objectives_mask_team0: int\n    objectives_mask_team1: int\n    is_high_skill_range_parties: bool | None = Field(None)\n    low_pri_pool: bool | None = Field(None)\n    new_player_pool: bool | None = Field(None)\n    objectives: list[MatchMetadataObjectives]\n    mid_boss: list[MatchMetadataMidBoss]\n    players: list[MatchMetadataPlayer]\n\n    @classmethod\n    def from_rows(\n        cls,\n        match_info: dict[str, any],\n        match_players: list[dict[str, any]],\n    ):\n        objectives = {\n            k.replace(\"objectives.\", \"\"): v\n            for k, v in match_info.items()\n            if k.startswith(\"objectives.\")\n        }\n        objectives = [dict(zip(objectives, t)) for t in zip(*objectives.values())]\n        mid_boss = {\n            k.replace(\"mid_boss.\", \"\"): v\n            for k, v in match_info.items()\n            if k.startswith(\"mid_boss.\")\n        }\n        mid_boss = [dict(zip(mid_boss, t)) for t in zip(*mid_boss.values())]\n        region = next(\n            r.get(\"region_mode\") for r in match_players if r.get(\"region_mode\")\n        )\n        return cls(\n            match_id=match_info[\"match_id\"],\n            region=region,\n            start_time=match_info[\"start_time\"],\n            winning_team=match_info[\"winning_team\"],\n            duration_s=match_info[\"duration_s\"],\n            match_outcome=match_info[\"match_outcome\"],\n            match_mode=match_info[\"match_mode\"],\n            game_mode=match_info[\"game_mode\"],\n            sample_time_s=match_info[\"sample_time_s\"],\n            stat_type=match_info[\"stat_type\"],\n            source_name=match_info[\"source_name\"],\n            objectives_mask_team0=match_info[\"objectives_mask_team0\"],\n            objectives_mask_team1=match_info[\"objectives_mask_team1\"],\n            is_high_skill_range_parties=match_info.get(\"is_high_skill_range_parties\"),\n            low_pri_pool=match_info.get(\"low_pri_pool\"),\n            new_player_pool=match_info.get(\"new_player_pool\"),\n            objectives=[MatchMetadataObjectives.from_dict(row) for row in objectives],\n            mid_boss=[MatchMetadataMidBoss.from_dict(row) for row in mid_boss],\n            players=[MatchMetadataPlayer.from_dict(row) for row in match_players],\n        )\n\n```\n\n\n",
        "eval_script": "## deadlock_analytics_api/models/match_metadata.py\nimport datetime\n\nfrom pydantic import BaseModel, ConfigDict, Field\n\nclass MatchMetadataObjectives(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    destroyed_time_s: int\n    creep_damage: int\n    creep_damage_mitigated: int\n    player_damage: int\n    player_damage_mitigated: int\n    first_damage_time_s: int\n    team_objective: str\n    team: str\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataMidBoss(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    team_killed: str\n    team_claimed: str\n    destroyed_time_s: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerBookReward(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    book_id: int\n    xp_amount: int\n    starting_xp: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerDeathDetails(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    killer_player_slot: int\n    death_pos: list[float]\n    killer_pos: list[float]\n    death_duration_s: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayerItems(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    game_time_s: int\n    item_id: int\n    upgrade_id: int\n    sold_time_s: int\n    flags: int\n    imbued_ability_id: int\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        return cls(**row)\n\nclass MatchMetadataPlayer(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    account_id: int\n    player_slot: int\n    team: str\n    kills: int\n    deaths: int\n    assists: int\n    net_worth: int\n    hero_id: int\n    last_hits: int\n    denies: int\n    ability_points: int\n    party: int\n    assigned_lane: int\n    player_level: int\n    abandon_match_time_s: int\n    ability_stats: dict[int, int]\n    stats_type_stat: list[float]\n    book_reward: list[MatchMetadataPlayerBookReward]\n    death_details: list[MatchMetadataPlayerDeathDetails]\n    items: list[MatchMetadataPlayerItems]\n\n    @classmethod\n    def from_dict(cls, row: dict[str, any]):\n        book_reward = {\n            k.replace(\"book_reward.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"book_reward.\")\n        }\n        book_reward = [dict(zip(book_reward, t)) for t in zip(*book_reward.values())]\n        items = {\n            k.replace(\"items.\", \"\"): v for k, v in row.items() if k.startswith(\"items.\")\n        }\n        items = [dict(zip(items, t)) for t in zip(*items.values())]\n        death_details = {\n            k.replace(\"death_details.\", \"\"): v\n            for k, v in row.items()\n            if k.startswith(\"death_details.\")\n        }\n        death_details = [\n            dict(zip(death_details, t)) for t in zip(*death_details.values())\n        ]\n        return cls(\n            account_id=row[\"account_id\"],\n            player_slot=row[\"player_slot\"],\n            team=row[\"team\"],\n            kills=row[\"kills\"],\n            deaths=row[\"deaths\"],\n            assists=row[\"assists\"],\n            net_worth=row[\"net_worth\"],\n            hero_id=row[\"hero_id\"],\n            last_hits=row[\"last_hits\"],\n            denies=row[\"denies\"],\n            ability_points=row[\"ability_points\"],\n            party=row[\"party\"],\n            assigned_lane=row[\"assigned_lane\"],\n            player_level=row[\"player_level\"],\n            abandon_match_time_s=row[\"abandon_match_time_s\"],\n            ability_stats=row[\"ability_stats\"],\n            stats_type_stat=row[\"stats_type_stat\"],\n            book_reward=[\n                MatchMetadataPlayerBookReward.from_dict(row) for row in book_reward\n            ],\n            death_details=[\n                MatchMetadataPlayerDeathDetails.from_dict(row) for row in death_details\n            ],\n            items=[MatchMetadataPlayerItems.from_dict(row) for row in items],\n        )\n\nclass MatchMetadata(BaseModel):\n    model_config = ConfigDict(populate_by_name=True)\n\n    match_id: int\n    region: str\n    start_time: datetime.datetime\n    winning_team: str\n    duration_s: int\n    match_outcome: str\n    match_mode: str\n    game_mode: str\n    sample_time_s: list[int]\n    stat_type: list[int]\n    source_name: list[str]\n    objectives_mask_team0: int\n    objectives_mask_team1: int\n    is_high_skill_range_parties: bool | None = Field(None)\n    low_pri_pool: bool | None = Field(None)\n    new_player_pool: bool | None = Field(None)\n    objectives: list[MatchMetadataObjectives]\n    mid_boss: list[MatchMetadataMidBoss]\n    players: list[MatchMetadataPlayer]\n\n    @classmethod\n    def from_rows(\n        cls,\n        match_info: dict[str, any],\n        match_players: list[dict[str, any]],\n    ):\n        objectives = {\n            k.replace(\"objectives.\", \"\"): v\n            for k, v in match_info.items()\n            if k.startswith(\"objectives.\")\n        }\n        objectives = [dict(zip(objectives, t)) for t in zip(*objectives.values())]\n        mid_boss = {\n            k.replace(\"mid_boss.\", \"\"): v\n            for k, v in match_info.items()\n            if k.startswith(\"mid_boss.\")\n        }\n        mid_boss = [dict(zip(mid_boss, t)) for t in zip(*mid_boss.values())]\n        region = next(\n            r.get(\"region_mode\") for r in match_players if r.get(\"region_mode\")\n        )\n        return cls(\n            match_id=match_info[\"match_id\"],\n            region=region,\n            start_time=match_info[\"start_time\"],\n            winning_team=match_info[\"winning_team\"],\n            duration_s=match_info[\"duration_s\"],\n            match_outcome=match_info[\"match_outcome\"],\n            match_mode=match_info[\"match_mode\"],\n            game_mode=match_info[\"game_mode\"],\n            sample_time_s=match_info[\"sample_time_s\"],\n            stat_type=match_info[\"stat_type\"],\n            source_name=match_info[\"source_name\"],\n            objectives_mask_team0=match_info[\"objectives_mask_team0\"],\n            objectives_mask_team1=match_info[\"objectives_mask_team1\"],\n            is_high_skill_range_parties=match_info.get(\"is_high_skill_range_parties\"),\n            low_pri_pool=match_info.get(\"low_pri_pool\"),\n            new_player_pool=match_info.get(\"new_player_pool\"),\n            objectives=[MatchMetadataObjectives.from_dict(row) for row in objectives],\n            mid_boss=[MatchMetadataMidBoss.from_dict(row) for row in mid_boss],\n            players=[MatchMetadataPlayer.from_dict(row) for row in match_players],\n        )\n\n\ndef test_from_rows():\n    match_info = {\n        \"match_id\": 1234,\n        \"start_time\": datetime.datetime.utcnow(),\n        \"winning_team\": \"Team A\",\n        \"duration_s\": 3600,\n        \"match_outcome\": \"victory\",\n        \"match_mode\": \"ranked\",\n        \"game_mode\": \"deathmatch\",\n        \"sample_time_s\": [0, 100, 200],\n        \"stat_type\": [1, 2, 3],\n        \"source_name\": [\"source1\", \"source2\", \"source3\"],\n        \"objectives_mask_team0\": 1024,\n        \"objectives_mask_team1\": 2048,\n        \"is_high_skill_range_parties\": True,\n        \"low_pri_pool\": False,\n        \"new_player_pool\": None,\n        \"objectives.team\": [\"Team A\", \"Team B\"],\n        \"objectives.destroyed_time_s\": [100, 200],\n        \"objectives.creep_damage\": [10, 20],\n        \"objectives.creep_damage_mitigated\": [5, 10],\n        \"objectives.player_damage\": [100, 200],\n        \"objectives.player_damage_mitigated\": [50, 100],\n        \"objectives.first_damage_time_s\": [15, 30],\n        \"objectives.team_objective\": [\"Objective A\", \"Objective B\"],\n        \"mid_boss.team_killed\": [\"Team A\", \"Team B\"],\n        \"mid_boss.team_claimed\": [\"Team A\", \"Team B\"],\n        \"mid_boss.destroyed_time_s\": [150, 250],\n    }\n    match_players = [\n        {\n            \"account_id\": 1,\n            \"player_slot\": 1,\n            \"team\": \"Team A\",\n            \"kills\": 10,\n            \"deaths\": 5,\n            \"assists\": 3,\n            \"net_worth\": 5000,\n            \"hero_id\": 111,\n            \"last_hits\": 111,\n            \"denies\": 5,\n            \"ability_points\": 10,\n            \"party\": 1,\n            \"assigned_lane\": 1,\n            \"player_level\": 30,\n            \"abandon_match_time_s\": 0,\n            \"ability_stats\": {1: 100, 2: 200},\n            \"stats_type_stat\": [0.1, 0.2, 0.3],\n            \"book_reward.book_id\": [1, 2],\n            \"book_reward.xp_amount\": [1000, 2000],\n            \"book_reward.starting_xp\": [0, 500],\n            \"death_details.game_time_s\": [300, 600],\n            \"death_details.killer_player_slot\": [2, 3],\n            \"death_details.death_pos\": [[1.0, 2.0], [3.0, 4.0]],\n            \"death_details.killer_pos\": [[5.0, 6.0], [7.0, 8.0]],\n            \"death_details.death_duration_s\": [20, 30],\n            \"items.game_time_s\": [100, 200],\n            \"items.item_id\": [123, 456],\n            \"items.upgrade_id\": [0, 1],\n            \"items.sold_time_s\": [-1, -1],\n            \"items.flags\": [0, 1],\n            \"items.imbued_ability_id\": [1, 2],\n            \"region_mode\": \"NA\"\n        }\n    ]\n\n    # Test 1: Ensure outputs from both implementations are the same\n    original = MatchMetadata.from_rows(match_info, match_players)\n    new_implementation = MatchMetadata.from_rows_new_implementation(match_info, match_players)\n    assert original == new_implementation, \"The new implementation does not match the original for test 1.\"\n\n    # Test 2: Check if attributes are set correctly and uniformly\n    assert original.match_id == new_implementation.match_id, \"The match_id does not match.\"\n    assert original.players[0].account_id == new_implementation.players[0].account_id, \"The account_id does not match.\"\n\n    # Test 3: Ensure both have the same number of objectives\n    assert len(original.objectives) == len(new_implementation.objectives), \"The number of objectives does not match.\"\n\nif __name__ == \"__main__\":\n    test_from_rows()"
    },
    {
        "func_name": "TelaMenu.desenhar",
        "idx": "272",
        "repo_name": "gustavorodrigues45___calculadoraPrimo",
        "func_path": "calculaPrimo.py",
        "orig_func": "def desenhar(self):\n    tela.fill(BRANCO)\n    titulo = fonte.render('Bem-vindo \u00e0 Calculadora de N\u00fameros Primos', True, PRETO)\n    tela.blit(titulo, (100, 100))\n    pygame.draw.rect(tela, VERMELHO, self.iniciar_botao)\n    iniciar_texto = fonte.render('Iniciar', True, BRANCO)\n    tela.blit(iniciar_texto, (self.iniciar_botao.x + 15, self.iniciar_botao.y + 10))",
        "orig_context": "```python\n## calculaPrimo.py\nimport pygame\n\ntela = pygame.display.set_mode((largura, altura))\n\nBRANCO = (255, 255, 255)\n\nPRETO = (0, 0, 0)\n\nVERMELHO = (255, 0, 0)\n\nfonte = pygame.font.Font(None, 32)\n\nclass TelaMenu:\n    def __init__(self):\n        self.iniciar_botao = pygame.Rect(250, 200, 100, 50)\n    \n    def desenhar(self):\n        tela.fill(BRANCO)\n        titulo = fonte.render(\"Bem-vindo \u00e0 Calculadora de N\u00fameros Primos\", True, PRETO)\n        tela.blit(titulo, (100, 100))\n        \n        # Desenha o bot\u00e3o \"Iniciar\"\n        pygame.draw.rect(tela, VERMELHO, self.iniciar_botao)\n        iniciar_texto = fonte.render(\"Iniciar\", True, BRANCO)\n        tela.blit(iniciar_texto, (self.iniciar_botao.x + 15, self.iniciar_botao.y + 10))\n    \n    def lidar_eventos(self, evento):\n        if evento.type == pygame.MOUSEBUTTONDOWN:\n            # Verifica se o usu\u00e1rio clicou no bot\u00e3o \"Iniciar\"\n            if self.iniciar_botao.collidepoint(evento.pos):\n                return \"calculadora\"  # Muda para a tela da calculadora\n        return \"menu\"\n\n```\n\n\n",
        "eval_script": "# Necessary imports\nimport pygame\nimport numpy as np\n\n# Initialize pygame\npygame.init()\n\n# Define screen dimensions\nlargura, altura = 640, 480\n\n# Set up font\nfonte = pygame.font.Font(None, 32)\n\nBRANCO = (255, 255, 255)\nPRETO = (0, 0, 0)\nVERMELHO = (255, 0, 0)\n\nclass TelaMenu:\n    def __init__(self):\n        self.iniciar_botao = pygame.Rect(250, 200, 100, 50)\n    \n    def desenhar(self):\n        surface = pygame.Surface((largura, altura))\n        surface.fill(BRANCO)\n        titulo = fonte.render(\"Bem-vindo \u00e0 Calculadora de N\u00fameros Primos\", True, PRETO)\n        surface.blit(titulo, (100, 100))\n        \n        # Desenha o bot\u00e3o \"Iniciar\"\n        pygame.draw.rect(surface, VERMELHO, self.iniciar_botao)\n        iniciar_texto = fonte.render(\"Iniciar\", True, BRANCO)\n        surface.blit(iniciar_texto, (self.iniciar_botao.x + 15, self.iniciar_botao.y + 10))\n        return surface\n    \n\n    \n    def lidar_eventos(self, evento):\n        if evento.type == pygame.MOUSEBUTTONDOWN:\n            # Verifica se o usu\u00e1rio clicou no bot\u00e3o \"Iniciar\"\n            if self.iniciar_botao.collidepoint(evento.pos):\n                return \"calculadora\"  # Muda para a tela da calculadora\n        return \"menu\"\n\ndef test_desenhar():\n    tela_menu = TelaMenu()\n    surface_original = tela_menu.desenhar()\n    surface_new = tela_menu.desenhar_new_implementation()\n    \n    # Get pixel arrays for both surfaces\n    pixels_original = pygame.surfarray.pixels3d(surface_original)\n    pixels_new = pygame.surfarray.pixels3d(surface_new)\n    \n    # Perform the assertions to ensure they are the same\n    assert surface_original.get_size() == surface_new.get_size(), \"Surface sizes are different\"\n    assert (pixels_original == pixels_new).all(), \"Surface pixel arrays are different\"\n    assert np.array_equal(pixels_original, pixels_new), \"Arrays' content is not equal\"\n\nif __name__ == \"__main__\":\n    test_desenhar()\n    print(\"All tests passed successfully.\")"
    },
    {
        "func_name": "TelaCalculadora.desenhar",
        "idx": "273",
        "repo_name": "gustavorodrigues45___calculadoraPrimo",
        "func_path": "calculaPrimo.py",
        "orig_func": "def desenhar(self):\n    tela.fill(BRANCO)\n    instrucao = fonte.render('Insira algum n\u00famero para verificar se s\u00e3o primos', True, PRETO)\n    tela.blit(instrucao, (100, 50))\n    txt_surface = fonte.render(self.texto_input, True, PRETO)\n    largura_txt = max(200, txt_surface.get_width() + 10)\n    self.input_box.w = largura_txt\n    tela.blit(txt_surface, (self.input_box.x + 5, self.input_box.y + 5))\n    pygame.draw.rect(tela, self.cor_caixa, self.input_box, 2)\n    if self.mostrar_resultado:\n        resultado_surface = fonte.render(self.resultado, True, PRETO)\n        tela.blit(resultado_surface, (200, 250))",
        "orig_context": "```python\n## calculaPrimo.py\nimport pygame\n\ntela = pygame.display.set_mode((largura, altura))\n\nBRANCO = (255, 255, 255)\n\nPRETO = (0, 0, 0)\n\nVERMELHO = (255, 0, 0)\n\nfonte = pygame.font.Font(None, 32)\n\ndef eh_primo(numero):\n    if numero <= 1:\n        return False\n    for i in range(2, int(numero**0.5) + 1):\n        if numero % i == 0:\n            return False\n    return True\n\nclass TelaCalculadora:\n    def __init__(self):\n        self.input_box = pygame.Rect(200, 150, 140, 32)\n        self.cor_ativo = VERMELHO\n        self.cor_inativo = PRETO\n        self.cor_caixa = self.cor_inativo\n        self.texto_input = ''\n        self.mostrar_resultado = False\n        self.resultado = ''\n\n    def desenhar(self):\n        tela.fill(BRANCO)\n        \n        # Exibe a instru\u00e7\u00e3o para o usu\u00e1rio\n        instrucao = fonte.render(\"Insira algum n\u00famero para verificar se s\u00e3o primos\", True, PRETO)\n        tela.blit(instrucao, (100, 50))\n        \n        # Renderiza o texto da caixa de input\n        txt_surface = fonte.render(self.texto_input, True, PRETO)\n        largura_txt = max(200, txt_surface.get_width() + 10)\n        self.input_box.w = largura_txt\n        tela.blit(txt_surface, (self.input_box.x + 5, self.input_box.y + 5))\n        pygame.draw.rect(tela, self.cor_caixa, self.input_box, 2)\n        \n        # Renderiza o resultado, se necess\u00e1rio\n        if self.mostrar_resultado:\n            resultado_surface = fonte.render(self.resultado, True, PRETO)\n            tela.blit(resultado_surface, (200, 250))\n\n    def lidar_eventos(self, evento):\n        if evento.type == pygame.MOUSEBUTTONDOWN:\n            # Checa se o usu\u00e1rio clicou dentro da caixa de input\n            if self.input_box.collidepoint(evento.pos):\n                self.cor_caixa = self.cor_ativo\n            else:\n                self.cor_caixa = self.cor_inativo\n        elif evento.type == pygame.KEYDOWN:\n            # Verifica se a tecla \"Esc\" foi pressionada para retornar ao menu\n            if evento.key == pygame.K_ESCAPE:\n                return \"menu\"\n            if self.cor_caixa == self.cor_ativo:\n                if evento.key == pygame.K_RETURN or evento.key == pygame.K_KP_ENTER:\n                    # Verifica se o n\u00famero \u00e9 primo quando o usu\u00e1rio pressiona Enter\n                    try:\n                        numero = int(self.texto_input)\n                        if eh_primo(numero):\n                            self.resultado = f\"O n\u00famero {numero} \u00e9 primo.\"\n                        else:\n                            self.resultado = f\"O n\u00famero {numero} n\u00e3o \u00e9 primo.\"\n                    except ValueError:\n                        self.resultado = \"Digite um n\u00famero v\u00e1lido.\"\n                    self.mostrar_resultado = True\n                    self.texto_input = ''\n                elif evento.key == pygame.K_BACKSPACE:\n                    self.texto_input = self.texto_input[:-1]\n                else:\n                    self.texto_input += evento.unicode\n        return \"calculadora\"\n\n```\n\n\n",
        "eval_script": "import pygame\n\n# Initialize Pygame\npygame.init()\n\n# Define the width and height of the display\nlargura, altura = 640, 480\ntela = pygame.display.set_mode((largura, altura))\n\n# Define colors\nBRANCO = (255, 255, 255)\nPRETO = (0, 0, 0)\nVERMELHO = (255, 0, 0)\n\n# Initialize font after Pygame has been initialized\nfonte = pygame.font.Font(None, 32)\n\ndef eh_primo(numero):\n    if numero <= 1:\n        return False\n    for i in range(2, int(numero**0.5) + 1):\n        if numero % i == 0:\n            return False\n    return True\n\nclass TelaCalculadora:\n    def __init__(self):\n        self.input_box = pygame.Rect(200, 150, 140, 32)\n        self.cor_ativo = VERMELHO\n        self.cor_inativo = PRETO\n        self.cor_caixa = self.cor_inativo\n        self.texto_input = ''\n        self.mostrar_resultado = False\n        self.resultado = ''\n\n    def desenhar(self):\n        tela.fill(BRANCO)\n        \n        # Display instruction for the user\n        instrucao = fonte.render(\"Insira algum n\u00famero para verificar se s\u00e3o primos\", True, PRETO)\n        tela.blit(instrucao, (100, 50))\n        \n        # Render the text inside the input box\n        txt_surface = fonte.render(self.texto_input, True, PRETO)\n        largura_txt = max(200, txt_surface.get_width() + 10)\n        self.input_box.w = largura_txt\n        tela.blit(txt_surface, (self.input_box.x + 5, self.input_box.y + 5))\n        pygame.draw.rect(tela, self.cor_caixa, self.input_box, 2)\n        \n        # Render the result if necessary\n        if self.mostrar_resultado:\n            resultado_surface = fonte.render(self.resultado, True, PRETO)\n            tela.blit(resultado_surface, (200, 250))\n\n\n    def lidar_eventos(self, evento):\n        if evento.type == pygame.MOUSEBUTTONDOWN:\n            if self.input_box.collidepoint(evento.pos):\n                self.cor_caixa = self.cor_ativo\n            else:\n                self.cor_caixa = self.cor_inativo\n        elif evento.type == pygame.KEYDOWN:\n            if evento.key == pygame.K_ESCAPE:\n                return \"menu\"\n            if self.cor_caixa == self.cor_ativo:\n                if evento.key == pygame.K_RETURN or evento.key == pygame.K_KP_ENTER:\n                    try:\n                        numero = int(self.texto_input)\n                        if eh_primo(numero):\n                            self.resultado = f\"O n\u00famero {numero} \u00e9 primo.\"\n                        else:\n                            self.resultado = f\"O n\u00famero {numero} n\u00e3o \u00e9 primo.\"\n                    except ValueError:\n                        self.resultado = \"Digite um n\u00famero v\u00e1lido.\"\n                    self.mostrar_resultado = True\n                    self.texto_input = ''\n                elif evento.key == pygame.K_BACKSPACE:\n                    self.texto_input = self.texto_input[:-1]\n                else:\n                    self.texto_input += evento.unicode\n        return \"calculadora\"\n\ndef test_desenhar():\n    calculadora = TelaCalculadora()\n    \n    # Test Case 1: Default state - check initial rendering\n    calculadora.texto_input = '7'\n    calculadora.mostrar_resultado = False\n    tela.fill(BRANCO)  # Clear the screen before each test\n    calculadora.desenhar()\n    surface1 = tela.copy()\n    \n    tela.fill(BRANCO)\n    calculadora.desenhar_new_implementation()\n    surface2 = tela.copy()\n    \n    assert surface1.get_buffer().raw == surface2.get_buffer().raw, \"Failed on default state rendering.\"\n\n    # Test Case 2: Verification of active state\n    calculadora.mostrar_resultado = True\n    calculadora.resultado = \"O n\u00famero 7 \u00e9 primo.\"\n    tela.fill(BRANCO)\n    calculadora.desenhar()\n    surface3 = tela.copy()\n    \n    tela.fill(BRANCO)\n    calculadora.desenhar_new_implementation()\n    surface4 = tela.copy()\n    \n    assert surface3.get_buffer().raw == surface4.get_buffer().raw, \"Failed to render when result should be shown.\"\n\n    # Test Case 3: Verification of inactivated input box\n    calculadora.cor_caixa = calculadora.cor_inativo\n    tela.fill(BRANCO)\n    calculadora.desenhar()\n    surface5 = tela.copy()\n    \n    tela.fill(BRANCO)\n    calculadora.desenhar_new_implementation()\n    surface6 = tela.copy()\n    \n    assert surface5.get_buffer().raw == surface6.get_buffer().raw, \"Failed on inactive input box rendering.\"\n\nif __name__ == \"__main__\":\n    test_desenhar()"
    },
    {
        "func_name": "CodeAssistantAI.get_chat_history",
        "idx": "275",
        "repo_name": "thercior___code-assistent",
        "func_path": "clients/agent.py",
        "orig_func": "def get_chat_history(self, chats):\n    chat_history = []\n    for chat in chats:\n        chat_history.append(('human', chat.message))\n        chat_history.append(('ai', chat.response))\n    return chat_history",
        "orig_context": "```python\n## clients/agent.py\nfrom langchain_groq import ChatGroq\n\nfrom markdown import markdown\n\nclass CodeAssistantAI:\n\n    def __init__(self):\n        self.__model = ChatGroq(model='llama-3.2-90b-vision-preview')\n        self.__system_message = (\n            'Voc\u00ea \u00e9 um assistente de c\u00f3digo, respons\u00e1vel por tirar d\u00favidas sobre programa\u00e7\u00e3o da Linguagem Python, JavaScript/TypeScript, HTML/CSS, React, Tailwind/Boostrap, Docker, git e GithubActions.'\n            'Responda em formato markdown e sempre em portugu\u00eas brasileiro.'\n        )\n\n    def get_chat_history(self, chats):\n        chat_history = []\n\n        for chat in chats:\n            chat_history.append(('human', chat.message,))\n            chat_history.append(('ai', chat.response,))\n\n        return chat_history\n\n    def ask_ai(self, context, message):\n        messages = [\n            ('system', self.__system_message)\n        ]\n        messages.extend(context)\n        messages.append(('human', message,))\n\n        response = self.__model.invoke(messages)\n\n        return markdown(response.content, output_format='html')\n\n    def summarize_title(self, initial_message):\n        prompt = f\"Resuma esta mensagem em um t\u00edtulo curto: '{initial_message}'\"\n        response = self.ask_ai(context=\"\", message=prompt)\n        return response.strip()\n\n```\n\n\n",
        "eval_script": "## clients/agent.py\nfrom langchain_groq import ChatGroq\n\nfrom markdown import markdown\n\nclass CodeAssistantAI:\n\n    def __init__(self):\n        self.__model = ChatGroq(model='llama-3.2-90b-vision-preview', api_key='YOUR_API_KEY_HERE')\n        self.__system_message = (\n            'Voc\u00ea \u00e9 um assistente de c\u00f3digo, respons\u00e1vel por tirar d\u00favidas sobre programa\u00e7\u00e3o da Linguagem Python, JavaScript/TypeScript, HTML/CSS, React, Tailwind/Boostrap, Docker, git e GithubActions.'\n            'Responda em formato markdown e sempre em portugu\u00eas brasileiro.'\n        )\n\n    def get_chat_history(self, chats):\n        chat_history = []\n\n        for chat in chats:\n            chat_history.append(('human', chat.message,))\n            chat_history.append(('ai', chat.response,))\n\n        return chat_history\n\n\n    # Placeholder for the new implementation to be tested.\n\n\n    def ask_ai(self, context, message):\n        messages = [\n            ('system', self.__system_message)\n        ]\n        messages.extend(context)\n        messages.append(('human', message,))\n\n        response = self.__model.invoke(messages)\n\n        return markdown(response.content, output_format='html')\n\n    def summarize_title(self, initial_message):\n        prompt = f\"Resuma esta mensagem em um t\u00edtulo curto: '{initial_message}'\"\n        response = self.ask_ai(context=\"\", message=prompt)\n        return response.strip()\n\ndef test_get_chat_history():\n    # Create an instance of CodeAssistantAI\n    ai = CodeAssistantAI()\n\n    # Define some mock chat data for testing\n    mock_chats = [\n        type('MockChat', (object,), {'message': 'Ol\u00e1', 'response': 'Oi! Como posso ajudar?'}),\n        type('MockChat', (object,), {'message': 'Como criar um loop em Python?', 'response': 'Voc\u00ea pode usar for ou while.'}),\n        type('MockChat', (object,), {'message': 'O que \u00e9 uma fun\u00e7\u00e3o?', 'response': '\u00c9 um bloco de c\u00f3digo que s\u00f3 \u00e9 executado quando chamado.'}),\n    ]\n\n    # Expected output using the original implementation\n    expected_output = ai.get_chat_history(mock_chats)\n\n    # Test the new implementation against expected output\n    assert ai.get_chat_history_new_implementation(mock_chats) == expected_output\n    assert ai.get_chat_history_new_implementation([]) == ai.get_chat_history([])\n    assert ai.get_chat_history_new_implementation([mock_chats[0]]) == ai.get_chat_history([mock_chats[0]])\n\nif __name__ == \"__main__\":\n    test_get_chat_history()"
    },
    {
        "func_name": "CodeAssistantAI.ask_ai",
        "idx": "276",
        "repo_name": "thercior___code-assistent",
        "func_path": "clients/agent.py",
        "orig_func": "def ask_ai(self, context, message):\n    messages = [('system', self.__system_message)]\n    messages.extend(context)\n    messages.append(('human', message))\n    response = self.__model.invoke(messages)\n    return markdown(response.content, output_format='html')",
        "orig_context": "```python\n## clients/agent.py\nfrom langchain_groq import ChatGroq\n\nfrom markdown import markdown\n\nclass CodeAssistantAI:\n\n    def __init__(self):\n        self.__model = ChatGroq(model='llama-3.2-90b-vision-preview')\n        self.__system_message = (\n            'Voc\u00ea \u00e9 um assistente de c\u00f3digo, respons\u00e1vel por tirar d\u00favidas sobre programa\u00e7\u00e3o da Linguagem Python, JavaScript/TypeScript, HTML/CSS, React, Tailwind/Boostrap, Docker, git e GithubActions.'\n            'Responda em formato markdown e sempre em portugu\u00eas brasileiro.'\n        )\n\n    def get_chat_history(self, chats):\n        chat_history = []\n\n        for chat in chats:\n            chat_history.append(('human', chat.message,))\n            chat_history.append(('ai', chat.response,))\n\n        return chat_history\n\n    def ask_ai(self, context, message):\n        messages = [\n            ('system', self.__system_message)\n        ]\n        messages.extend(context)\n        messages.append(('human', message,))\n\n        response = self.__model.invoke(messages)\n\n        return markdown(response.content, output_format='html')\n\n    def summarize_title(self, initial_message):\n        prompt = f\"Resuma esta mensagem em um t\u00edtulo curto: '{initial_message}'\"\n        response = self.ask_ai(context=\"\", message=prompt)\n        return response.strip()\n\n```\n\n\n",
        "eval_script": "# clients/agent.py\nfrom markdown import markdown\n\n# Mock the ChatGroq class\nclass MockChatGroq:\n    def __init__(self, model):\n        self.model = model\n    \n    class Response:\n        def __init__(self, content):\n            self.content = content\n    \n    def invoke(self, messages):\n        # Simulating a response from the supposed ChatGroq API\n        return self.Response(content=\"Essa \u00e9 uma resposta simulada em portugu\u00eas brasileiro.\")\n\n# Using the mock class instead of the real ChatGroq\nclass CodeAssistantAI:\n    def __init__(self):\n        self.__model = MockChatGroq(model='llama-3.2-90b-vision-preview')\n        self.__system_message = (\n            'Voc\u00ea \u00e9 um assistente de c\u00f3digo, respons\u00e1vel por tirar d\u00favidas sobre programa\u00e7\u00e3o da Linguagem Python, JavaScript/TypeScript, HTML/CSS, React, Tailwind/Boostrap, Docker, git e GithubActions.'\n            'Responda em formato markdown e sempre em portugu\u00eas brasileiro.'\n        )\n\n    def get_chat_history(self, chats):\n        chat_history = []\n\n        for chat in chats:\n            chat_history.append(('human', chat.message,))\n            chat_history.append(('ai', chat.response,))\n\n        return chat_history\n\n    def ask_ai(self, context, message):\n        messages = [\n            ('system', self.__system_message)\n        ]\n        messages.extend(context)\n        messages.append(('human', message,))\n\n        response = self.__model.invoke(messages)\n\n        return markdown(response.content, output_format='html')\n\n\n\n    def summarize_title(self, initial_message):\n        prompt = f\"Resuma esta mensagem em um t\u00edtulo curto: '{initial_message}'\"\n        response = self.ask_ai(context=\"\", message=prompt)\n        return response.strip()\n\ndef test_ask_ai():\n    code_assistant = CodeAssistantAI()\n    \n    # Test case 1: Basic similarity check\n    context = [('human', 'Como criar uma vari\u00e1vel em Python?')]\n    message = 'Qual o comando para criar uma vari\u00e1vel?'\n    assert code_assistant.ask_ai(context, message) == code_assistant.ask_ai_new_implementation(context, message)\n    \n    # Test case 2: Empty context and message\n    context = []\n    message = ''\n    assert code_assistant.ask_ai(context, message) == code_assistant.ask_ai_new_implementation(context, message)\n    \n    # Test case 3: Long and complex message\n    complex_message = 'Explique como o gerenciamento de mem\u00f3ria funciona em C++, incluindo aloca\u00e7\u00e3o est\u00e1tica e dynamic, com exemplos detalhados.'\n    assert code_assistant.ask_ai(context, complex_message) == code_assistant.ask_ai_new_implementation(context, complex_message)\n\nif __name__ == \"__main__\":\n    test_ask_ai()"
    },
    {
        "func_name": "ConversationListView.get",
        "idx": "278",
        "repo_name": "thercior___code-assistent",
        "func_path": "apps/chatbot/views.py",
        "orig_func": "def get(self, request, *args, **kwargs):\n    conversations = request.user.conversations_chat.all()\n    return render(request, 'Chatbot/conversations_list.html', {'conversations': conversations})",
        "orig_context": "```python\n## apps/chatbot/views.py\nfrom django.contrib.auth.mixins import LoginRequiredMixin\n\nfrom django.shortcuts import render, redirect, get_object_or_404\n\nfrom django.views import View\n\nclass ConversationListView(LoginRequiredMixin, View):\n\n    def get(self, request, *args, **kwargs):\n        conversations = request.user.conversations_chat.all()\n        return render(request, 'Chatbot/conversations_list.html', {'conversations': conversations})\n\n```\n\n\n",
        "eval_script": "from types import SimpleNamespace\n\n# Mock render function that outputs the context instead of an actual HTML response\ndef mock_render(request, template_name, context):\n    return context\n\n# Mock user and request setup\nclass MockUser:\n    def __init__(self):\n        self.conversations_chat = self\n    \n    def all(self):\n        # Simulate a list of conversations\n        return [\"Conversation 1\", \"Conversation 2\", \"Conversation 3\"]\n\nclass MockRequest:\n    def __init__(self):\n        self.user = MockUser()\n\n# Minimal View class to serve as a base class for ConversationListView\nclass View:\n    pass\n\n# Define the class ConversationListView with slight modifications\nclass ConversationListView(View):\n\n    # Replacing the real render with our mocked one\n    render = staticmethod(mock_render)\n\n    def get(self, request, *args, **kwargs):\n        conversations = request.user.conversations_chat.all()\n        return self.render(request, 'Chatbot/conversations_list.html', {'conversations': conversations})\n\n\ndef test_get():\n    # Create a mock request object\n    mock_request = MockRequest()\n\n    # Instantiate the view\n    view = ConversationListView()\n\n    # Call both implementations\n    result_original = view.get(mock_request)\n    result_new = view.get_new_implementation(mock_request)\n\n    # Perform assertions\n    assert result_original == result_new, \"Results from original and new implementation do not match\"\n    assert len(result_original['conversations']) == len(result_new['conversations']), \"Number of conversations do not match\"\n    assert result_original['conversations'][0] == result_new['conversations'][0], \"First conversation does not match\"\n\nif __name__ == \"__main__\":\n    test_get()\n    print(\"All tests passed.\")"
    },
    {
        "func_name": "ChatBotView.get",
        "idx": "280",
        "repo_name": "thercior___code-assistent",
        "func_path": "apps/chatbot/views.py",
        "orig_func": "def get(self, request, conversation_id, *args, **kwargs):\n    conversations = request.user.conversations_chat.all()\n    conversation = get_object_or_404(ConversationChat, id=conversation_id, user=request.user)\n    chats = conversation.chats.all()\n    return render(request, 'Chatbot/chatbot.html', {'chats': chats, 'conversation': conversation, 'conversation_selected': conversation_id, 'conversations': conversations})",
        "orig_context": "```python\n## clients/agent.py\nfrom langchain_groq import ChatGroq\n\nfrom markdown import markdown\n\nclass CodeAssistantAI:\n\n    def __init__(self):\n        self.__model = ChatGroq(model='llama-3.2-90b-vision-preview')\n        self.__system_message = (\n            'Voc\u00ea \u00e9 um assistente de c\u00f3digo, respons\u00e1vel por tirar d\u00favidas sobre programa\u00e7\u00e3o da Linguagem Python, JavaScript/TypeScript, HTML/CSS, React, Tailwind/Boostrap, Docker, git e GithubActions.'\n            'Responda em formato markdown e sempre em portugu\u00eas brasileiro.'\n        )\n\n    def get_chat_history(self, chats):\n        chat_history = []\n\n        for chat in chats:\n            chat_history.append(('human', chat.message,))\n            chat_history.append(('ai', chat.response,))\n\n        return chat_history\n\n    def ask_ai(self, context, message):\n        messages = [\n            ('system', self.__system_message)\n        ]\n        messages.extend(context)\n        messages.append(('human', message,))\n\n        response = self.__model.invoke(messages)\n\n        return markdown(response.content, output_format='html')\n\n    def summarize_title(self, initial_message):\n        prompt = f\"Resuma esta mensagem em um t\u00edtulo curto: '{initial_message}'\"\n        response = self.ask_ai(context=\"\", message=prompt)\n        return response.strip()\n\n```\n\n\n```python\n## apps/chatbot/models.py\nfrom django.contrib.auth.models import User\n\nfrom django.db import models\n\nfrom clients.agent import CodeAssistantAI\n\nclass ConversationChat(models.Model):\n    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='conversations_chat', db_column='conversa_chat', verbose_name='Conversa de chat')\n    title = models.CharField(max_length=255, default='', blank=True, db_column='t\u00edtulo', verbose_name='t\u00edtulo')\n    created_at = models.DateTimeField(auto_now_add=True)\n\n    def __str__(self):\n        return f\"Conversa: {self.title} ({self.user.username})\"\n\n    def generate_title(self, initial_message):\n        code_assistant = CodeAssistantAI()\n        self.title = code_assistant.summarize_title(initial_message)\n        self.save()\n\nclass Chat(models.Model):\n    conversation = models.ForeignKey(ConversationChat, on_delete=models.CASCADE, related_name='chats', db_column='chat', verbose_name='chat')\n    message = models.TextField()\n    response = models.TextField()\n    created_at = models.DateTimeField(auto_now_add=True)\n\n    def __str__(self):\n        return f'Mensagem de {self.conversation.user.username}: {self.message[:50]}'\n\n```\n\n\n```python\n## apps/chatbot/views.py\nfrom django.contrib.auth.mixins import LoginRequiredMixin\n\nfrom django.http import JsonResponse\n\nfrom django.shortcuts import render, redirect, get_object_or_404\n\nfrom django.views import View\n\nfrom chatbot.models import Chat, ConversationChat\n\nfrom clients.agent import CodeAssistantAI\n\nclass ChatBotView(LoginRequiredMixin, View):\n    def __init__(self):\n        self.code_assistant = CodeAssistantAI()\n\n    def get(self, request, conversation_id, *args, **kwargs):\n        conversations = request.user.conversations_chat.all()\n        conversation = get_object_or_404(ConversationChat, id=conversation_id, user=request.user)\n        chats = conversation.chats.all()\n        return render(\n            request,\n            'Chatbot/chatbot.html',\n            {\n                'chats': chats,\n                'conversation': conversation,\n                'conversation_selected': conversation_id,\n                'conversations': conversations\n            }\n        )\n\n    def post(self, request, conversation_id, *args, **kwargs):\n        conversation = get_object_or_404(ConversationChat, id=conversation_id, user=request.user)\n        message = request.POST.get('message')\n\n        chats = conversation.chats.all()\n        context = self.code_assistant.get_chat_history(chats=chats)\n\n        response = self.code_assistant.ask_ai(\n            context=context,\n            message=message,\n        )\n\n        chat = Chat(\n            conversation=conversation,\n            message=message,\n            response=response,\n        )\n        chat.save()\n\n        if not conversation.title:\n            conversation.generate_title(initial_message=message)\n\n        return JsonResponse({\n            'message': message,\n            'response': response\n        })\n\n```\n\n\n",
        "eval_script": "# Mock implementations for necessary Django components and dependencies \n\n# Mocking Django's ORM-related components\nclass User:\n    def __init__(self, username):\n        self.username = username\n        self.conversations_chat = MockQuerySet()  # Use MockQuerySet here\n\nclass MockQuerySet(list):\n    def all(self):\n        return self\n\n    def filter(self, **kwargs):\n        return MockQuerySet([conversation for conversation in self if all(getattr(conversation, k) == v for k, v in kwargs.items())])\n\n# Mock for Django's ForeignKey related features\nclass ForeignKeyMock:\n    def __init__(self, to, on_delete, related_name, db_column, verbose_name):\n        self.to = to\n        self.on_delete = on_delete\n        self.related_name = related_name\n        self.db_column = db_column\n        self.verbose_name = verbose_name\n\n\n# Mocking Django's models.Model and its functionality\nclass BaseModel:\n    def save(self, *args, **kwargs):\n        # In a real scenario, this method would save to the database\n        pass\n\n\n# Mocking Django's models\nclass ConversationChat(BaseModel):\n    def __init__(self, user):\n        self.user = user\n        self.title = \"\"\n        self.chats = MockQuerySet()\n        self.user.conversations_chat.append(self)\n\n    def generate_title(self, initial_message):\n        code_assistant = CodeAssistantAI()\n        self.title = code_assistant.summarize_title(initial_message)\n\nclass Chat(BaseModel):\n    def __init__(self, conversation, message, response):\n        self.conversation = conversation\n        self.message = message\n        self.response = response\n        self.conversation.chats.append(self)\n\nclass LoginRequiredMixin:\n    pass\n\n# Mocking Django's View\nclass View:\n    pass\n\n# Mocking Django's get_object_or_404\ndef get_object_or_404(obj_class, id, user):\n    for index, item in enumerate(user.conversations_chat, start=1):\n        if id == index and item.user == user:\n            return item\n    raise Exception(\"Object not found\")\n\n# Mocking Django's render\ndef render(request, template_name, context):\n    return context\n\n# Mocking JsonResponse\nclass JsonResponse:\n    def __init__(self, data):\n        self.data = data\n\n# The code for CodeAssistantAI must be included as it is required by the provided code.\nclass CodeAssistantAI:\n    def __init__(self):\n        self.__model = \"ChatGroq Model Placeholder\"\n        self.__system_message = 'Voc\u00ea \u00e9 um assistente de c\u00f3digo, respons\u00e1vel por tirar d\u00favidas sobre programa\u00e7\u00e3o da Linguagem Python, JavaScript/TypeScript, HTML/CSS, React, Tailwind/Boostrap, Docker, git e GithubActions.' 'Responda em formato markdown e sempre em portugu\u00eas brasileiro.'\n\n    def get_chat_history(self, chats):\n        chat_history = []\n        for chat in chats:\n            chat_history.append(('human', chat.message,))\n            chat_history.append(('ai', chat.response,))\n        return chat_history\n\n    def ask_ai(self, context, message):\n        response = \"Simulated AI Response\"\n        return f\"<p>{response}</p>\"\n\n    def summarize_title(self, initial_message):\n        return \"Simulated Title\"\n\n# The main ChatBotView class provided above\nclass ChatBotView(LoginRequiredMixin, View):\n    def __init__(self):\n        self.code_assistant = CodeAssistantAI()\n\n    def get(self, request, conversation_id, *args, **kwargs):\n        conversations = request.user.conversations_chat.all()\n        conversation = get_object_or_404(ConversationChat, id=conversation_id, user=request.user)\n        chats = conversation.chats.all()\n        return render(\n            request,\n            'Chatbot/chatbot.html',\n            {\n                'chats': chats,\n                'conversation': conversation,\n                'conversation_selected': conversation_id,\n                'conversations': conversations\n            }\n        )\n\n\n    # New implementation method placeholder\n\n\n# Creating a mock request and user\nclass MockRequest:\n    def __init__(self, user):\n        self.user = user\n\n# Test function to ensure both implementations are identical in functionality\ndef test_get():\n    user = User(username=\"test_user\")\n    conversation = ConversationChat(user=user)\n    Chat(conversation, \"Ol\u00e1, como voc\u00ea est\u00e1?\", \"Estou bem, obrigado por perguntar.\")\n    Chat(conversation, \"Qual \u00e9 a capital da Fran\u00e7a?\", \"A capital da Fran\u00e7a \u00e9 Paris.\")\n\n    mock_request = MockRequest(user=user)\n    chat_view = ChatBotView()\n    conversation_id = 1\n\n    # Get responses from both implementations\n    response_original = chat_view.get(mock_request, conversation_id)\n    response_new = chat_view.get_new_implementation(mock_request, conversation_id)\n\n    # Assertions to ensure functionality is identical\n    assert response_original == response_new, \"Mismatch between original and new implementation\"\n    \n    # Additional tests\n    other_user = User(username=\"different_user\")\n    conversation2 = ConversationChat(user=other_user)\n    Chat(conversation2, \"Hello\", \"Hi\")\n\n    other_request = MockRequest(user=other_user)\n    response_original_2 = chat_view.get(other_request, 1)\n    response_new_2 = chat_view.get_new_implementation(other_request, 1)\n\n    assert response_original_2 == response_new_2, \"Second test mismatch\"\n\n    another_conversation = ConversationChat(user=user)\n    Chat(another_conversation, \"Testing\", \"Response\")\n    response_original_3 = chat_view.get(mock_request, 2)\n    response_new_3 = chat_view.get_new_implementation(mock_request, 2)\n\n    assert response_original_3 == response_new_3, \"Third test mismatch\"\n\n# Main function for execution\nif __name__ == \"__main__\":\n    # Execute the test\n    test_get()"
    },
    {
        "func_name": "CustomUserCreationForm.save",
        "idx": "282",
        "repo_name": "thercior___code-assistent",
        "func_path": "apps/accounts/forms.py",
        "orig_func": "def save(self, commit=True):\n    user = super().save(commit=False)\n    user.email = self.cleaned_data['email']\n    user.first_name = self.cleaned_data['first_name']\n    user.last_name = self.cleaned_data['last_name']\n    if commit:\n        user.save()\n    return user",
        "orig_context": "```python\n## apps/accounts/forms.py\nfrom django import forms\n\nfrom django.contrib.auth.models import User\n\nfrom django.contrib.auth.forms import UserCreationForm\n\nclass CustomUserCreationForm(UserCreationForm):\n    email = forms.EmailField(required=True)\n    first_name = forms.CharField(max_length=30, required=True)\n    last_name = forms.CharField(max_length=30, required=True)\n\n    class Meta:\n        model = User\n        fields = ['username', 'first_name', 'last_name', 'email', 'password1', 'password2']\n\n    def save(self, commit=True):\n        user = super().save(commit=False)\n        user.email = self.cleaned_data['email']\n        user.first_name = self.cleaned_data['first_name']\n        user.last_name = self.cleaned_data['last_name']\n\n        if commit:\n            user.save()\n        return user\n\n```\n\n\n",
        "eval_script": "# Mock setup to allow direct calling of CustomUserCreationForm.save\n\n# First, we'll create a mock User class similar to Django's User model for testing purposes\nclass MockUser:\n    def __init__(self, username='', first_name='', last_name='', email=''):\n        self.username = username\n        self.first_name = first_name\n        self.last_name = last_name\n        self.email = email\n        self.is_saved = False\n\n    def save(self):\n        self.is_saved = True\n\n# Mock Meta class and fields decorator to replace Django's behavior\nclass Meta:\n    fields = ['username', 'first_name', 'last_name', 'email', 'password1', 'password2']\n\n# We create a MockCleanedData to simulate Django's cleaned_data\nclass MockCleanedData:\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, item):\n        return self.data[item]\n\nclass CustomUserCreationForm:\n    # Simulated fields in the CustomUserCreationForm\n    cleaned_data = None\n\n    def __init__(self, data=None):\n        self.cleaned_data = MockCleanedData(data or {})\n        self.instance = MockUser()\n\n    class Meta:\n        model = MockUser\n        fields = Meta.fields\n\n    def save(self, commit=True):\n        user = self.instance\n        user.email = self.cleaned_data['email']\n        user.first_name = self.cleaned_data['first_name']\n        user.last_name = self.cleaned_data['last_name']\n\n        if commit:\n            user.save()\n        return user\n\n\n\ndef test_save():\n    data = {\n        'username': 'testuser',\n        'first_name': 'Test',\n        'last_name': 'User',\n        'email': 'test@example.com',\n        'password1': 'password123',\n        'password2': 'password123'\n    }\n    \n    form = CustomUserCreationForm(data)\n    \n    # Test with commit=True\n    user1 = form.save(commit=True)\n    user2 = form.save_new_implementation(commit=True)\n    \n    assert user1.email == user2.email\n    assert user1.first_name == user2.first_name\n    assert user1.last_name == user2.last_name\n    assert user1.is_saved == user2.is_saved\n    \n    # Test with commit=False\n    user1_no_commit = form.save(commit=False)\n    user2_no_commit = form.save_new_implementation(commit=False)\n    \n    assert user1_no_commit.is_saved == user2_no_commit.is_saved\n\nif __name__ == \"__main__\":\n    test_save()"
    },
    {
        "func_name": "NetworkXTrafficEngine._set_up_graph",
        "idx": "283",
        "repo_name": "GreenStarMatter___traffic_circuit_diagram",
        "func_path": "src/traffic_circuit_diagram/networkx_engine.py",
        "orig_func": "def _set_up_graph(self):\n    G = nx.Graph()\n    return G",
        "orig_context": "```python\n## src/traffic_circuit_diagram/traffic_light.py\nclass TrafficLight:\n    def __init__(self,outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\",\"In Progress\",\"Roadblock\",\"Not Started\",\"Abandoned\"]:\n             self.traffic_status = incoming_status\n             viable_status = True\n        else:\n             print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\",\"AND\",\"OR\",\"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/traffic_container.py\nfrom traffic_circuit_diagram.traffic_light import LogicGateTrafficLight\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/networkx_engine.py\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\nfrom traffic_circuit_diagram.traffic_container import TrafficLight, LogicGateTrafficLight\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm = \"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object,LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color = color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\n```\n\n\n",
        "eval_script": "import networkx as nx\nimport matplotlib.pyplot as plt\n\nclass TrafficLight:\n    def __init__(self, outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\", \"In Progress\", \"Roadblock\", \"Not Started\", \"Abandoned\"]:\n            self.traffic_status = incoming_status\n            viable_status = True\n        else:\n            print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\", \"AND\", \"OR\", \"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\nclass MockTrafficContainer:\n    def __init__(self):\n        self.traffic_lights = {\n            'light1': TrafficLight('Light 1', 'Complete'),\n            'light2': LogicGateTrafficLight('Light 2', 'In Progress')\n        }\n        self.roads = {\n            'road1': MockRoad('light1', 'light2', 'road1', True)\n        }\n        self.traffic_lights_coordinates = None\n    \nclass MockRoad:\n    def __init__(self, incoming_light, outgoing_light, road_name, was_traversed):\n        self.incoming_light = incoming_light\n        self.outgoing_light = outgoing_light\n        self.road_name = road_name\n        self.was_road_traversed = was_traversed\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm = \"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n    \n\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object,LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color = color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> str:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\ndef test__set_up_graph():\n    engine_old = NetworkXTrafficEngine()\n    graph_old = engine_old._set_up_graph()\n    \n    engine_new = NetworkXTrafficEngine()\n    graph_new = engine_new._set_up_graph_new_implementation()\n    \n    assert type(graph_old) == type(graph_new), \"Graph types don't match\"\n    assert len(graph_old.nodes) == len(graph_new.nodes), \"Node count mismatch\"\n    assert len(graph_old.edges) == len(graph_new.edges), \"Edge count mismatch\"\n\ndef main():\n    test__set_up_graph()\n    print(\"All tests passed!\")\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
        "func_name": "NetworkXTrafficEngine._set_up_graph_position",
        "idx": "284",
        "repo_name": "GreenStarMatter___traffic_circuit_diagram",
        "func_path": "src/traffic_circuit_diagram/networkx_engine.py",
        "orig_func": "def _set_up_graph_position(self, incoming_traffic_container):\n    if self.graph_position_algorithm == 'default':\n        pos = nx.spring_layout(self.traffic_circuit_diagram)\n    elif self.graph_position_algorithm == 'Custom Algorithm 1':\n        pos = incoming_traffic_container.traffic_lights_coordinates\n    else:\n        print('graph position algorithm not recognized, using default')\n        pos = nx.spring_layout(self.traffic_circuit_diagram)\n    return pos",
        "orig_context": "```python\n## src/traffic_circuit_diagram/traffic_light.py\nclass TrafficLight:\n    def __init__(self,outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\",\"In Progress\",\"Roadblock\",\"Not Started\",\"Abandoned\"]:\n             self.traffic_status = incoming_status\n             viable_status = True\n        else:\n             print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\",\"AND\",\"OR\",\"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/traffic_container.py\nfrom traffic_circuit_diagram.traffic_light import LogicGateTrafficLight\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/networkx_engine.py\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\nfrom traffic_circuit_diagram.traffic_container import TrafficLight, LogicGateTrafficLight\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm = \"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object,LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color = color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\n```\n\n\n",
        "eval_script": "import networkx as nx\nimport matplotlib.pyplot as plt\n\nclass TrafficLight:\n    def __init__(self, outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\", \"In Progress\", \"Roadblock\", \"Not Started\", \"Abandoned\"]:\n            self.traffic_status = incoming_status\n            viable_status = True\n        else:\n            print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\", \"AND\", \"OR\", \"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\nclass TrafficContainer:\n    def __init__(self):\n        # Mock: Replace with actual data as per real use-case\n        self.traffic_lights = {\n            '1': TrafficLight('Light1', 'Complete'),\n            '2': LogicGateTrafficLight('Light2', 'In Progress')\n        }\n        self.roads = {\n            'road1': MockRoad('Light1', 'Light2', True)\n        }\n        self.traffic_lights_coordinates = self.define_mock_coordinates()\n\n    def define_mock_coordinates(self):\n        # Mock implementation\n        return {\n            'Light1': (0, 0),\n            'Light2': (1, 1)\n        }\n\nclass MockRoad:\n    def __init__(self, incoming_light, outgoing_light, was_road_traversed):\n        self.incoming_light = incoming_light\n        self.outgoing_light = outgoing_light\n        self.was_road_traversed = was_road_traversed\n        self.road_name = \"Mock Road\"\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm=\"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object, LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(\n                traffic_light_object.outcome_name,\n                name=traffic_light_object.outcome_name,\n                status=traffic_light_object.traffic_status,\n                color=color,\n                shape=shape\n            )\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(\n                road_object.incoming_light,\n                road_object.outgoing_light,\n                label=road,\n                width=width\n            )\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\ndef test__set_up_graph_position():\n    container = TrafficContainer()\n    engine_default = NetworkXTrafficEngine(graph_position_algorithm=\"default\")\n    engine_custom = NetworkXTrafficEngine(graph_position_algorithm=\"Custom Algorithm 1\")\n    engine_invalid = NetworkXTrafficEngine(graph_position_algorithm=\"Invalid Algorithm\")\n\n    pos_old_default = engine_default._set_up_graph_position(container)\n    pos_new_default = engine_default._set_up_graph_position_new_implementation(container)\n    assert pos_old_default == pos_new_default, \"Default algorithm positions should match\"\n\n    pos_old_custom = engine_custom._set_up_graph_position(container)\n    pos_new_custom = engine_custom._set_up_graph_position_new_implementation(container)\n    assert pos_old_custom == pos_new_custom, \"Custom algorithm positions should match\"\n\n    pos_old_invalid = engine_invalid._set_up_graph_position(container)\n    pos_new_invalid = engine_invalid._set_up_graph_position_new_implementation(container)\n    assert pos_old_invalid == pos_new_invalid, \"Invalid algorithm positions should match\"\n\nif __name__ == \"__main__\":\n    test__set_up_graph_position()"
    },
    {
        "func_name": "NetworkXTrafficEngine.upload_traffic_graph_to_engine",
        "idx": "285",
        "repo_name": "GreenStarMatter___traffic_circuit_diagram",
        "func_path": "src/traffic_circuit_diagram/networkx_engine.py",
        "orig_func": "def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n    for traffic_light in incoming_traffic_container.traffic_lights:\n        traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n        color = self._get_node_color(traffic_light_object.traffic_status)\n        shape = 'o'\n        if isinstance(traffic_light_object, LogicGateTrafficLight):\n            shape = 's'\n        self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color=color, shape=shape)\n    for road in incoming_traffic_container.roads:\n        road_object = incoming_traffic_container.roads[road]\n        width = 1\n        print(road)\n        if road_object.was_road_traversed:\n            width = 2\n        self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n    self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)",
        "orig_context": "```python\n## src/traffic_circuit_diagram/traffic_light.py\nclass TrafficLight:\n    def __init__(self,outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\",\"In Progress\",\"Roadblock\",\"Not Started\",\"Abandoned\"]:\n             self.traffic_status = incoming_status\n             viable_status = True\n        else:\n             print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\",\"AND\",\"OR\",\"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/traffic_container.py\nfrom traffic_circuit_diagram.traffic_light import LogicGateTrafficLight\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/networkx_engine.py\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\nfrom traffic_circuit_diagram.traffic_container import TrafficLight, LogicGateTrafficLight\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm = \"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object,LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color = color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\n```\n\n\n",
        "eval_script": "import networkx as nx\nimport matplotlib.pyplot as plt\n\n\nclass TrafficLight:\n    def __init__(self, outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\", \"In Progress\", \"Roadblock\", \"Not Started\", \"Abandoned\"]:\n            self.traffic_status = incoming_status\n            viable_status = True\n        else:\n            print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\", \"AND\", \"OR\", \"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n\nclass MockTrafficContainer:\n    def __init__(self):\n        self.traffic_lights = {\n            'Light1': TrafficLight('A', 'Complete'),\n            'Light2': LogicGateTrafficLight('B', 'In Progress')\n        }\n\n        self.roads = {\n            'Road1': self.MockRoad('A', 'B', True)\n        }\n\n        self.traffic_lights_coordinates = {\n            'A': (0, 0),\n            'B': (1, 1)\n        }\n\n    class MockRoad:\n        def __init__(self, incoming_light, outgoing_light, was_road_traversed):\n            self.incoming_light = incoming_light\n            self.outgoing_light = outgoing_light\n            self.was_road_traversed = was_road_traversed\n            self.road_name = f\"{incoming_light}->{outgoing_light}\"\n\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm=\"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        self.traffic_circuit_diagram = self._set_up_graph()  # Reset graph\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object, LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name,\n                                                  name=traffic_light_object.outcome_name,\n                                                  status=traffic_light_object.traffic_status,\n                                                  color=color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light,\n                                                  label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\",\n                                font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\n\ndef test_upload_traffic_graph_to_engine():\n    engine1 = NetworkXTrafficEngine()\n    engine2 = NetworkXTrafficEngine()\n    mock_container = MockTrafficContainer()\n\n    engine1.upload_traffic_graph_to_engine(mock_container)\n    engine2.upload_traffic_graph_to_engine_new_implementation(mock_container)\n\n    assert len(engine1.traffic_circuit_diagram.nodes) == len(engine2.traffic_circuit_diagram.nodes), \\\n        \"Node count mismatch\"\n    assert len(engine1.traffic_circuit_diagram.edges) == len(engine2.traffic_circuit_diagram.edges), \\\n        \"Edge count mismatch\"\n    assert engine1.traffic_circuit_diagram.nodes(data=True) == engine2.traffic_circuit_diagram.nodes(data=True), \\\n        \"Node attributes mismatch\"\n    assert list(engine1.traffic_circuit_diagram.edges(data=True)) == list(engine2.traffic_circuit_diagram.edges(data=True)), \\\n        \"Edge attributes mismatch\"\n\n\nif __name__ == \"__main__\":\n    test_upload_traffic_graph_to_engine()"
    },
    {
        "func_name": "NetworkXTrafficEngine._get_node_color",
        "idx": "286",
        "repo_name": "GreenStarMatter___traffic_circuit_diagram",
        "func_path": "src/traffic_circuit_diagram/networkx_engine.py",
        "orig_func": "def _get_node_color(self, incoming_status) -> bool:\n    color = '#FFFFFF'\n    if incoming_status == 'Complete':\n        color = '#90EE90'\n    elif incoming_status == 'In Progress':\n        color = '#ADD8E6'\n    elif incoming_status == 'Roadblock':\n        color = '#FFFF00'\n    elif incoming_status == 'Not Started':\n        color = '#FFFFFF'\n    elif incoming_status == 'Abandoned':\n        color = '#FF0000'\n    else:\n        print('Color not recognized')\n    return color",
        "orig_context": "```python\n## src/traffic_circuit_diagram/traffic_light.py\nclass TrafficLight:\n    def __init__(self,outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\",\"In Progress\",\"Roadblock\",\"Not Started\",\"Abandoned\"]:\n             self.traffic_status = incoming_status\n             viable_status = True\n        else:\n             print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\",\"AND\",\"OR\",\"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/traffic_container.py\nfrom traffic_circuit_diagram.traffic_light import LogicGateTrafficLight\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/networkx_engine.py\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\nfrom traffic_circuit_diagram.traffic_container import TrafficLight, LogicGateTrafficLight\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm = \"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object,LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color = color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n\n        return color\n\n```\n\n\n",
        "eval_script": "import networkx as nx\nimport matplotlib.pyplot as plt\n\nclass TrafficLight:\n    def __init__(self, outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\", \"In Progress\", \"Roadblock\", \"Not Started\", \"Abandoned\"]:\n            self.traffic_status = incoming_status\n            viable_status = True\n        else:\n            print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\", \"AND\", \"OR\", \"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n# Mock Traffic Container\nclass MockTrafficContainer:\n    def __init__(self):\n        self.traffic_lights = {\n            'Light1': TrafficLight('Light1', 'Complete'),\n            'Light2': TrafficLight('Light2', 'In Progress')\n        }\n        self.roads = {\n            'Road1': MockRoad('Road1', 'Light1', 'Light2', True)\n        }\n        self.traffic_lights_coordinates = {\n            'Light1': (0, 0),\n            'Light2': (1, 1)\n        }\n\nclass MockRoad:\n    def __init__(self, road_name, incoming_light, outgoing_light, was_road_traversed):\n        self.road_name = road_name\n        self.incoming_light = incoming_light\n        self.outgoing_light = outgoing_light\n        self.was_road_traversed = was_road_traversed\n\nclass NetworkXTrafficEngine:\n    def __init__(self, graph_position_algorithm=\"default\"):\n        self.graph_position_algorithm = graph_position_algorithm\n        self.traffic_circuit_diagram = self._set_up_graph()\n        self.traffic_circuit_diagram_position = None\n\n    def _set_up_graph(self):\n        G = nx.Graph()\n        return G\n\n    def _set_up_graph_position(self, incoming_traffic_container):\n        if self.graph_position_algorithm == \"default\":\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        elif self.graph_position_algorithm == \"Custom Algorithm 1\":\n            pos = incoming_traffic_container.traffic_lights_coordinates\n        else:\n            print(\"graph position algorithm not recognized, using default\")\n            pos = nx.spring_layout(self.traffic_circuit_diagram)\n        return pos\n\n    def upload_traffic_graph_to_engine(self, incoming_traffic_container):\n        for traffic_light in incoming_traffic_container.traffic_lights:\n            traffic_light_object = incoming_traffic_container.traffic_lights[traffic_light]\n            color = self._get_node_color(traffic_light_object.traffic_status)\n            shape = \"o\"\n            if isinstance(traffic_light_object, LogicGateTrafficLight):\n                shape = \"s\"\n            self.traffic_circuit_diagram.add_node(traffic_light_object.outcome_name, name=traffic_light_object.outcome_name, status=traffic_light_object.traffic_status, color=color, shape=shape)\n        for road in incoming_traffic_container.roads:\n            road_object = incoming_traffic_container.roads[road]\n            width = 1\n            print(road)\n            if road_object.was_road_traversed:\n                width = 2\n            self.traffic_circuit_diagram.add_edge(road_object.incoming_light, road_object.outgoing_light, label=road, width=width)\n        self.traffic_circuit_diagram_position = self._set_up_graph_position(incoming_traffic_container)\n\n    def show_network_graph(self):\n        for node, attributes in self.traffic_circuit_diagram.nodes(data=True):\n            nx.draw_networkx_nodes(\n                self.traffic_circuit_diagram, self.traffic_circuit_diagram_position,\n                nodelist=[node],\n                node_color=attributes[\"color\"],\n                node_shape=attributes[\"shape\"],\n                edgecolors=\"black\",\n                node_size=3000\n            )\n        nx.draw_networkx_edges(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position)\n        nx.draw_networkx_labels(self.traffic_circuit_diagram, self.traffic_circuit_diagram_position, font_weight=\"bold\", font_size=8)\n        plt.show()\n\n    def _get_node_color(self, incoming_status) -> bool:\n        color = \"#FFFFFF\"\n        if incoming_status == \"Complete\":\n            color = \"#90EE90\"\n        elif incoming_status == \"In Progress\":\n            color = \"#ADD8E6\"\n        elif incoming_status == \"Roadblock\":\n            color = \"#FFFF00\"\n        elif incoming_status == \"Not Started\":\n            color = \"#FFFFFF\"\n        elif incoming_status == \"Abandoned\":\n            color = \"#FF0000\"\n        else:\n            print(\"Color not recognized\")\n        return color\n    \n\n\ndef test__get_node_color():\n    engine = NetworkXTrafficEngine()\n    assert engine._get_node_color(\"Complete\") == engine._get_node_color_new_implementation(\"Complete\"), \"Mismatch for 'Complete'\"\n    assert engine._get_node_color(\"In Progress\") == engine._get_node_color_new_implementation(\"In Progress\"), \"Mismatch for 'In Progress'\"\n    assert engine._get_node_color(\"Roadblock\") == engine._get_node_color_new_implementation(\"Roadblock\"), \"Mismatch for 'Roadblock'\"\n\nif __name__ == \"__main__\":\n    test__get_node_color()"
    },
    {
        "func_name": "TrafficContainer._calculate_y_coordinate_from_overlaps",
        "idx": "287",
        "repo_name": "GreenStarMatter___traffic_circuit_diagram",
        "func_path": "src/traffic_circuit_diagram/traffic_container.py",
        "orig_func": "def _calculate_y_coordinate_from_overlaps(self):\n    x_position_dict = {}\n    for traffic_light in self.traffic_lights_coordinates:\n        if self.traffic_lights_coordinates[traffic_light][0] not in x_position_dict:\n            x_position_dict[self.traffic_lights_coordinates[traffic_light][0]] = []\n        x_position_dict[self.traffic_lights_coordinates[traffic_light][0]].append(traffic_light)\n    for x_position in x_position_dict:\n        height_counter = 0\n        for traffic_light in x_position_dict[x_position]:\n            self.traffic_lights_coordinates[traffic_light][1] = height_counter\n            height_counter += 1",
        "orig_context": "```python\n## src/traffic_circuit_diagram/traffic_light.py\nclass TrafficLight:\n    def __init__(self,outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\",\"In Progress\",\"Roadblock\",\"Not Started\",\"Abandoned\"]:\n             self.traffic_status = incoming_status\n             viable_status = True\n        else:\n             print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\",\"AND\",\"OR\",\"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/road.py\nclass Road:\n    def __init__(self,road_name: str, was_road_traversed: bool):\n        self.road_name = road_name\n        self.was_road_traversed = was_road_traversed\n        self.incoming_light = None\n        self.outgoing_light = None\n\n    def set_road_name(self, incoming_name: str) -> bool:\n        self.road_name = incoming_name\n        return True\n\n    def set_road_traversel_status(self, incoming_traversal_status: bool) -> bool:\n        self.was_road_traversed = incoming_traversal_status\n        return True\n\n    def get_if_road_was_traversed(self) -> bool:\n        return self.was_road_traversed\n\n    def set_incoming_traffic_light(self, incoming_traffic_light) -> bool:\n        self.incoming_light = incoming_traffic_light\n        return True\n\n    def set_outgoing_traffic_light(self, outgoing_traffic_light) -> bool:\n        self.outgoing_light = outgoing_traffic_light\n        return True\n\n    def _set_line_thickness(self) -> bool:\n        return True\n\n```\n\n\n```python\n## src/traffic_circuit_diagram/traffic_container.py\nfrom traffic_circuit_diagram.traffic_light import TrafficLight\n\nfrom traffic_circuit_diagram.traffic_light import LogicGateTrafficLight\n\nfrom traffic_circuit_diagram.road import Road\n\nclass TrafficContainer:\n    def __init__(self):\n        self.traffic_lights = {}\n        self.roads = {}\n        self.connection_map = {}\n        self.traffic_lights_coordinates = {}\n        self.root_traffic_light = None\n\n    def add_traffic_light(self, incoming_traffic_light_command):\n        if len(incoming_traffic_light_command) == 3:\n            new_traffic_light = TrafficLight(incoming_traffic_light_command[1], incoming_traffic_light_command[2])\n        elif len(incoming_traffic_light_command) == 4:\n            new_traffic_light = LogicGateTrafficLight(incoming_traffic_light_command[1], incoming_traffic_light_command[2])\n            new_traffic_light.set_gate_type(incoming_traffic_light_command[3])\n        self.traffic_lights[new_traffic_light.outcome_name] = new_traffic_light\n        if new_traffic_light.outcome_name == \"Project Start\":\n            self.root_traffic_light = new_traffic_light.outcome_name\n        return True\n\n    def add_road(self, incoming_road_command):\n        new_road = Road(incoming_road_command[1], incoming_road_command[2])\n        new_road.set_incoming_traffic_light(incoming_road_command[3])\n        new_road.set_outgoing_traffic_light(incoming_road_command[4])\n        self.roads[new_road.road_name] = new_road\n        return True\n\n    def connect_roads_to_traffic_lights(self):\n        for road in self.roads:\n            road_object = self.roads[road]\n            if road_object.incoming_light not in self.traffic_lights.keys():\n                print(road_object.incoming_light+ \" not in traffic_lights, added automatically\")\n                new_traffic_light = TrafficLight(road_object.incoming_light, \"Not Started\")\n                self.traffic_lights[new_traffic_light.outcome_name] = new_traffic_light\n            if road_object.outgoing_light not in self.traffic_lights.keys():\n                print(road_object.outgoing_light + \" not in traffic_lights, added automatically\")\n                new_traffic_light = TrafficLight(road_object.outgoing_light, \"Not Started\")\n                self.traffic_lights[road_object.outgoing_light] = new_traffic_light\n            self.traffic_lights[road_object.incoming_light].outgoing_roads[road_object.road_name] = road_object\n            self.traffic_lights[road_object.outgoing_light].incoming_roads[road_object.road_name] = road_object\n            self.connection_map[road] = [road_object.incoming_light, road_object.outgoing_light]\n        #print(self.connection_map)\n        self._calculate_discrete_cartesian_coordinates_of_traffic_lights()\n        #print(self.traffic_lights_coordinates)\n        return True\n\n    def _calculate_discrete_cartesian_coordinates_of_traffic_lights(self):\n        in_process_nodes = [self.root_traffic_light]\n        while len(in_process_nodes)> 0:\n            current_traffic_light = in_process_nodes.pop(0)\n            current_traffic_light_object = self.traffic_lights[current_traffic_light]\n            for road in current_traffic_light_object.outgoing_roads:\n                outgoing_road_object = self.roads[road]\n                outgoing_traffic_light = self.roads[road].outgoing_light\n                outgoing_node = self.roads[road]\n                #This is wrong, this pulls in a road object, but needs to pull in the string associated with it instead\n                if (outgoing_traffic_light not in self.traffic_lights_coordinates) and (outgoing_traffic_light not in in_process_nodes):\n                    in_process_nodes.append(outgoing_traffic_light)\n            all_incoming_lights_accounted_for = True\n            incoming_max_x = -1\n            incoming_max_y = -1\n            for road in self.traffic_lights[current_traffic_light].incoming_roads:\n                if self.roads[road].incoming_light not in self.traffic_lights_coordinates:\n                    if self.roads[road].incoming_light not in in_process_nodes:\n                        in_process_nodes.insert(0, self.roads[road].incoming_light)\n                    all_incoming_lights_accounted_for = False\n                    break\n                incoming_max_x = max(incoming_max_x, self.traffic_lights_coordinates[self.roads[road].incoming_light][0])\n                incoming_max_y = max(incoming_max_y, self.traffic_lights_coordinates[self.roads[road].incoming_light][1])\n            if all_incoming_lights_accounted_for:\n                self.traffic_lights_coordinates[current_traffic_light] = [incoming_max_x + 1, incoming_max_y + 1]\n            else:\n                in_process_nodes.append(current_traffic_light)\n        self._calculate_y_coordinate_from_overlaps()\n        self._handle_for_overlapping_edges()\n\n    def _calculate_y_coordinate_from_overlaps(self):\n        x_position_dict = {}\n        for traffic_light in self.traffic_lights_coordinates:\n            if self.traffic_lights_coordinates[traffic_light][0] not in x_position_dict:\n                x_position_dict[self.traffic_lights_coordinates[traffic_light][0]] = []\n            x_position_dict[self.traffic_lights_coordinates[traffic_light][0]].append(traffic_light)\n\n        for x_position in x_position_dict:\n            height_counter = 0\n            for traffic_light in x_position_dict[x_position]:\n                self.traffic_lights_coordinates[traffic_light][1] = height_counter\n                height_counter += 1\n\n    def _handle_for_overlapping_edges(self):\n        road_cartesian_info = {}\n        for road in self.roads:\n            starting_coordinates = self.traffic_lights_coordinates[self.roads[road].incoming_light]\n            ending_coordinates = self.traffic_lights_coordinates[self.roads[road].outgoing_light]\n            x_start = starting_coordinates[0]\n            x_end = ending_coordinates[0]\n            slope = self._calc_slope_from_coordinates(x_start, x_end, starting_coordinates[1], ending_coordinates[1])\n            road_cartesian_info[road] = [x_start, x_end, slope]\n        \n        list_of_roads_to_verify = list(road_cartesian_info.keys())\n        searched_index = 1\n        max_y = 0\n        for road in list_of_roads_to_verify:\n            max_y = max(max_y, self.traffic_lights_coordinates[self.roads[road].outgoing_light][1], self.traffic_lights_coordinates[self.roads[road].incoming_light][1])\n        for road in list_of_roads_to_verify:\n            leftmost_road_interval = [self.traffic_lights_coordinates[self.roads[road].incoming_light][0], self.traffic_lights_coordinates[self.roads[road].outgoing_light][0]]\n            for further_road in list_of_roads_to_verify[searched_index:]:\n                comparing_road_interval = [self.traffic_lights_coordinates[self.roads[further_road].incoming_light][0], self.traffic_lights_coordinates[self.roads[further_road].outgoing_light][0]]\n                intersection_detected = self._calc_coordinates_intersection(leftmost_road_interval, comparing_road_interval)\n                if (road_cartesian_info[road][2] == road_cartesian_info[further_road][2]) and intersection_detected:\n                    max_y += 1\n                    self.traffic_lights_coordinates[self.roads[further_road].outgoing_light][1] = max_y\n                    further_road_starting_coordinates = self.traffic_lights_coordinates[self.roads[further_road].incoming_light]\n                    further_road_ending_coordinates = self.traffic_lights_coordinates[self.roads[further_road].outgoing_light]\n                    new_slope = self._calc_slope_from_coordinates(further_road_starting_coordinates[0], further_road_ending_coordinates[0], further_road_starting_coordinates[1], max_y)\n                    road_cartesian_info[further_road][2] = new_slope\n            searched_index += 1\n\n    def _calc_slope_from_coordinates(self, x_start, x_end, y_start, y_end):\n        return (y_end - y_start)/(x_end - x_start)\n\n    def _calc_coordinates_intersection(self, interval_1, interval_2):\n        return ((interval_1[0] <= interval_2[1]) and (interval_2[0] <= interval_1[1]))\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\n# src/traffic_circuit_diagram/traffic_container.py\nclass TrafficLight:\n    def __init__(self, outcome_name: str, traffic_status: str):\n        self.outcome_name = outcome_name\n        self.traffic_status = traffic_status\n        self.incoming_roads = {}\n        self.outgoing_roads = {}\n\n    def set_outcome_name(self, incoming_name: str) -> bool:\n        self.outcome_name = incoming_name\n        return True\n\n    def set_traffic_status(self, incoming_status: str) -> bool:\n        viable_status = False\n        if incoming_status in [\"Complete\", \"In Progress\", \"Roadblock\", \"Not Started\", \"Abandoned\"]:\n            self.traffic_status = incoming_status\n            viable_status = True\n        else:\n            print(\"Nonviable Status, Retaining Old Status\")\n        return viable_status\n\n    def get_traffic_status(self) -> str:\n        return self.traffic_status\n\n    def set_incoming_road(self, incoming_road) -> bool:\n        self.incoming_roads[incoming_road] = incoming_road.road_name\n        return True\n\n    def set_outgoing_road(self, outgoing_road) -> bool:\n        self.outgoing_roads[outgoing_road] = outgoing_road.road_name\n        return True\n\n    def _set_light_color(self) -> bool:\n        return True\n\nclass LogicGateTrafficLight(TrafficLight):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gate_type = \"MULTI\"\n\n    def set_gate_type(self, incoming_gate_type: str) -> bool:\n        viable_gate = False\n        if incoming_gate_type in [\"MULTI\", \"AND\", \"OR\", \"XOR\"]:\n            self.gate_type = incoming_gate_type\n            viable_gate = True\n        return viable_gate\n\n    def get_gate_type(self) -> str:\n        return self.gate_type\n\n    def _set_gate_shape(self) -> bool:\n        return True\n\nclass Road:\n    def __init__(self, road_name: str, was_road_traversed: bool):\n        self.road_name = road_name\n        self.was_road_traversed = was_road_traversed\n        self.incoming_light = None\n        self.outgoing_light = None\n\n    def set_road_name(self, incoming_name: str) -> bool:\n        self.road_name = incoming_name\n        return True\n\n    def set_road_traversel_status(self, incoming_traversal_status: bool) -> bool:\n        self.was_road_traversed = incoming_traversal_status\n        return True\n\n    def get_if_road_was_traversed(self) -> bool:\n        return self.was_road_traversed\n\n    def set_incoming_traffic_light(self, incoming_traffic_light) -> bool:\n        self.incoming_light = incoming_traffic_light\n        return True\n\n    def set_outgoing_traffic_light(self, outgoing_traffic_light) -> bool:\n        self.outgoing_light = outgoing_traffic_light\n        return True\n\n    def _set_line_thickness(self) -> bool:\n        return True\n\nclass TrafficContainer:\n    def __init__(self):\n        self.traffic_lights = {}\n        self.roads = {}\n        self.connection_map = {}\n        self.traffic_lights_coordinates = {}\n        self.root_traffic_light = None\n\n    def add_traffic_light(self, incoming_traffic_light_command):\n        if len(incoming_traffic_light_command) == 3:\n            new_traffic_light = TrafficLight(incoming_traffic_light_command[1], incoming_traffic_light_command[2])\n        elif len(incoming_traffic_light_command) == 4:\n            new_traffic_light = LogicGateTrafficLight(incoming_traffic_light_command[1], incoming_traffic_light_command[2])\n            new_traffic_light.set_gate_type(incoming_traffic_light_command[3])\n        self.traffic_lights[new_traffic_light.outcome_name] = new_traffic_light\n        if new_traffic_light.outcome_name == \"Project Start\":\n            self.root_traffic_light = new_traffic_light.outcome_name\n        return True\n\n    def add_road(self, incoming_road_command):\n        new_road = Road(incoming_road_command[1], incoming_road_command[2])\n        new_road.set_incoming_traffic_light(incoming_road_command[3])\n        new_road.set_outgoing_traffic_light(incoming_road_command[4])\n        self.roads[new_road.road_name] = new_road\n        return True\n\n    def connect_roads_to_traffic_lights(self):\n        for road in self.roads:\n            road_object = self.roads[road]\n            if road_object.incoming_light not in self.traffic_lights.keys():\n                print(road_object.incoming_light + \" not in traffic_lights, added automatically\")\n                new_traffic_light = TrafficLight(road_object.incoming_light, \"Not Started\")\n                self.traffic_lights[new_traffic_light.outcome_name] = new_traffic_light\n            if road_object.outgoing_light not in self.traffic_lights.keys():\n                print(road_object.outgoing_light + \" not in traffic_lights, added automatically\")\n                new_traffic_light = TrafficLight(road_object.outgoing_light, \"Not Started\")\n                self.traffic_lights[road_object.outgoing_light] = new_traffic_light\n            self.traffic_lights[road_object.incoming_light].outgoing_roads[road_object.road_name] = road_object\n            self.traffic_lights[road_object.outgoing_light].incoming_roads[road_object.road_name] = road_object\n            self.connection_map[road] = [road_object.incoming_light, road_object.outgoing_light]\n        # print(self.connection_map)\n        self._calculate_discrete_cartesian_coordinates_of_traffic_lights()\n        # print(self.traffic_lights_coordinates)\n        return True\n\n    def _calculate_discrete_cartesian_coordinates_of_traffic_lights(self):\n        in_process_nodes = [self.root_traffic_light]\n        while len(in_process_nodes) > 0:\n            current_traffic_light = in_process_nodes.pop(0)\n            current_traffic_light_object = self.traffic_lights[current_traffic_light]\n            for road in current_traffic_light_object.outgoing_roads:\n                outgoing_road_object = self.roads[road]\n                outgoing_traffic_light = self.roads[road].outgoing_light\n                outgoing_node = self.roads[road]\n                # This is wrong, this pulls in a road object, but needs to pull in the string associated with it\n                if (outgoing_traffic_light not in self.traffic_lights_coordinates) and (\n                        outgoing_traffic_light not in in_process_nodes):\n                    in_process_nodes.append(outgoing_traffic_light)\n            all_incoming_lights_accounted_for = True\n            incoming_max_x = -1\n            incoming_max_y = -1\n            for road in self.traffic_lights[current_traffic_light].incoming_roads:\n                if self.roads[road].incoming_light not in self.traffic_lights_coordinates:\n                    if self.roads[road].incoming_light not in in_process_nodes:\n                        in_process_nodes.insert(0, self.roads[road].incoming_light)\n                    all_incoming_lights_accounted_for = False\n                    break\n                incoming_max_x = max(incoming_max_x,\n                                     self.traffic_lights_coordinates[self.roads[road].incoming_light][0])\n                incoming_max_y = max(incoming_max_y,\n                                     self.traffic_lights_coordinates[self.roads[road].incoming_light][1])\n            if all_incoming_lights_accounted_for:\n                self.traffic_lights_coordinates[current_traffic_light] = [incoming_max_x + 1, incoming_max_y + 1]\n            else:\n                in_process_nodes.append(current_traffic_light)\n        self._calculate_y_coordinate_from_overlaps()\n        self._handle_for_overlapping_edges()\n\n    def _calculate_y_coordinate_from_overlaps(self):\n        x_position_dict = {}\n        for traffic_light in self.traffic_lights_coordinates:\n            if self.traffic_lights_coordinates[traffic_light][0] not in x_position_dict:\n                x_position_dict[self.traffic_lights_coordinates[traffic_light][0]] = []\n            x_position_dict[self.traffic_lights_coordinates[traffic_light][0]].append(traffic_light)\n\n        for x_position in x_position_dict:\n            height_counter = 0\n            for traffic_light in x_position_dict[x_position]:\n                self.traffic_lights_coordinates[traffic_light][1] = height_counter\n                height_counter += 1\n\n\n    def _handle_for_overlapping_edges(self):\n        road_cartesian_info = {}\n        for road in self.roads:\n            starting_coordinates = self.traffic_lights_coordinates[self.roads[road].incoming_light]\n            ending_coordinates = self.traffic_lights_coordinates[self.roads[road].outgoing_light]\n            x_start = starting_coordinates[0]\n            x_end = ending_coordinates[0]\n            slope = self._calc_slope_from_coordinates(x_start, x_end, starting_coordinates[1], ending_coordinates[1])\n            road_cartesian_info[road] = [x_start, x_end, slope]\n\n        list_of_roads_to_verify = list(road_cartesian_info.keys())\n        searched_index = 1\n        max_y = 0\n        for road in list_of_roads_to_verify:\n            max_y = max(max_y, self.traffic_lights_coordinates[self.roads[road].outgoing_light][1],\n                        self.traffic_lights_coordinates[self.roads[road].incoming_light][1])\n        for road in list_of_roads_to_verify:\n            leftmost_road_interval = [self.traffic_lights_coordinates[self.roads[road].incoming_light][0],\n                                      self.traffic_lights_coordinates[self.roads[road].outgoing_light][0]]\n            for further_road in list_of_roads_to_verify[searched_index:]:\n                comparing_road_interval = [self.traffic_lights_coordinates[self.roads[further_road].incoming_light][0],\n                                           self.traffic_lights_coordinates[self.roads[further_road].outgoing_light][0]]\n                intersection_detected = self._calc_coordinates_intersection(leftmost_road_interval,\n                                                                            comparing_road_interval)\n                if (road_cartesian_info[road][2] == road_cartesian_info[further_road][2]) and intersection_detected:\n                    max_y += 1\n                    self.traffic_lights_coordinates[self.roads[further_road].outgoing_light][1] = max_y\n                    further_road_starting_coordinates = self.traffic_lights_coordinates[\n                        self.roads[further_road].incoming_light]\n                    further_road_ending_coordinates = self.traffic_lights_coordinates[\n                        self.roads[further_road].outgoing_light]\n                    new_slope = self._calc_slope_from_coordinates(further_road_starting_coordinates[0],\n                                                                  further_road_ending_coordinates[0],\n                                                                  further_road_starting_coordinates[1], max_y)\n                    road_cartesian_info[further_road][2] = new_slope\n            searched_index += 1\n\n    def _calc_slope_from_coordinates(self, x_start, x_end, y_start, y_end):\n        return (y_end - y_start) / (x_end - x_start)\n\n    def _calc_coordinates_intersection(self, interval_1, interval_2):\n        return (interval_1[0] <= interval_2[1]) and (interval_2[0] <= interval_1[1])\n\ndef test__calculate_y_coordinate_from_overlaps():\n    # Setup a test traffic container\n    tc1 = TrafficContainer()\n    tc1.add_traffic_light([\"ADD LIGHT\", \"t1\", \"Not Started\"])\n    tc1.add_traffic_light([\"ADD LIGHT\", \"t2\", \"Not Started\"])\n    tc1.add_traffic_light([\"ADD LIGHT\", \"t3\", \"Not Started\"])\n    tc1.root_traffic_light = \"t1\"  # Explicitly set a root traffic light to fix KeyError\n    tc1.add_road([\"ADD ROAD\", \"r1\", False, \"t1\", \"t2\"])\n    tc1.add_road([\"ADD ROAD\", \"r2\", False, \"t2\", \"t3\"])\n    tc1.connect_roads_to_traffic_lights()\n\n    # First Calculation with the original method\n    tc1._calculate_y_coordinate_from_overlaps()\n    original_coordinates = dict(tc1.traffic_lights_coordinates)\n    \n    # Setup a second identical traffic container for the new implementation\n    tc2 = TrafficContainer()\n    tc2.add_traffic_light([\"ADD LIGHT\", \"t1\", \"Not Started\"])\n    tc2.add_traffic_light([\"ADD LIGHT\", \"t2\", \"Not Started\"])\n    tc2.add_traffic_light([\"ADD LIGHT\", \"t3\", \"Not Started\"])\n    tc2.root_traffic_light = \"t1\"  # Explicitly set a root traffic light to fix KeyError\n    tc2.add_road([\"ADD ROAD\", \"r1\", False, \"t1\", \"t2\"])\n    tc2.add_road([\"ADD ROAD\", \"r2\", False, \"t2\", \"t3\"])\n    tc2.connect_roads_to_traffic_lights()\n\n    # Second Calculation with the new implementation\n    tc2._calculate_y_coordinate_from_overlaps_new_implementation()\n    new_impl_coordinates = dict(tc2.traffic_lights_coordinates)\n    \n    # Assert to ensure functionality is identical\n    assert original_coordinates == new_impl_coordinates, \"New implementation gives different results\"\n    assert tc1.traffic_lights_coordinates == tc2.traffic_lights_coordinates, \"Coordinate results should be identical between implementations\"\n    assert len(original_coordinates) == len(new_impl_coordinates), \"Both implementations should produce coordinates for all lights\"\n\nif __name__ == \"__main__\":\n    test__calculate_y_coordinate_from_overlaps()"
    },
    {
        "func_name": "UserPostListView.get_queryset",
        "idx": "288",
        "repo_name": "vsanjay008___my-blog-website",
        "func_path": "blog/views.py",
        "orig_func": "def get_queryset(self):\n    user = get_object_or_404(User, username=self.kwargs.get('username'))\n    return Post.objects.filter(author=user).order_by('-date_posted')",
        "orig_context": "```python\n## blog/models.py\nfrom django.db import models\n\nfrom django.utils import timezone\n\nfrom django.contrib.auth.models import User\n\nfrom django.urls import reverse\n\nclass Post(models.Model):\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n    date_posted = models.DateTimeField(default=timezone.now)\n    author = models.ForeignKey(User, on_delete=models.CASCADE)\n\n    def __str__(self):\n        return self.title\n\n    def get_absolute_url(self):\n        return reverse('post-detail', kwargs={'pk': self.pk})\n\n```\n\n\n```python\n## blog/views.py\nfrom django.shortcuts import render, get_object_or_404\n\nfrom django.contrib.auth.models import User\n\nfrom django.views.generic import (\n    ListView,\n    DetailView,\n    CreateView,\n    UpdateView,\n    DeleteView\n)\n\nfrom .models import Post\n\nclass UserPostListView(ListView):\n    model = Post\n    template_name = 'blog/user_posts.html'  # <app>/<model>_<viewtype>.html\n    context_object_name = 'posts'\n    paginate_by = 2\n\n    def get_queryset(self):\n        user = get_object_or_404(User, username=self.kwargs.get('username'))\n        return Post.objects.filter(author=user).order_by('-date_posted')\n\n```\n\n\n",
        "eval_script": "# Your Code. \nfrom datetime import datetime\nfrom django.utils import timezone\n\n# Mock User class\nclass User:\n    def __init__(self, id, username):\n        self.id = id\n        self.username = username\n\n# Mock in-memory \"database\" for users and posts\nuser_db = {\n    'johndoe': User(1, 'johndoe'),\n}\n\nclass Post:\n    def __init__(self, title, content, date_posted, author):\n        self.title = title\n        self.content = content\n        self.date_posted = date_posted\n        self.author = author\n\n    def __str__(self):\n        return self.title\n\n    @staticmethod\n    def objects_filter(author):\n        # Assuming we have a list of Post instances for demonstration\n        return [\n            Post('Post 1', 'Content 1', datetime(2023, 10, 10, 10, 10, 10), author),\n            Post('Post 2', 'Content 2', datetime(2023, 10, 12, 11, 11, 11), author),\n        ]\n\n    @staticmethod\n    def objects_order_by(posts, attr):\n        return sorted(posts, key=lambda post: getattr(post, attr), reverse=True)\n\nclass UserPostListView:\n    context_object_name = 'posts'\n    paginate_by = 2\n\n    @staticmethod\n    def get_object_or_404(User, username):\n        if username in user_db:\n            return user_db[username]\n        raise ValueError(\"User not found\")\n\n    def get_queryset(self, username):\n        user = self.get_object_or_404(User, username)\n        posts = Post.objects_filter(author=user)\n        ordered_posts = Post.objects_order_by(posts, 'date_posted')\n        return ordered_posts\n\n    # New implementation for the sake of comparison\n\n\ndef test_get_queryset():\n    view = UserPostListView()\n\n    # Compare results from both implementations\n    original_posts = view.get_queryset('johndoe')\n    new_posts = view.get_queryset_new_implementation('johndoe')\n\n    # Assert that the number of posts is equal\n    assert len(original_posts) == len(new_posts), \"Number of posts mismatch\"\n\n    # Assert that the titles of the posts are equal\n    for orig, new in zip(original_posts, new_posts):\n        assert orig.title == new.title, f\"Title mismatch: {orig.title} != {new.title}\"\n    \n    # Assert that the date posted are equal\n    for orig, new in zip(original_posts, new_posts):\n        assert orig.date_posted == new.date_posted, f\"Date posted mismatch: {orig.date_posted} != {new.date_posted}\"\n\nif __name__ == \"__main__\":\n    test_get_queryset()"
    },
    {
        "func_name": "PostCreateView.form_valid",
        "idx": "289",
        "repo_name": "vsanjay008___my-blog-website",
        "func_path": "blog/views.py",
        "orig_func": "def form_valid(self, form):\n    form.instance.author = self.request.user\n    return super().form_valid(form)",
        "orig_context": "```python\n## blog/models.py\nfrom django.db import models\n\nfrom django.utils import timezone\n\nfrom django.contrib.auth.models import User\n\nfrom django.urls import reverse\n\nclass Post(models.Model):\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n    date_posted = models.DateTimeField(default=timezone.now)\n    author = models.ForeignKey(User, on_delete=models.CASCADE)\n\n    def __str__(self):\n        return self.title\n\n    def get_absolute_url(self):\n        return reverse('post-detail', kwargs={'pk': self.pk})\n\n```\n\n\n```python\n## blog/views.py\nfrom django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\n\nfrom django.views.generic import (\n    ListView,\n    DetailView,\n    CreateView,\n    UpdateView,\n    DeleteView\n)\n\nfrom .models import Post\n\nclass PostCreateView(LoginRequiredMixin, CreateView):\n    model = Post\n    fields = ['title', 'content']\n\n    def form_valid(self, form):\n        form.instance.author = self.request.user\n        return super().form_valid(form)\n\n```\n\n\n",
        "eval_script": "# blog/models.py\nfrom django.urls import reverse\n\nclass Post:\n    def __init__(self, title, content, author):\n        self.title = title\n        self.content = content\n        self.author = author\n        self.pk = 1  # mocking primary key\n\n    def get_absolute_url(self):\n        return reverse('post-detail', kwargs={'pk': self.pk})\n\n    def __str__(self):\n        return self.title\n\n# blog/views.py\nfrom django import forms\nfrom django.views.generic.edit import FormView\nfrom django.forms import BaseForm\nimport copy\n\n# Mock User class\nclass User:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\nmock_user = User(username='mockuser', password='mockpassword')\n\n# Mock request object, not inheriting from HttpRequest\nclass MockRequest:\n    user = mock_user\n\n# Mock form for our Post class\nclass PostForm(BaseForm):\n    data = None\n    instance = None\n\n    def __init__(self, data):\n        self.data = data\n        self.instance = Post(\n            title=data.get('title'),\n            content=data.get('content'),\n            author=None\n        )\n\n# Including form_valid based operation without complete Django environment\nclass PostCreateView:\n    form_class = PostForm\n\n    def form_valid(self, form):\n        form.instance.author = self.request.user  # Mock user assignment\n        print(f\"Form is valid. Author: {form.instance.author.username}, Title: {form.instance.title}, Content: {form.instance.content}\")\n        return form\n\n\ndef test_form_valid():\n    form_initial_data = {'title': 'Test Title', 'content': 'Test Content'}\n    \n    request = MockRequest()\n    form_original = PostForm(data=form_initial_data)\n    form_new_impl = PostForm(data=form_initial_data)\n    \n    view = PostCreateView()\n    view.request = request\n\n    form_original_valid = view.form_valid(copy.deepcopy(form_original))\n    form_new_impl_valid = view.form_valid_new_implementation(copy.deepcopy(form_new_impl))\n\n    assert form_original_valid.instance.author == form_new_impl_valid.instance.author, \"Authors do not match\"\n    assert form_original_valid.instance.title == form_new_impl_valid.instance.title, \"Titles do not match\"\n    assert form_original_valid.instance.content == form_new_impl_valid.instance.content, \"Contents do not match\"\n\nif __name__ == \"__main__\":\n    test_form_valid()"
    },
    {
        "func_name": "PostUpdateView.form_valid",
        "idx": "290",
        "repo_name": "vsanjay008___my-blog-website",
        "func_path": "blog/views.py",
        "orig_func": "def form_valid(self, form):\n    form.instance.author = self.request.user\n    return super().form_valid(form)",
        "orig_context": "```python\n## blog/models.py\nfrom django.db import models\n\nfrom django.utils import timezone\n\nfrom django.contrib.auth.models import User\n\nfrom django.urls import reverse\n\nclass Post(models.Model):\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n    date_posted = models.DateTimeField(default=timezone.now)\n    author = models.ForeignKey(User, on_delete=models.CASCADE)\n\n    def __str__(self):\n        return self.title\n\n    def get_absolute_url(self):\n        return reverse('post-detail', kwargs={'pk': self.pk})\n\n```\n\n\n```python\n## blog/views.py\nfrom django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\n\nfrom django.views.generic import (\n    ListView,\n    DetailView,\n    CreateView,\n    UpdateView,\n    DeleteView\n)\n\nfrom .models import Post\n\nclass PostUpdateView(LoginRequiredMixin, UserPassesTestMixin, UpdateView):\n    model = Post\n    fields = ['title', 'content']\n\n    def form_valid(self, form):\n        form.instance.author = self.request.user\n        return super().form_valid(form)\n\n    def test_func(self):\n        post = self.get_object()\n        if self.request.user == post.author:\n            return True\n        return False\n\n```\n\n\n",
        "eval_script": "# Mock Django setup for standalone execution\n\n# Mocking the necessary Django components\nclass MockUser:\n    def __init__(self, username):\n        self.username = username\n\nclass MockRequest:\n    def __init__(self, user):\n        self.user = user\n\nclass MockForm:\n    def __init__(self):\n        self.instance = None\n\nclass BaseView:\n    def form_valid(self, form):\n        return f\"Form is valid. Instance author: {form.instance.author.username}\"\n\nclass MockLoginRequiredMixin:\n    pass\n\nclass MockUserPassesTestMixin:\n    pass\n\n# Mocking Django Models\nclass MockModel:\n    pass\n\n# Mocking Django's timezone\ndef mock_timezone_now():\n    import datetime\n    return datetime.datetime.now()\n\n# Mocking Django's reverse function\ndef mock_reverse(viewname, kwargs=None):\n    return f\"/mock-url/{kwargs['pk']}\"\n\n# Mock Post model\nclass Post(MockModel):\n    def __init__(self, title, content, author):\n        self.title = title\n        self.content = content\n        self.date_posted = mock_timezone_now()\n        self.author = author\n\n    def __str__(self):\n        return self.title\n\n    def get_absolute_url(self):\n        return mock_reverse('post-detail', kwargs={'pk': 1})\n\n# Original Context from models.py\n# Original PostUpdateView from views.py\n\n# Removed unnecessary imports\n# from django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\n# from django.views.generic import UpdateView\n\n# Integrating the provided code with minimal changes\nclass PostUpdateView(MockLoginRequiredMixin, MockUserPassesTestMixin, BaseView):\n    model = Post\n    fields = ['title', 'content']\n\n    def __init__(self, request):\n        self.request = request\n\n    def form_valid(self, form):\n        form.instance.author = self.request.user\n        return super().form_valid(form)\n\n\n    def get_object(self):\n        # Mock retrieving an object\n        return Post(\"Mock Title\", \"Mock Content\", self.request.user)\n\n    def test_func(self):\n        post = self.get_object()\n        if self.request.user == post.author:\n            return True\n        return False\n\n# Assuming the new implementation exists\nclass PostUpdateViewNew(PostUpdateView):\n\n    def form_valid_new_implementation(self, form):\n        # This function is assumed to have the same intended functionality as form_valid\n        form.instance.author = self.request.user\n        return super().form_valid(form)\n\ndef test_form_valid():\n    # Mock data for testing\n    user = MockUser(\"testuser\")\n    request = MockRequest(user)\n    form = MockForm()\n    form.instance = MockModel()\n    \n    view = PostUpdateView(request)\n    new_view = PostUpdateViewNew(request)\n    \n    original_result = view.form_valid(form)\n    new_result = new_view.form_valid_new_implementation(form)\n    \n    # Assertions\n    assert original_result == new_result, \"The result of form validation is different\"\n    assert form.instance.author == request.user, \"form.instance.author is not set correctly in the new implementation\"\n    assert isinstance(new_result, str), \"Return type should be a string\"\n\nif __name__ == \"__main__\":\n    test_form_valid()"
    },
    {
        "func_name": "Profile.save",
        "idx": "291",
        "repo_name": "vsanjay008___my-blog-website",
        "func_path": "users/models.py",
        "orig_func": "def save(self, *args, **kwargs):\n    super(Profile, self).save(*args, **kwargs)\n    img = Image.open(self.image.path)\n    if img.height > 300 or img.width > 300:\n        output_size = (300, 300)\n        img.thumbnail(output_size)\n        img.save(self.image.path)",
        "orig_context": "```python\n## users/models.py\nfrom django.db import models\n\nfrom django.contrib.auth.models import User\n\nfrom PIL import Image\n\nclass Profile(models.Model):\n    user = models.OneToOneField(User, on_delete=models.CASCADE)\n    image = models.ImageField(default='default.jpg', upload_to='profile_pics')\n\n    def __str__(self):\n        return f'{self.user.username} Profile'\n\n    def save(self, *args, **kwargs):\n        super(Profile, self).save(*args, **kwargs)\n\n        img = Image.open(self.image.path)\n\n        if img.height > 300 or img.width > 300:\n            output_size = (300, 300)\n            img.thumbnail(output_size)\n            img.save(self.image.path)\n\n```\n\n\n",
        "eval_script": "# users/models.py\n\nfrom unittest.mock import MagicMock\nfrom PIL import Image\nimport os\n\n# Mock User class to avoid any dependencies\nclass User:\n    def __init__(self, username):\n        self.username = username\n\n# Mock OneToOneField to simulate Django ORM behavior\nclass OneToOneField:\n    def __init__(self, user, on_delete):\n        self.user = user\n        self.on_delete = on_delete\n\nclass Profile:\n    def __init__(self, user, image_path='default.jpg'):\n        self.user = user\n        self.image = ImageField(default='default.jpg', upload_to='profile_pics')\n        self.image.path = image_path  # Mocking the image path\n\n    def __str__(self):\n        return f'{self.user.username} Profile'\n\n    def save(self, *args, **kwargs):\n        # Here, we mock the super call\n        print(\"Profile saved with super class call\")\n        \n        img = Image.open(self.image.path)\n        \n        if img.height > 300 or img.width > 300:\n            output_size = (300, 300)\n            img.thumbnail(output_size)\n            img.save(self.image.path)\n    \n\n\n# Mock ImageField to simulate image path functionality\nclass ImageField:\n    def __init__(self, default, upload_to):\n        self.default = default\n        self.upload_to = upload_to\n\n# Create a mock image for testing\ndef create_mock_image(image_path):\n    if not os.path.exists('/home/user/tmp'):\n        os.makedirs('/home/user/tmp')\n    img = Image.new('RGB', (500, 500))  # Create a large image to trigger resizing\n    img.save(image_path)\n\n# Test Function\ndef test_save():\n    # Create a mock user\n    mock_user = User(username='testuser')\n\n    # Prepare a mock image\n    mock_image_path = '/home/user/tmp/test_image.jpg'\n    create_mock_image(mock_image_path)\n\n    # Create a profile instance using the original save method\n    profile_old = Profile(user=mock_user, image_path=mock_image_path)\n    profile_old.save()\n    \n    # Verify the resulting image for original save method\n    img_old = Image.open(profile_old.image.path)\n    img_old_size = img_old.size\n    \n    # Reset the image to its original state for a fair test\n    create_mock_image(mock_image_path)\n\n    # Create a profile instance using the new save method\n    profile_new = Profile(user=mock_user, image_path=mock_image_path)\n    profile_new.save_new_implementation()\n    \n    # Verify the resulting image for new save method\n    img_new = Image.open(profile_new.image.path)\n    img_new_size = img_new.size\n    \n    # Assert that both methods produce the same image size\n    assert img_old_size == img_new_size, \"Image sizes do not match\"\n    \n    # Additional Assertions\n    assert img_old.width <= 300, \"Image width exceeds 300 pixels\"\n    assert img_old.height <= 300, \"Image height exceeds 300 pixels\"\n\n# Main function\nif __name__ == \"__main__\":\n    test_save()"
    },
    {
        "func_name": "NgrokManager.start_tunnel",
        "idx": "292",
        "repo_name": "Dartvauder___NgrokManager",
        "func_path": "ngrok_manager.py",
        "orig_func": "def start_tunnel(self, port, proto='http', domain=None, auth=None):\n    options = {'proto': proto, 'addr': port}\n    if domain:\n        options['hostname'] = domain\n    if auth:\n        options['auth'] = auth\n    tunnel = ngrok.connect(**options)\n    return tunnel.public_url",
        "orig_context": "```python\n## ngrok_manager.py\nfrom pyngrok import ngrok, conf\n\nclass NgrokManager:\n    def __init__(self, token):\n        conf.get_default().auth_token = token\n\n    def start_tunnel(self, port, proto=\"http\", domain=None, auth=None):\n        options = {\"proto\": proto, \"addr\": port}\n\n        if domain:\n            options[\"hostname\"] = domain\n\n        if auth:\n            options[\"auth\"] = auth\n\n        tunnel = ngrok.connect(**options)\n        return tunnel.public_url\n\n    def stop_tunnel(self, port):\n        for tunnel in ngrok.get_tunnels():\n            if str(port) in tunnel.config[\"addr\"]:\n                ngrok.disconnect(tunnel.public_url)\n                break\n\n    def get_active_tunnels(self):\n        return ngrok.get_tunnels()\n\n    def __del__(self):\n        ngrok.kill()\n\n```\n\n\n",
        "eval_script": "# The debugged PYTHON CODE in one piece.\n\n# Mock pyngrok library\n\nclass MockNgrokTunnel:\n    def __init__(self, public_url, config):\n        self.public_url = public_url\n        self.config = config\n\nclass MockNgrok:\n    active_tunnels = []\n\n    @classmethod\n    def connect(cls, **options):\n        url = f\"http://{options.get('hostname', options['proto'] + '://' + 'mock_url')}\"\n        tunnel = MockNgrokTunnel(url, options)\n        cls.active_tunnels.append(tunnel)\n        return tunnel\n\n    @classmethod\n    def get_tunnels(cls):\n        return cls.active_tunnels\n\n    @classmethod\n    def disconnect(cls, url):\n        cls.active_tunnels = [t for t in cls.active_tunnels if t.public_url != url]\n\n    @classmethod\n    def kill(cls):\n        cls.active_tunnels = []\n\nclass MockConf:\n    class Default:\n        auth_token = None\n\n    @classmethod\n    def get_default(cls):\n        return cls.Default\n\nclass MockPyngrokModule:\n    ngrok = MockNgrok\n    conf = MockConf\n\n# Use feature of Python's dynamic nature to mock pyngrok\nimport sys\nsys.modules['pyngrok'] = MockPyngrokModule\n\n# Now we include the original NgrokManager code after setting up our mocked pyngrok\n\n# Revised ngrok_manager.py\nfrom pyngrok import ngrok, conf\n\nclass NgrokManager:\n    def __init__(self, token):\n        conf.get_default().auth_token = token\n\n    def start_tunnel(self, port, proto=\"http\", domain=None, auth=None):\n        options = {\"proto\": proto, \"addr\": port}\n\n        if domain:\n            options[\"hostname\"] = domain\n\n        if auth:\n            options[\"auth\"] = auth\n\n        tunnel = ngrok.connect(**options)\n        return tunnel.public_url\n\n\n    def stop_tunnel(self, port):\n        for tunnel in ngrok.get_tunnels():\n            if str(port) in tunnel.config[\"addr\"]:\n                ngrok.disconnect(tunnel.public_url)\n                break\n\n    def get_active_tunnels(self):\n        return ngrok.get_tunnels()\n\n    def __del__(self):\n        ngrok.kill()\n\ndef test_start_tunnel():\n    # Test setup\n    token = \"mock-token\"\n    manager = NgrokManager(token)\n    \n    # Test case 1\n    port = 8080\n    url_old = manager.start_tunnel(port)\n    url_new = manager.start_tunnel_new_implementation(port)\n    assert url_old == url_new, \"URLs do not match for default settings\"\n\n    # Test case 2\n    domain = \"example.com\"\n    url_old = manager.start_tunnel(port, domain=domain)\n    url_new = manager.start_tunnel_new_implementation(port, domain=domain)\n    assert url_old == url_new, \"URLs do not match when domain is specified\"\n\n    # Test case 3\n    auth = \"user:password\"\n    url_old = manager.start_tunnel(port, auth=auth)\n    url_new = manager.start_tunnel_new_implementation(port, auth=auth)\n    assert url_old == url_new, \"URLs do not match when authentication is provided\"\n\nif __name__ == \"__main__\":\n    test_start_tunnel()"
    }
]